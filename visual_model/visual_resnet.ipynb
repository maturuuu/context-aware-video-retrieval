{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6b0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install torch torchvision opencv-python numpy tqdm Pillow PyDrive2 pandas scikit-learn openai-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531295ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision\n",
    "# !pip install opencv-python numpy tqdm Pillow PyDrive2\n",
    "# !pip install pandas scikit-learn\n",
    "# !pip install openai-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f74aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shanette\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.6) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42179bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. SET YOUR LOCAL DIRECTORIES ---\n",
    "# We are in the .../visual_model/ folder, so we go up one level\n",
    "os.chdir(\"..\")\n",
    "print(f\"Working directory set to: {os.getcwd()}\")\n",
    "\n",
    "# Path(\".\") is now your main project folder (e.g., 'context-aware-video-retrieval')\n",
    "INPUT_DIR = Path(\".\") / \"media\"\n",
    "OUTPUT_DIR = Path(\".\") / \"embeddings_out\" / \"video2048\"\n",
    "\n",
    "# --- 2. SET YOUR MODEL PARAMETERS ---\n",
    "FRAME_SAMPLE_RATE = 30\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- 3. DEFINE VIDEO EXTENSIONS TO FIND ---\n",
    "VIDEO_EXTENSIONS = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37ff622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model and device...\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "def get_resnet_model(device: str):\n",
    "    \"\"\"Loads the pre-trained ResNet-50 model and its associated transforms.\"\"\"\n",
    "    weights = models.ResNet50_Weights.DEFAULT\n",
    "    model = models.resnet50(weights=weights)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preprocess = weights.transforms()\n",
    "    return model, preprocess\n",
    "\n",
    "print(\"Setting up model and device...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = get_resnet_model(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8775a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_resnet_embeddings(\n",
    "    video_path: Path, \n",
    "    model, \n",
    "    preprocess, \n",
    "    device: str, \n",
    "    frame_sample_rate: int = 30, \n",
    "    batch_size: int = 32\n",
    ") -> np.ndarray:\n",
    "    if not video_path.exists():\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video file: {video_path}\")\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    all_features = []\n",
    "    frame_batch = []\n",
    "    frame_idx = 0\n",
    "    \n",
    "    pbar = tqdm(total=frame_count, desc=f\"Frames for {video_path.name}\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if frame_idx % frame_sample_rate == 0:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(frame_rgb)\n",
    "                frame_batch.append(pil_img)\n",
    "\n",
    "                if len(frame_batch) == batch_size:\n",
    "                    image_inputs = torch.stack(\n",
    "                        [preprocess(img) for img in frame_batch]\n",
    "                    ).to(device)\n",
    "                    image_features = model(image_inputs)\n",
    "                    all_features.append(image_features.squeeze().cpu().numpy())\n",
    "                    frame_batch = []\n",
    "            frame_idx += 1\n",
    "        \n",
    "        if frame_batch:\n",
    "            image_inputs = torch.stack(\n",
    "                [preprocess(img) for img in frame_batch]\n",
    "            ).to(device)\n",
    "            image_features = model(image_inputs)\n",
    "            all_features.append(image_features.squeeze().cpu().numpy())\n",
    "\n",
    "    cap.release()\n",
    "    pbar.close()\n",
    "    if not all_features:\n",
    "        raise ValueError(f\"No frames sampled for {video_path.name}\")\n",
    "\n",
    "    embeddings = np.vstack(all_features)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778a809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files from Google Drive...\n",
      "Found 100 videos in Drive.\n",
      "Found 100 existing embeddings in output.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556ab42614c2461d940ad4c84526bc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Videos:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch processing complete. ---\n"
     ]
    }
   ],
   "source": [
    "# Create the output directory if it doesn't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Reading videos from: {INPUT_DIR.resolve()}\")\n",
    "print(f\"Saving embeddings to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# Find all video files\n",
    "video_files = []\n",
    "for ext in VIDEO_EXTENSIONS:\n",
    "    video_files.extend(INPUT_DIR.glob(f\"*{ext}\"))\n",
    "print(f\"Found {len(video_files)} videos.\")\n",
    "\n",
    "# Get list of files ALREADY in the output folder to skip them\n",
    "existing_embeddings = {f.name for f in OUTPUT_DIR.glob('*_resnet.npy')}\n",
    "print(f\"Found {len(existing_embeddings)} existing ResNet embeddings.\")\n",
    "\n",
    "for video_path in tqdm(video_files, desc=\"Processing Videos (ResNet)\"):\n",
    "    output_filename = f\"{video_path.stem}_resnet.npy\"\n",
    "\n",
    "    # Skip if already processed\n",
    "    if output_filename in existing_embeddings:\n",
    "        continue\n",
    "    \n",
    "    output_path = OUTPUT_DIR / output_filename\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing {video_path.name}...\")\n",
    "        mean_embedding = extract_resnet_embeddings(\n",
    "            video_path=video_path,\n",
    "            model=model,\n",
    "            preprocess=preprocess,\n",
    "            device=device,\n",
    "            frame_sample_rate=FRAME_SAMPLE_RATE,\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "        np.save(output_path, mean_embedding)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Failed to process {video_path.name}: {e}\")\n",
    "\n",
    "print(\"\\n--- Batch processing complete. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
