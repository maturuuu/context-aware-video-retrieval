# -*- coding: utf-8 -*-
"""text embedding

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1koMsBrGtLV72blqluLNUGWLOpooh_Adx

## TEXT EMBEDDING
"""

import os
import json
import ast
import re
import numpy as np
import pandas as pd
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import average_precision_score

CSV_PATH = "video_text_outputs.csv"
JSON_DIR = "sample_data"
MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
SAVE_DIR = "embeddings_output"
BATCH_SIZE = 8
CHUNK_OVERLAP = 50
STEP = 0.10  # grid-search step for weights (0.10 = coarse, 0.05 finer)
DEVICE = "cuda" if __import__("torch").cuda.is_available() else "cpu"
os.makedirs(SAVE_DIR, exist_ok=True)

def file_base_no_ext(fname):
    return os.path.splitext(os.path.basename(str(fname)))[0]

def build_hashtags_text(hashtags):
    if not hashtags:
        return ""
    if isinstance(hashtags, str):
        try:
            parsed = ast.literal_eval(hashtags)
            if isinstance(parsed, (list, tuple)):
                hashtags = parsed
        except Exception:
            hashtags = [hashtags]
    return " ".join([f"#{h}" if not str(h).startswith("#") else str(h)
                     for h in hashtags if str(h).strip()])

def safe_parse_list_field(x):
    import pandas as _pd
    if _pd.isna(x) or x is None:
        return []
    if isinstance(x, (list, tuple)):
        return [str(i) for i in x]
    s = str(x).strip()
    if not s:
        return []
    if (s.startswith("[") and s.endswith("]")) or (s.startswith("(") and s.endswith(")")):
        try:
            parsed = ast.literal_eval(s)
            if isinstance(parsed, (list, tuple)):
                return [str(i) for i in parsed]
        except Exception:
            pass
    return [s]

def infer_label_from_filename(base):
    """
    Infer class label from filename by taking prefix before first separator.
    E.g. 'airball_1' -> 'airball', 'freewill-2.mp4' -> 'freewill'
    This uses filename *data* automatically (no hard-coded labels).
    """
    if not base:
        return ""
    tokens = re.split(r'[_\-\s]+', base)
    return tokens[0].lower() if tokens else base.lower()

# ------- LOAD CSV -------
df = pd.read_csv(CSV_PATH, dtype=str).fillna("")
if "video" not in df.columns:
    raise RuntimeError("CSV must include a 'video' column.")

if "asr" not in df.columns:
    alt = [c for c in df.columns if "asr" in c.lower() or "transcript" in c.lower()]
    df["asr"] = df[alt[0]] if alt else ""
if "ocr_final" not in df.columns:
    alt = [c for c in df.columns if "ocr" in c.lower()]
    df["ocr_final"] = df[alt[0]] if alt else ""

df["video_base"] = df["video"].apply(file_base_no_ext)
df["inferred_label"] = df["video_base"].apply(infer_label_from_filename)

# ------- LOAD METADATA FROM JSON -------
json_map = {}
if os.path.isdir(JSON_DIR):
    for fname in os.listdir(JSON_DIR):
        if fname.lower().endswith(".json"):
            json_map[os.path.splitext(fname)[0]] = os.path.join(JSON_DIR, fname)

descs, hashtags_texts, missing_json_bases = [], [], []
for base in df["video_base"].tolist():
    p = json_map.get(base)
    if not p:
        descs.append("")
        hashtags_texts.append("")
        missing_json_bases.append(base)
        continue
    try:
        with open(p, "r", encoding="utf-8") as fh:
            j = json.load(fh)
        vm = j.get("video_metadata", j)
        descs.append(vm.get("description", "") or "")
        hashtags_texts.append(build_hashtags_text(vm.get("hashtags", []) or []))
    except Exception:
        descs.append("")
        hashtags_texts.append("")
        missing_json_bases.append(base)

df["description"] = descs
df["hashtags_text"] = hashtags_texts

# ------- MODEL INIT -------
model = SentenceTransformer(MODEL_NAME, device=DEVICE)
tokenizer = model.tokenizer
max_tokens = getattr(tokenizer, "model_max_length", 384)
embed_dim = model.get_sentence_embedding_dimension()
print(f"Model: {MODEL_NAME} | output dim: {embed_dim} | token limit: {max_tokens}")

# ------- CHUNKING + ENCODING -------
def chunk_and_encode_text(text):
    ids = tokenizer.encode(text, add_special_tokens=False)
    if len(ids) == 0:
        return np.zeros(embed_dim, dtype=np.float32), 0

    chunk_size = max_tokens - 20
    if len(ids) <= chunk_size:
        emb = model.encode(text, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
        return emb.astype(np.float32), 1

    # overlapping chunks
    chunks_ids = []
    start = 0
    while start < len(ids):
        end = min(len(ids), start + chunk_size)
        chunks_ids.append(ids[start:end])
        if end == len(ids):
            break
        start = end - CHUNK_OVERLAP

    chunk_embs = []
    for i in range(0, len(chunks_ids), BATCH_SIZE):
        batch_ids = chunks_ids[i:i+BATCH_SIZE]
        batch_texts = [tokenizer.decode(chunk, clean_up_tokenization_spaces=True) for chunk in batch_ids]
        embs = model.encode(batch_texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
        chunk_embs.append(embs)

    chunk_embs = np.vstack(chunk_embs)
    video_emb = chunk_embs.mean(axis=0)
    norm = np.linalg.norm(video_emb)
    if norm > 0:
        video_emb = video_emb / norm
    return video_emb.astype(np.float32), len(chunk_embs)

modalities = ["ocr_final", "asr", "hashtags_text", "description"]
emb_dict = {m: [] for m in modalities}
chunk_counts = []

for _, row in tqdm(df.iterrows(), total=len(df), desc="Embedding each modality"):
    counts = {}
    for m in modalities:
        text = row.get(m, "")
        if not isinstance(text, str) or not text.strip():
            emb, n_chunks = np.zeros(embed_dim, dtype=np.float32), 0
        else:
            emb, n_chunks = chunk_and_encode_text(text)
        emb_dict[m].append(emb)
        counts[m] = n_chunks
    chunk_counts.append(counts)

for m in modalities:
    emb_dict[m] = np.vstack(emb_dict[m])

# ------- WEIGHT SEARCH (OPTIMIZE mAP) -------
def compute_map_for_weights(weights, emb_dict, labels):
    W = np.array(weights, dtype=np.float32)
    combined = (
        W[0] * emb_dict["ocr_final"] +
        W[1] * emb_dict["hashtags_text"] +
        W[2] * emb_dict["asr"] +
        W[3] * emb_dict["description"]
    )
    norms = np.linalg.norm(combined, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    combined = combined / norms

    sim = cosine_similarity(combined)

    n = len(labels)
    ap_list = []
    for i in range(n):
        sims = sim[i].copy()
        sims[i] = -np.inf
        idx_sorted = np.argsort(-sims)
        relevance = (labels[idx_sorted] == labels[i]).astype(int)
        if relevance.sum() == 0:
            continue
        scores_sorted = sims[idx_sorted]
        try:
            ap = average_precision_score(relevance, scores_sorted)
        except Exception:
            cum_pos = 0
            precisions = []
            for k, r in enumerate(relevance, start=1):
                if r:
                    cum_pos += 1
                    precisions.append(cum_pos / k)
            ap = np.mean(precisions) if precisions else 0.0
        ap_list.append(ap)
    if len(ap_list) == 0:
        return 0.0
    return float(np.mean(ap_list))

labels = df["inferred_label"].values

best_score = -1.0
best_weights = None
r = np.arange(0.0, 1.0 + 1e-9, STEP)
for w0 in r:
    for w1 in r:
        if w0 + w1 > 1.0 + 1e-12:
            continue
        for w2 in r:
            if w0 + w1 + w2 > 1.0 + 1e-12:
                continue
            w3 = 1.0 - (w0 + w1 + w2)
            if w3 < -1e-12:
                continue
            weights = [w0, w1, w2, w3]
            score = compute_map_for_weights(weights, emb_dict, labels)
            if score > best_score:
                best_score = score
                best_weights = weights

print(f"\nGrid search done. Best mAP: {best_score:.4f} with weights (ocr, hashtags, asr, desc) = {best_weights}")

if best_weights is None:
    best_weights = [0.4, 0.15, 0.35, 0.10]
    print("No best weights found by search; using default heuristic:", best_weights)

# ------- BUILD FINAL COMBINED EMBEDDINGS -------
combined_embs = (
    best_weights[0] * emb_dict["ocr_final"] +
    best_weights[1] * emb_dict["hashtags_text"] +
    best_weights[2] * emb_dict["asr"] +
    best_weights[3] * emb_dict["description"]
)

# Normalize
norms = np.linalg.norm(combined_embs, axis=1, keepdims=True)
norms[norms == 0] = 1.0
combined_embs = combined_embs / norms

np.save(os.path.join(SAVE_DIR, "textual_embeddings.npy"), combined_embs)

emb_paths = []
for i, base in enumerate(df["video_base"].tolist()):
    emb = combined_embs[i]
    fname = f"{base}_text_emb.npy"
    fpath = os.path.join(SAVE_DIR, fname)
    np.save(fpath, emb.astype(np.float32))
    emb_paths.append(fpath)

df["text_embedding_path"] = emb_paths

with open(os.path.join(SAVE_DIR, "best_text_weights.json"), "w", encoding="utf-8") as fh:
    json.dump({
        "weights": {
            "ocr_final": float(best_weights[0]),
            "hashtags_text": float(best_weights[1]),
            "asr": float(best_weights[2]),
            "description": float(best_weights[3])
        },
        "best_map": float(best_score)
    }, fh, indent=2)

df.to_csv(os.path.join(SAVE_DIR, "embeddings_metadata.csv"), index=False)

print(f"âœ… Saved fused embeddings + per-file embeddings + best weights to {SAVE_DIR}")
print("Combined shape:", combined_embs.shape)

# Rank videos by similarity
def rank_similar_videos(query_index, top_k=10):
    sims = sim_matrix[query_index]
    sorted_idx = np.argsort(-sims)
    query_name = df.iloc[query_index]["video_base"]
    print(f"\nTop similar videos to: {query_name}")
    for i, idx in enumerate(sorted_idx[:top_k]):
        vid = df.iloc[idx]["video_base"]
        print(f"{i+1}. {vid} (similarity: {sims[idx]:.4f})")

# Example: rank for first video
rank_similar_videos(43)

# Zip the entire folder
!zip -r embeddings_output.zip embeddings_output

# Download the zip file
from google.colab import files
files.download("embeddings_output.zip")