# -*- coding: utf-8 -*-
"""text embedding - make string then embed

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L5_cSxK8YkaxGCfPcGO3vam8iqgLaEC9

## TEXT EMBEDDING  most correct to paper
"""

import os
import json
import ast
import re
import numpy as np
import pandas as pd
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

# CONFIG
CSV_PATH = "video_text_outputs.csv"
JSON_DIR = "sample_data"
MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
SAVE_DIR_0 = "base_dir"
SAVE_DIR = os.path.join(SAVE_DIR_0, "text768")
BATCH_SIZE = 8
DEVICE = "cuda" if __import__("torch").cuda.is_available() else "cpu"

os.makedirs(SAVE_DIR, exist_ok=True)

# LOAD DATA
df = pd.read_csv(CSV_PATH).fillna("")
df["video_base"] = df["video"].apply(lambda x: os.path.splitext(os.path.basename(str(x)))[0])

# MODEL
model = SentenceTransformer(MODEL_NAME, device=DEVICE)

# FIXED WEIGHTS
weights = {
    "ocr_final": 0.4,
    "hashtags_text": 0.1,
    "asr": 0.3,
    "description": 0.2
}
print(f"Using fixed weights (ocr, hashtags, asr, desc) = {list(weights.values())}")

# BUILD TEXT
def make_weighted_fused_text(row):
    ocr = str(row.get("ocr_final", "")).strip()
    hashtags = str(row.get("hashtags_text", "")).strip()
    asr = str(row.get("asr", "")).strip()
    desc = str(row.get("description", "")).strip()

    parts = [
        (ocr + " ") * int(round(weights["ocr_final"] * 10)),
        (hashtags + " ") * int(round(weights["hashtags_text"] * 10)),
        (asr + " ") * int(round(weights["asr"] * 10)),
        (desc + " ") * int(round(weights["description"] * 10))
    ]
    fused_text = " ".join([p for p in parts if p.strip()])
    return fused_text.strip()

# CONCATENATE ALL TEXTS
texts = []
print("\n==== CONCATENATED TEXTS =====\n")

for i, row in tqdm(df.iterrows(), total=len(df), desc="Building text"):
    text = make_weighted_fused_text(row)
    texts.append(text)
    print(f"[{i+1}] {text}\n")

# ENCODE + SAVE PER FILE
print("\nEncoding and saving embeddings per file...")

for i, text in tqdm(enumerate(texts), total=len(texts), desc="Encoding text embeddings"):
    base_name = df.iloc[i]["video_base"]
    embedding = model.encode([text], batch_size=1, convert_to_numpy=True, normalize_embeddings=True)[0]
    save_path = os.path.join(SAVE_DIR, f"{base_name}.npy")
    np.save(save_path, embedding)

print(f"\n✅ Saved {len(texts)} individual embeddings to: {SAVE_DIR}")

# COSINE SIMILARITY
sim_matrix = cosine_similarity(embeddings)

def rank_similar_videos(query_index, top_k=10):
    sims = sim_matrix[query_index]
    sorted_idx = np.argsort(-sims)
    query_name = df.iloc[query_index].get("video_base", f"index_{query_index}")
    print(f"\nTop similar videos to: {query_name}")
    for i, idx in enumerate(sorted_idx[:top_k]):
        vid = df.iloc[idx].get("video_base", f"index_{idx}")
        print(f"{i+1}. {vid} (similarity: {sims[idx]:.4f})")

rank_similar_videos(3)

import shutil

shutil.rmtree("base_dir", ignore_errors=True)
print("✅ Deleted folder: base_dir")

from google.colab import drive
drive.mount('/content/drive')