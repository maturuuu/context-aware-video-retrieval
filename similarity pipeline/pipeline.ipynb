{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99223acb",
   "metadata": {},
   "source": [
    "<img src=\"../DLSU-ALTDSI-logo.png\" width=\"100%\" style=\"margin-bottom:-40px; margin-top:-30px;\"/>\n",
    "\n",
    "**This notebook contains the context-aware video retrieval pipeline used in the study:**\n",
    "\n",
    "## *Comparing Modality Representation Schemes in Video Retrieval for More Context-Aware Auto-Annotation of Trending Short-Form Videos*\n",
    "\n",
    "**By the following researchers from the Andrew L. Tan Data Science Institute:**\n",
    "1. Ong, Matthew Kristoffer Y. (matthew_kristoffer_ong@dlsu.edu.ph)\n",
    "2. Presas, Shanette Giane G. (shanette_giane_presas@dlsu.edu.ph)\n",
    "3. Sarreal, Sophia Althea R. (sophia_sarreal@dlsu.edu.ph)\n",
    "4. To, Jersey Jaclyn K. (jers_to@dlsu.edu.ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a61d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6f596",
   "metadata": {},
   "source": [
    "Note to thesismates:\n",
    "1. Run this to activate venv for the terminal instance: .venv\\Scripts\\activate\n",
    "2. NOTE: you will also need the ff files:\n",
    "    1. 'class_labels_indices.csv'\n",
    "    2. 'Cnn14_mAP=0.431.pth' (these are the model weights to be used) from https://zenodo.org/records/3987831"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa930e",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ffmpeg\n",
    "import torch\n",
    "import librosa\n",
    "from panns_inference import AudioTagging\n",
    "\n",
    "# Make sure cuda (gpu) is active!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38964d22",
   "metadata": {},
   "source": [
    "---\n",
    "## **AUDIO MODALITY**\n",
    "**Goal**: Produce embeddings representing the audio modality of a given set of videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25bf68",
   "metadata": {},
   "source": [
    "**Preprocessing step:** extract 32kHz waveform files from the input videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd08338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_to_wavs(video_path: str, out32: str, overwrite: bool=True):\n",
    "    extract_32k=(\n",
    "        ffmpeg.input(video_path).output(out32, format='wav', acodec='pcm_s16le', ac=1, ar=32000)\n",
    "    )\n",
    "    if overwrite:\n",
    "        extract_32k = extract_32k.overwrite_output()\n",
    "    \n",
    "    extract_32k.run(quiet=True)\n",
    "    print(\"Wrote 32kHz\", out32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6aefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path: str, out_dir: str =\"proc_out\"):\n",
    "    out_dir = Path(out_dir)\n",
    "    audio_dir = out_dir.parent / (out_dir.name + \"_32kHz\")\n",
    "    audio_dir.mkdir(parents=True, exist_ok=True) # 32kHz goes to audio_dir\n",
    "\n",
    "    video = Path(video_path)\n",
    "    out32 = audio_dir / (video.stem + \"_32k.wav\") # 32kHz output\n",
    "\n",
    "    # Extract audio\n",
    "    extract_audio_to_wavs(str(video), str(out32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_dir = Path(\"media\")\n",
    "videos = list(media_dir.glob(\"*.mp4\"))\n",
    "print(f\"{len(videos)} videos found!\")\n",
    "\n",
    "for video in videos:\n",
    "    print(f\"\\nProcessing: {video.name}\")\n",
    "    process_video(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f21d0d",
   "metadata": {},
   "source": [
    "**Feature extraction step:** produce embeddings in the form of a 2048-dimensional feature vector representing the audio of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ca0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_out_32kHz_dir = Path(\"proc_out_32kHz\")\n",
    "emb_out_dir = Path(\"embeddings_out/audio2048\") # 2048-d vectors go here\n",
    "emb_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "at_model = AudioTagging(checkpoint_path=None, device=device) #this is the pretrained CNN14\n",
    "\n",
    "wav_files = sorted(proc_out_32kHz_dir.glob(\"*_32k.wav\"))\n",
    "print(f\"{len(wav_files)} WAV files found!\")\n",
    "\n",
    "for wav_path in wav_files:\n",
    "    print(f\"\\nProcessing: {wav_path.name}\")\n",
    "    wav, sr = librosa.load(str(wav_path), sr=32000, mono=True) # just to make sure wav is 32kHz\n",
    "    audio_batch = np.expand_dims(wav, axis=0) # matches the expected shape of PANN\n",
    "\n",
    "    _, embedding = at_model.inference(audio_batch) # gets the embedding as numpy array\n",
    "\n",
    "    embedding_vec = embedding[0] # first element of embedding array\n",
    "\n",
    "    out_path = emb_out_dir / (wav_path.stem + \"_embedding2048.npy\")\n",
    "    np.save(str(out_path), embedding_vec)\n",
    "    print(\"Embedding saved: \", out_path)\n",
    "\n",
    "    print(embedding_vec) # if you want to see the vector\n",
    "    print(embedding_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213682a5",
   "metadata": {},
   "source": [
    "---\n",
    "## **VISUAL MODALITY**\n",
    "**Goal**: Produce embeddings representing the visual modality of a given set of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf09c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7365085",
   "metadata": {},
   "source": [
    "---\n",
    "## **TEXT MODALITY**\n",
    "**Goal**: Produce embeddings representing the text modality of a given set of videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3c38b",
   "metadata": {},
   "source": [
    "---\n",
    "## **RETRIEVING SIMILAR VIDEOS**\n",
    "**Goal**: Produce a list of most similar videos based on a weighted combination of modality-specific cosine similarity scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
