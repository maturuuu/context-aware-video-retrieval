{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99223acb",
   "metadata": {},
   "source": [
    "<img src=\"../DLSU-ALTDSI-logo.png\" width=\"100%\" style=\"margin-bottom:10px; margin-top:0px;\"/>\n",
    "\n",
    "**This notebook contains the context-aware video retrieval pipeline used in the study:**\n",
    "\n",
    "## *Comparing Modality Representation Schemes in Video Retrieval for More Context-Aware Auto-Annotation of Trending Short-Form Videos*\n",
    "\n",
    "**By the following researchers from the Andrew L. Tan Data Science Institute:**\n",
    "1. Ong, Matthew Kristoffer Y. (matthew_kristoffer_ong@dlsu.edu.ph)\n",
    "2. Presas, Shanette Giane G. (shanette_giane_presas@dlsu.edu.ph)\n",
    "3. Sarreal, Sophia Althea R. (sophia_sarreal@dlsu.edu.ph)\n",
    "4. To, Jersey Jaclyn K. (jers_to@dlsu.edu.ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a61d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6f596",
   "metadata": {},
   "source": [
    "Note to thesismates:\n",
    "1. Navigate first to the similarity pipeline folder\n",
    "2. Run this to activate venv for the terminal instance: .venv\\Scripts\\activate\n",
    "3. NOTE: you will also need the ff files:\n",
    "    1. 'class_labels_indices.csv'\n",
    "    2. 'Cnn14_mAP=0.431.pth' (these are the model weights to be used) from https://zenodo.org/records/3987831\n",
    "    3. This specific torchaudio/torchvision model: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa930e",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mkyod\\OneDrive\\Documents\\GitHub\\context-aware-video-retrieval\\similarity pipeline\\.venv\\Lib\\site-packages\\ctranslate2\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# audio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ffmpeg\n",
    "import torch\n",
    "import librosa\n",
    "from panns_inference import AudioTagging\n",
    "\n",
    "# visuals\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#text\n",
    "import easyocr\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "import string\n",
    "from ftfy import fix_text\n",
    "from wordsegment import load as ws_load, segment as ws_segment\n",
    "from spellchecker import SpellChecker\n",
    "from transformers import pipeline\n",
    "import wordninja\n",
    "from wordfreq import zipf_frequency\n",
    "import glob\n",
    "import pathlib\n",
    "import subprocess\n",
    "import sys\n",
    "from faster_whisper import WhisperModel\n",
    "from openai import OpenAI\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "#similarity\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import faulthandler\n",
    "faulthandler.enable()\n",
    "\n",
    "# Make sure cuda (gpu) is active!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38964d22",
   "metadata": {},
   "source": [
    "---\n",
    "## **AUDIO MODALITY**\n",
    "**Goal**: Produce embeddings representing the audio modality of a given set of videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25bf68",
   "metadata": {},
   "source": [
    "**Preprocessing step:** extracts 32kHz waveform files from the input videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd08338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_to_wavs(video_path: str, out32: str, overwrite: bool=True):\n",
    "    extract_32k=(\n",
    "        ffmpeg.input(video_path).output(out32, format='wav', acodec='pcm_s16le', ac=1, ar=32000)\n",
    "    )\n",
    "    if overwrite:\n",
    "        extract_32k = extract_32k.overwrite_output()\n",
    "    \n",
    "    extract_32k.run(quiet=True)\n",
    "    print(\"Wrote 32kHz\", out32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6aefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path: str, out_dir: str =\"proc_out\"):\n",
    "    out_dir = Path(out_dir)\n",
    "    audio_dir = out_dir.parent / (out_dir.name + \"_32kHz\")\n",
    "    audio_dir.mkdir(parents=True, exist_ok=True) # 32kHz goes to audio_dir\n",
    "\n",
    "    video = Path(video_path)\n",
    "    out32 = audio_dir / (video.stem + \"_32k.wav\") # 32kHz output\n",
    "\n",
    "    # Extract audio\n",
    "    extract_audio_to_wavs(str(video), str(out32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dae2ede4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 videos found!\n",
      "\n",
      "Processing: trend1vid1.mp4\n",
      "Wrote 32kHz proc_out_32kHz\\trend1vid1_32k.wav\n",
      "\n",
      "Processing: trend1vid2.mp4\n",
      "Wrote 32kHz proc_out_32kHz\\trend1vid2_32k.wav\n",
      "\n",
      "Processing: trend3vid1.mp4\n",
      "Wrote 32kHz proc_out_32kHz\\trend3vid1_32k.wav\n"
     ]
    }
   ],
   "source": [
    "media_dir = Path(\"media\")\n",
    "videos = list(media_dir.glob(\"*.mp4\"))\n",
    "print(f\"{len(videos)} videos found!\")\n",
    "\n",
    "for video in videos:\n",
    "    print(f\"\\nProcessing: {video.name}\")\n",
    "    process_video(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f21d0d",
   "metadata": {},
   "source": [
    "**Feature extraction step:** produces embeddings in the form of a 2048-dimensional feature vector representing the audio of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a3ca0fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: C:\\Users\\mkyod/panns_data/Cnn14_mAP=0.431.pth\n",
      "GPU number: 1\n",
      "3 WAV files found!\n",
      "\n",
      "Processing: trend1vid1_32k.wav\n",
      "Embedding saved:  embeddings_out\\audio2048\\trend1vid1_emb-audio2048.npy\n",
      "[0.         0.13245013 0.         ... 0.         1.0539732  0.        ]\n",
      "(2048,)\n",
      "\n",
      "Processing: trend1vid2_32k.wav\n",
      "Embedding saved:  embeddings_out\\audio2048\\trend1vid2_emb-audio2048.npy\n",
      "[0.         0.         0.         ... 0.35793105 0.         0.        ]\n",
      "(2048,)\n",
      "\n",
      "Processing: trend3vid1_32k.wav\n",
      "Embedding saved:  embeddings_out\\audio2048\\trend3vid1_emb-audio2048.npy\n",
      "[0.         0.         0.         ... 0.23337889 0.60742337 0.        ]\n",
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "proc_out_32kHz_dir = Path(\"proc_out_32kHz\")\n",
    "emb_out_dir = Path(\"embeddings_out/audio2048\") # 2048-d vectors go here\n",
    "emb_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "at_model = AudioTagging(checkpoint_path=None, device=device) #this is the pretrained CNN14\n",
    "\n",
    "wav_files = sorted(proc_out_32kHz_dir.glob(\"*_32k.wav\"))\n",
    "print(f\"{len(wav_files)} WAV files found!\")\n",
    "\n",
    "for wav_path in wav_files:\n",
    "    print(f\"\\nProcessing: {wav_path.name}\")\n",
    "    wav, sr = librosa.load(str(wav_path), sr=32000, mono=True) # just to make sure wav is 32kHz\n",
    "    audio_batch = np.expand_dims(wav, axis=0) # matches the expected shape of PANN\n",
    "\n",
    "    _, embedding = at_model.inference(audio_batch) # gets the embedding as numpy array\n",
    "\n",
    "    embedding_vec = embedding[0] # first element of embedding array\n",
    "\n",
    "    # just removing the \"_32k\" for filename consistency\n",
    "    stem = wav_path.stem\n",
    "    if stem.endswith(\"_32k\"):\n",
    "        stem = stem[:-4]\n",
    "\n",
    "    out_path = emb_out_dir / f\"{stem}_emb-audio2048.npy\"\n",
    "    np.save(str(out_path), embedding_vec)\n",
    "    print(\"Embedding saved: \", out_path)\n",
    "\n",
    "    print(embedding_vec) # if you want to see the vector\n",
    "    print(embedding_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213682a5",
   "metadata": {},
   "source": [
    "---\n",
    "## **VISUAL MODALITY**\n",
    "**Goal**: Produce embeddings representing the visual modality of a given set of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf09c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. SET YOUR LOCAL DIRECTORIES ---\n",
    "INPUT_DIR = Path(\"media\")\n",
    "OUTPUT_DIR = Path(\"embeddings_out/video2048\")\n",
    "\n",
    "# --- 2. SET YOUR MODEL PARAMETERS ---\n",
    "FRAME_SAMPLE_RATE = 30\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- 3. DEFINE VIDEO EXTENSIONS TO FIND ---\n",
    "VIDEO_EXTENSIONS = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a263dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_model(device: str):\n",
    "    \"\"\"Loads the pre-trained ResNet-50 model and its associated transforms.\"\"\"\n",
    "    weights = models.ResNet50_Weights.DEFAULT\n",
    "    model = models.resnet50(weights=weights)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preprocess = weights.transforms()\n",
    "    return model, preprocess\n",
    "\n",
    "model, preprocess = get_resnet_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c66cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_resnet_embeddings(\n",
    "    video_path: Path, \n",
    "    model, \n",
    "    preprocess, \n",
    "    device: str, \n",
    "    frame_sample_rate: int = 30, \n",
    "    batch_size: int = 32\n",
    ") -> np.ndarray:\n",
    "    if not video_path.exists():\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video file: {video_path}\")\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    all_features = []\n",
    "    frame_batch = []\n",
    "    frame_idx = 0\n",
    "    \n",
    "    pbar = tqdm(total=frame_count, desc=f\"Frames for {video_path.name}\", leave=True, disable=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if frame_idx % frame_sample_rate == 0:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(frame_rgb)\n",
    "                frame_batch.append(pil_img)\n",
    "\n",
    "                if len(frame_batch) == batch_size:\n",
    "                    image_inputs = torch.stack(\n",
    "                        [preprocess(img) for img in frame_batch]\n",
    "                    ).to(device)\n",
    "                    image_features = model(image_inputs)\n",
    "                    all_features.append(image_features.squeeze().cpu().numpy())\n",
    "                    frame_batch = []\n",
    "            frame_idx += 1\n",
    "        \n",
    "        if frame_batch:\n",
    "            image_inputs = torch.stack(\n",
    "                [preprocess(img) for img in frame_batch]\n",
    "            ).to(device)\n",
    "            image_features = model(image_inputs)\n",
    "            all_features.append(image_features.squeeze().cpu().numpy())\n",
    "\n",
    "    cap.release()\n",
    "    pbar.close()\n",
    "    if not all_features:\n",
    "        raise ValueError(f\"No frames sampled for {video_path.name}\")\n",
    "\n",
    "    embeddings = np.vstack(all_features)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9418b712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading videos from: C:\\Users\\mkyod\\OneDrive\\Documents\\GitHub\\context-aware-video-retrieval\\similarity pipeline\\media\n",
      "Saving embeddings to: C:\\Users\\mkyod\\OneDrive\\Documents\\GitHub\\context-aware-video-retrieval\\similarity pipeline\\embeddings_out\\video2048\n",
      "Found 3 videos.\n",
      "Found 0 existing ResNet embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos (ResNet):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trend1vid1.mp4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos (ResNet):  33%|███▎      | 1/3 [00:00<00:00,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trend1vid2.mp4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos (ResNet):  67%|██████▋   | 2/3 [00:03<00:01,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trend3vid1.mp4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos (ResNet): 100%|██████████| 3/3 [00:03<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch processing complete. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Reading videos from: {INPUT_DIR.resolve()}\")\n",
    "print(f\"Saving embeddings to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# Find all video files\n",
    "video_files = []\n",
    "for ext in VIDEO_EXTENSIONS:\n",
    "    video_files.extend(INPUT_DIR.glob(f\"*{ext}\"))\n",
    "print(f\"Found {len(video_files)} videos.\")\n",
    "\n",
    "# Get list of files ALREADY in the output folder to skip them\n",
    "existing_embeddings = {f.name for f in OUTPUT_DIR.glob('*_resnet.npy')}\n",
    "print(f\"Found {len(existing_embeddings)} existing ResNet embeddings.\")\n",
    "\n",
    "for video_path in tqdm(video_files, desc=\"Processing Videos (ResNet)\"):\n",
    "    output_filename = f\"{video_path.stem}_emb-visual2048.npy\"\n",
    "\n",
    "    # Skip if already processed\n",
    "    if output_filename in existing_embeddings:\n",
    "        continue\n",
    "    \n",
    "    output_path = OUTPUT_DIR / output_filename\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing {video_path.name}...\")\n",
    "        mean_embedding = extract_resnet_embeddings(\n",
    "            video_path=video_path,\n",
    "            model=model,\n",
    "            preprocess=preprocess,\n",
    "            device=device,\n",
    "            frame_sample_rate=FRAME_SAMPLE_RATE,\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "        np.save(output_path, mean_embedding)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Failed to process {video_path.name}: {e}\")\n",
    "\n",
    "print(\"\\n--- Batch processing complete. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7365085",
   "metadata": {},
   "source": [
    "---\n",
    "## **TEXT MODALITY**\n",
    "**Goal**: Produce embeddings representing the text modality of a given set of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce55d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CSV = \"video_text_outputs.csv\"\n",
    "OPENAI_API_KEY = \"KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c58e1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _list_local_videos(root_dir):\n",
    "    exts = ('.mp4', '.mov', '.m4v', '.mkv', '.avi', '.webm')\n",
    "    paths = []\n",
    "    for ext in exts:\n",
    "        paths.extend(glob.glob(os.path.join(root_dir, f\"**/*{ext}\"), recursive=True))\n",
    "    return sorted(paths)\n",
    "\n",
    "def _fetch_videos_from_folder(folder_path):\n",
    "    if not folder_path or folder_path.strip() == \"\":\n",
    "        raise ValueError(\"Folder path is empty.\")\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        return _list_local_videos(folder_path)\n",
    "\n",
    "    raise ValueError(f\"Not a valid local directory: {folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee87f5",
   "metadata": {},
   "source": [
    "**Automatic speech recognition (ASR) step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f360a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_with_whisper(video_path):\n",
    "    try:\n",
    "        model = WhisperModel(\"base\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                             compute_type=\"int8_float16\" if torch.cuda.is_available() else \"int8\")\n",
    "        segments, _ = model.transcribe(video_path, beam_size=5)\n",
    "        return \" \".join(s.text for s in segments).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"ASR Error: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f07a7",
   "metadata": {},
   "source": [
    "**Optical character recognition (OCR) step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85ccb50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_text(text):\n",
    "    if not text or len(text.strip()) < 2:\n",
    "        return False\n",
    "    clean = re.sub(r'[^\\w#@]', '', text)\n",
    "    return len(clean) > 0\n",
    "\n",
    "def preprocess_frame_for_ocr(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "    denoised = cv2.GaussianBlur(enhanced, (3, 3), 0)\n",
    "    kernel = np.array([[-1,-1,-1], [-1, 9,-1], [-1,-1,-1]])\n",
    "    sharpened = cv2.filter2D(denoised, -1, kernel)\n",
    "    return sharpened\n",
    "\n",
    "def extract_ocr_from_video(video_path, sample_rate_fps=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Cannot open video {video_path}\")\n",
    "        return {}\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "    frame_interval = max(1, int(round(fps / max(0.1, sample_rate_fps))))\n",
    "\n",
    "    reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "    text_detections = defaultdict(lambda: {'count': 0, 'timestamps': [], 'positions': []})\n",
    "\n",
    "    print(\"Processing video frames for OCR...\")\n",
    "    processed = 0\n",
    "\n",
    "    for frame_idx in range(0, total_frames, frame_interval):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame is None:\n",
    "            continue\n",
    "\n",
    "        h, w = frame.shape[:2]\n",
    "        max_w = 960\n",
    "        if w > max_w:\n",
    "            scale = max_w / float(w)\n",
    "            frame = cv2.resize(frame, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        timestamp = frame_idx / fps\n",
    "\n",
    "        try:\n",
    "            frame_pp = preprocess_frame_for_ocr(frame)\n",
    "            results = reader.readtext(frame_pp, detail=1, paragraph=False)\n",
    "            for (bbox, text, confidence) in results:\n",
    "                if confidence > 0.5 and is_valid_text(text):\n",
    "                    xs = [p[0] for p in bbox]\n",
    "                    ys = [p[1] for p in bbox]\n",
    "                    x_left = float(min(xs))\n",
    "                    y_top = float(min(ys))\n",
    "\n",
    "                    text_detections[text]['count'] += 1\n",
    "                    if len(text_detections[text]['timestamps']) < 5:\n",
    "                        text_detections[text]['timestamps'].append(round(timestamp, 2))\n",
    "                    if len(text_detections[text]['positions']) < 5:\n",
    "                        text_detections[text]['positions'].append((round(y_top, 2), round(x_left, 2)))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"OCR error at frame {frame_idx}: {e}\")\n",
    "\n",
    "        processed += 1\n",
    "        if processed % 10 == 0:\n",
    "            print(f\"Processed {processed} sampled frames...\")\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"OCR processing complete. Found {len(text_detections)} unique text phrases.\")\n",
    "    return dict(text_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ff2d9",
   "metadata": {},
   "source": [
    "**OCR cleaning step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8304a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ocr_with_openai(ocr_phrases, api_key, model=\"gpt-4o-mini\"):\n",
    "    phrases = list(ocr_phrases.keys())\n",
    "    if not phrases:\n",
    "        return []\n",
    "\n",
    "    print(f\"Cleaning {len(phrases)} OCR phrases with {model}...\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    system_prompt = \"\"\"You are an OCR error correction assistant. Fix only obvious OCR mistakes.\n",
    "\n",
    "Common OCR errors:\n",
    "- Character confusion: 'v' → 'y', 'rn' → 'm', '0' → 'O', 'i' → 'l', 'vv' → 'w', '@' → 'a', '@' → 'o'\n",
    "- Missing spaces: 'helloworld' → 'hello world'\n",
    "- Extra spaces: 'hel lo' → 'hello'\n",
    "\n",
    "Rules:\n",
    "1. ONLY fix clear OCR errors - do not rephrase or change meaning\n",
    "2. Preserve hashtags (#) exactly\n",
    "3. Keep original capitalization only for proper nouns, otherwise make everything lowercase.\n",
    "4. Output ONLY the corrected text (no quotes, explanations, or extra words)\n",
    "5. If a word has the letter 'v' in it and it looks misspelled, try swapping the 'v' with a 'y' to see if it makes more sense, and vice versa.\n",
    "6. If a word other than \"I\" has the letter 'i' in it and it looks misspelled, try swapping the 'i' with a 'l' to see if it makes more sense, and vice versa.\n",
    "7. Unless an acronym makes sense in the context, make it lowercase.\"\"\"\n",
    "\n",
    "    cleaned = []\n",
    "    total_cost = 0.0\n",
    "\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            print(f\"  Cleaned {i}/{len(phrases)} (Cost: ${total_cost:.4f})\")\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"Fix OCR errors: {phrase}\"}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=100\n",
    "            )\n",
    "\n",
    "            result = response.choices[0].message.content.strip()\n",
    "\n",
    "            # Track cost\n",
    "            if hasattr(response, 'usage') and response.usage:\n",
    "                input_tok = response.usage.prompt_tokens or 0\n",
    "                output_tok = response.usage.completion_tokens or 0\n",
    "                total_cost += (input_tok * 0.15 + output_tok * 0.60) / 1_000_000\n",
    "\n",
    "            # Remove quotes if added\n",
    "            if (result.startswith('\"') and result.endswith('\"')) or \\\n",
    "               (result.startswith(\"'\") and result.endswith(\"'\")):\n",
    "                result = result[1:-1]\n",
    "\n",
    "            # Fallback if result is empty or way too different\n",
    "            if not result or len(result) > len(phrase) * 3:\n",
    "                result = phrase\n",
    "\n",
    "            cleaned.append(result.strip())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error cleaning '{phrase}': {e}\")\n",
    "            cleaned.append(phrase)\n",
    "\n",
    "    print(f\"Cleaning complete! Total cost: ${total_cost:.4f}\")\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828ed7f",
   "metadata": {},
   "source": [
    "**Deduplication and merge step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b795f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text)  # Keep hashtags and mentions\n",
    "    return re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "def text_similarity(s1, s2):\n",
    "    return SequenceMatcher(None, normalize_text(s1), normalize_text(s2)).ratio()\n",
    "\n",
    "def smart_deduplicate_and_merge(ocr_data, cleaned_phrases):\n",
    "    # Create phrase objects with metadata\n",
    "    phrases = []\n",
    "    for orig, clean in zip(ocr_data.keys(), cleaned_phrases):\n",
    "        data = ocr_data[orig]\n",
    "        phrases.append({\n",
    "            'original': orig,\n",
    "            'clean': clean,\n",
    "            'normalized': normalize_text(clean),\n",
    "            'count': data['count'],\n",
    "            'timestamps': data.get('timestamps', []),\n",
    "            'positions': data.get('positions', [])\n",
    "        })\n",
    "\n",
    "    # Sort by frequency (most common first) and timestamp (earliest first)\n",
    "    phrases.sort(key=lambda x: (-x['count'], min(x['timestamps']) if x['timestamps'] else float('inf')))\n",
    "\n",
    "    merged = []\n",
    "    skip_indices = set()\n",
    "\n",
    "    for i, phrase1 in enumerate(phrases):\n",
    "        if i in skip_indices:\n",
    "            continue\n",
    "\n",
    "        # Start with this phrase as the canonical version\n",
    "        canonical = phrase1.copy()\n",
    "\n",
    "        # Check against remaining phrases\n",
    "        for j in range(i + 1, len(phrases)):\n",
    "            if j in skip_indices:\n",
    "                continue\n",
    "\n",
    "            phrase2 = phrases[j]\n",
    "\n",
    "            # Calculate similarity\n",
    "            similarity = text_similarity(phrase1['clean'], phrase2['clean'])\n",
    "\n",
    "            # Merge if very similar (likely same text with OCR errors)\n",
    "            if similarity > 0.85:\n",
    "                # Choose the better version (longer, more common, or earlier)\n",
    "                if len(phrase2['clean']) > len(canonical['clean']):\n",
    "                    canonical['clean'] = phrase2['clean']\n",
    "                    canonical['normalized'] = phrase2['normalized']\n",
    "\n",
    "                # Merge metadata\n",
    "                canonical['count'] += phrase2['count']\n",
    "                canonical['timestamps'].extend(phrase2['timestamps'])\n",
    "                canonical['positions'].extend(phrase2['positions'])\n",
    "\n",
    "                skip_indices.add(j)\n",
    "\n",
    "            # Check if one is substring of another\n",
    "            elif canonical['normalized'] in phrase2['normalized']:\n",
    "                # phrase1 is substring of phrase2, keep phrase2's text\n",
    "                canonical['clean'] = phrase2['clean']\n",
    "                canonical['normalized'] = phrase2['normalized']\n",
    "                canonical['count'] += phrase2['count']\n",
    "                canonical['timestamps'].extend(phrase2['timestamps'])\n",
    "                canonical['positions'].extend(phrase2['positions'])\n",
    "                skip_indices.add(j)\n",
    "\n",
    "            elif phrase2['normalized'] in canonical['normalized']:\n",
    "                # phrase2 is substring of phrase1, keep canonical and merge counts\n",
    "                canonical['count'] += phrase2['count']\n",
    "                canonical['timestamps'].extend(phrase2['timestamps'])\n",
    "                canonical['positions'].extend(phrase2['positions'])\n",
    "                skip_indices.add(j)\n",
    "\n",
    "        # Clean up merged data\n",
    "        canonical['timestamps'] = sorted(set(canonical['timestamps']))[:10]\n",
    "        canonical['positions'] = list(set(map(tuple, canonical['positions'])))[:10]\n",
    "\n",
    "        merged.append(canonical)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8fce8",
   "metadata": {},
   "source": [
    "**Final ASR + OCR string assembly and export to CSV step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6582fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_final_text(merged_phrases, api_key, model=\"gpt-4o-mini\"):\n",
    "    if not merged_phrases:\n",
    "        return \"\"\n",
    "\n",
    "    # Sort by timestamp (chronological order)\n",
    "    sorted_phrases = sorted(merged_phrases,\n",
    "                           key=lambda x: min(x['timestamps']) if x['timestamps'] else float('inf'))\n",
    "\n",
    "    # Simple fallback assembly\n",
    "    simple_assembly = \" \".join(p['clean'] for p in sorted_phrases)\n",
    "\n",
    "    # Try LLM assembly for better coherence\n",
    "    try:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "\n",
    "        phrases_list = [p['clean'] for p in sorted_phrases]\n",
    "\n",
    "        system_prompt = \"\"\"You are a Gen-Z person familiar with Tiktok trends assembling OCR text fragments into one coherent sentence or phrase.\n",
    "\n",
    "Rules:\n",
    "1. Arrange the fragments by timestamp; if it doesn't make sense, then you can rearrange it minimally.\n",
    "2. Remove duplicate or very similar fragments\n",
    "3. Add minimal punctuation ONLY where clearly needed\n",
    "4. Do NOT add new words or rephrase or change existing words\n",
    "5. Preserve all hashtags\n",
    "6. Output ONE clean line of text\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Assemble these OCR fragments in order into one coherent line:\n",
    "\n",
    "{chr(10).join(f'{i+1}. {p}' for i, p in enumerate(phrases_list))}\n",
    "\n",
    "Assembled text:\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=200\n",
    "        )\n",
    "\n",
    "        result = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Remove quotes if present\n",
    "        if (result.startswith('\"') and result.endswith('\"')) or \\\n",
    "           (result.startswith(\"'\") and result.endswith(\"'\")):\n",
    "            result = result[1:-1]\n",
    "\n",
    "        # Validate result isn't too different from source material\n",
    "        if result and len(result) > 10 and len(result) < len(simple_assembly) * 2:\n",
    "            print(\"Using LLM-assembled text\")\n",
    "            return result\n",
    "        else:\n",
    "            print(\"LLM assembly invalid, using simple assembly\")\n",
    "            return simple_assembly\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LLM assembly failed ({e}), using simple assembly\")\n",
    "        return simple_assembly\n",
    "    \n",
    "def create_output_csv(asr_text, merged_phrases, final_text, output_csv):\n",
    "    rows = []\n",
    "\n",
    "    # Add ASR\n",
    "    if asr_text:\n",
    "        rows.append({\n",
    "            \"source\": \"ASR\",\n",
    "            \"text\": asr_text,\n",
    "            \"count\": 1,\n",
    "            \"timestamps\": \"[]\",\n",
    "            \"original_text\": \"\"\n",
    "        })\n",
    "\n",
    "    # Add individual OCR phrases\n",
    "    for phrase in merged_phrases:\n",
    "        rows.append({\n",
    "            \"source\": \"OCR_PHRASE\",\n",
    "            \"text\": phrase['clean'],\n",
    "            \"count\": phrase['count'],\n",
    "            \"timestamps\": json.dumps(phrase['timestamps'][:5]),\n",
    "            \"original_text\": phrase['original']\n",
    "        })\n",
    "\n",
    "    # Add final assembled text\n",
    "    if final_text:\n",
    "        rows.append({\n",
    "            \"source\": \"OCR_FINAL\",\n",
    "            \"text\": final_text,\n",
    "            \"count\": sum(p['count'] for p in merged_phrases),\n",
    "            \"timestamps\": \"[]\",\n",
    "            \"original_text\": \"\"\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ab036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovering videos...\n",
      "\n",
      "Found 3 video(s).\n",
      " - ./media\\trend1vid1.mp4\n",
      " - ./media\\trend1vid2.mp4\n",
      " - ./media\\trend3vid1.mp4\n",
      "\n",
      "\n",
      "==============================\n",
      "Processing video: ./media\\trend1vid1.mp4\n",
      "==============================\n",
      "\n",
      "=== STEP 1: Audio Transcription ===\n",
      "ASR Result: Music  Music  Music  Music\n",
      "\n",
      "=== STEP 2: OCR Extraction ===\n",
      "Processing video frames for OCR...\n",
      "OCR processing complete. Found 4 unique text phrases.\n",
      "Extracted 4 unique text phrases\n",
      "\n",
      "=== STEP 3: OCR Cleaning ===\n",
      "Cleaning 4 OCR phrases with gpt-4o-mini...\n",
      "Cleaning complete! Total cost: $0.0002\n",
      "\n",
      "OCR Corrections (sample):\n",
      "  ✓ 'POV: Yourememberyou' → 'pov: you remember you'\n",
      "  ✓ 'havefreewill' → 'have free will'\n",
      "  ✓ 'havefreewll' → 'have free will'\n",
      "  ✓ 'have freewill' → 'have free will'\n",
      "\n",
      "=== STEP 4: Deduplication & Merging ===\n",
      "Consolidated to 2 unique phrases\n",
      "\n",
      "=== STEP 5: Final Text Assembly ===\n",
      "Using LLM-assembled text\n",
      "Final Text: pov: you remember you have free will\n",
      "\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Allocated: 413.41748046875 MB\n",
      "Cached:    1054.0 MB\n",
      "\n",
      "==============================\n",
      "Processing video: ./media\\trend1vid2.mp4\n",
      "==============================\n",
      "\n",
      "=== STEP 1: Audio Transcription ===\n",
      "ASR Result: Music  I never could believe the things you do to me  I never could believe the way you are  Every day I'll bless the day that you got through to me  But maybe I'll believe that you'll stop  Yes, we w...\n",
      "\n",
      "=== STEP 2: OCR Extraction ===\n",
      "Processing video frames for OCR...\n",
      "Processed 10 sampled frames...\n",
      "Processed 20 sampled frames...\n",
      "Processed 30 sampled frames...\n",
      "Processed 40 sampled frames...\n",
      "Processed 50 sampled frames...\n",
      "Processed 60 sampled frames...\n",
      "OCR processing complete. Found 22 unique text phrases.\n",
      "Extracted 22 unique text phrases\n",
      "\n",
      "=== STEP 3: OCR Cleaning ===\n",
      "Cleaning 22 OCR phrases with gpt-4o-mini...\n",
      "  Cleaned 10/22 (Cost: $0.0005)\n",
      "  Cleaned 20/22 (Cost: $0.0009)\n",
      "Cleaning complete! Total cost: $0.0010\n",
      "\n",
      "OCR Corrections (sample):\n",
      "  ✓ 'Ifree will and no one can stop' → 'ifree will and no one can stop'\n",
      "  ✓ 'YOU from makinghome made' → 'you from making home made'\n",
      "  ✓ 'VOU from making home made' → 'you from making home made'\n",
      "  ✓ 'OU from makinghomemad' → 'ou from making homemade'\n",
      "  ✓ 'Jfree will and no one can stop' → 'jfree will and no one can stop'\n",
      "  ✓ 'VOU from makinghomemade' → 'you from makinghomemade'\n",
      "  ✓ 'VOU from makinghome made' → 'you from making home made'\n",
      "\n",
      "=== STEP 4: Deduplication & Merging ===\n",
      "Consolidated to 12 unique phrases\n",
      "\n",
      "=== STEP 5: Final Text Assembly ===\n",
      "Using LLM-assembled text\n",
      "Final Text: pov: you remember you have whole mini corn dogs you from making home made jfree will and no one can stop #bl #jffy #cm #yorr #ju #pigo #fi\n",
      "\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Allocated: 413.41748046875 MB\n",
      "Cached:    1054.0 MB\n",
      "\n",
      "==============================\n",
      "Processing video: ./media\\trend3vid1.mp4\n",
      "==============================\n",
      "\n",
      "=== STEP 1: Audio Transcription ===\n",
      "ASR Result: Waiting for the box to hit me?  Well, I'm still waiting for you to fill up the book of viewing form.  Still waiting?\n",
      "\n",
      "=== STEP 2: OCR Extraction ===\n",
      "Processing video frames for OCR...\n",
      "Processed 10 sampled frames...\n",
      "OCR processing complete. Found 2 unique text phrases.\n",
      "Extracted 2 unique text phrases\n",
      "\n",
      "=== STEP 3: OCR Cleaning ===\n",
      "Cleaning 2 OCR phrases with gpt-4o-mini...\n",
      "Cleaning complete! Total cost: $0.0001\n",
      "\n",
      "OCR Corrections (sample):\n",
      "  ✓ 'WAItinC FOR THE ' → 'waiting for the'\n",
      "  ✓ 'BOOK AVIEWINC EORMI' → 'book aviewinc eormi'\n",
      "\n",
      "=== STEP 4: Deduplication & Merging ===\n",
      "Consolidated to 2 unique phrases\n",
      "\n",
      "=== STEP 5: Final Text Assembly ===\n",
      "Using LLM-assembled text\n",
      "Final Text: waiting for the book aviewinc eormi\n",
      "\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Allocated: 413.41748046875 MB\n",
      "Cached:    1054.0 MB\n",
      "\n",
      "✓ Batch results saved to: video_text_outputs.csv\n",
      "Total videos processed: 3\n"
     ]
    }
   ],
   "source": [
    "def _final_cleaned_phrase_list(merged_phrases):\n",
    "    # Order by earliest timestamp first\n",
    "    ordered = sorted(\n",
    "        merged_phrases,\n",
    "        key=lambda x: min(x['timestamps']) if x.get('timestamps') else float('inf')\n",
    "    )\n",
    "\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in ordered:\n",
    "        key = normalize_text(p.get('clean', ''))\n",
    "        if key and key not in seen:\n",
    "            out.append(p['clean'])\n",
    "            seen.add(key)\n",
    "    return out\n",
    "\n",
    "def _process_one_video(video_path):\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"==============================\\n\")\n",
    "\n",
    "    # === STEP 1: ASR (unchanged) ===\n",
    "    print(\"=== STEP 1: Audio Transcription ===\")\n",
    "    asr_text = extract_audio_with_whisper(video_path)\n",
    "    print(f\"ASR Result: {asr_text[:200]}{'...' if len(asr_text) > 200 else ''}\\n\")\n",
    "\n",
    "    # === STEP 2: OCR (unchanged) ===\n",
    "    print(\"=== STEP 2: OCR Extraction ===\")\n",
    "    ocr_data = extract_ocr_from_video(video_path, sample_rate_fps=1)\n",
    "    print(f\"Extracted {len(ocr_data)} unique text phrases\\n\")\n",
    "\n",
    "    # === STEP 3: Clean OCR (unchanged) ===\n",
    "    print(\"=== STEP 3: OCR Cleaning ===\")\n",
    "    cleaned_phrases = clean_ocr_with_openai(ocr_data, OPENAI_API_KEY)\n",
    "\n",
    "    print(\"\\nOCR Corrections (sample):\")\n",
    "    for orig, clean in list(zip(ocr_data.keys(), cleaned_phrases))[:10]:\n",
    "        if orig != clean:\n",
    "            print(f\"  ✓ '{orig}' → '{clean}'\")\n",
    "    print()\n",
    "\n",
    "    # === STEP 4: Dedup & Merge (unchanged) ===\n",
    "    print(\"=== STEP 4: Deduplication & Merging ===\")\n",
    "    merged_phrases = smart_deduplicate_and_merge(ocr_data, cleaned_phrases)\n",
    "    print(f\"Consolidated to {len(merged_phrases)} unique phrases\\n\")\n",
    "\n",
    "    # === STEP 5: Final Assembly (unchanged) ===\n",
    "    print(\"=== STEP 5: Final Text Assembly ===\")\n",
    "    final_text = assemble_final_text(merged_phrases, OPENAI_API_KEY)\n",
    "    print(f\"Final Text: {final_text}\\n\")\n",
    "\n",
    "    return asr_text, merged_phrases, final_text\n",
    "\n",
    "def main():\n",
    "    print(\"Discovering videos...\\n\")\n",
    "\n",
    "    MEDIA_FOLDER = r\"./media\"\n",
    "\n",
    "    video_paths = _fetch_videos_from_folder(MEDIA_FOLDER)\n",
    "    if not video_paths:\n",
    "        print(\"No videos found. Please check the media folder\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(video_paths)} video(s).\")\n",
    "    for v in video_paths:\n",
    "        print(\" -\", v)\n",
    "    print()\n",
    "\n",
    "    rows = []\n",
    "    for vp in video_paths:\n",
    "        asr_text, merged_phrases, final_text = _process_one_video(vp)\n",
    "        phrases_list = _final_cleaned_phrase_list(merged_phrases)  # simple list of final cleaned phrases\n",
    "\n",
    "        rows.append({\n",
    "            \"video\": os.path.basename(vp),\n",
    "            \"asr\": asr_text,\n",
    "            \"ocr_final\": final_text,\n",
    "            \"cleaned_phrases\": json.dumps(phrases_list, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"video\", \"asr\", \"ocr_final\", \"cleaned_phrases\"])\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n✓ Batch results saved to: {OUTPUT_CSV}\")\n",
    "    print(f\"Total videos processed: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bcfc7",
   "metadata": {},
   "source": [
    "**Concatenation and embedding extraction step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb045193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bf3c38b",
   "metadata": {},
   "source": [
    "---\n",
    "## **RETRIEVING SIMILAR VIDEOS**\n",
    "**Goal**: Produce a list of most similar videos based on a weighted combination of modality-specific cosine similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a2252",
   "metadata": {},
   "source": [
    "**Embedding loading step:** creates a dict of embedding vectors following the below format to keep everything organized and so embedding retrieval for each video is trivial.\n",
    "$$\n",
    "video\\_name \\;\\rightarrow\\; \\{ audio,\\; video,\\; text \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_embeddings(base_dir=\"embeddings_out\"):\n",
    "    base_dir = Path(base_dir)\n",
    "\n",
    "    folders = {\n",
    "        \"audio\":  base_dir / \"audio2048\",\n",
    "        \"visual\": base_dir / \"video2048\",\n",
    "        \"text\":   base_dir / \"text768\",\n",
    "    }\n",
    "\n",
    "    suffix_map = {\n",
    "        \"audio\":  \"audio2048\",\n",
    "        \"visual\": \"visual2048\",\n",
    "        \"text\":   \"text768\",\n",
    "    }\n",
    "\n",
    "    modality_files = {} # collect keys per modality\n",
    "    for modality, folder in folders.items():\n",
    "        files = list(folder.glob(f\"*emb-{suffix_map[modality]}.npy\"))\n",
    "        modality_files[modality] = {f.stem.split(\"_emb-\")[0]: f for f in files}\n",
    "\n",
    "    all_video_ids = set()\n",
    "    for d in modality_files.values():\n",
    "        all_video_ids.update(d.keys())\n",
    "\n",
    "    embeddings = {}\n",
    "    missing = []\n",
    "\n",
    "    for vid in all_video_ids:\n",
    "        embeddings[vid] = {}\n",
    "        for modality in [\"audio\", \"visual\", \"text\"]:\n",
    "            file = modality_files[modality].get(vid, None)\n",
    "            if file is None:\n",
    "                missing.append((vid, modality))\n",
    "                embeddings[vid][modality] = None\n",
    "            else:\n",
    "                embeddings[vid][modality] = np.load(str(file))\n",
    "\n",
    "    if missing:\n",
    "        print(\"WARNING: Missing modality embeddings detected:\") # just to be safe\n",
    "        for vid, modality in missing:\n",
    "            print(f\"  - {vid} missing {modality}\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings = load_all_embeddings() # get embeddings with embeddings[\"video_name\"]\n",
    "\n",
    "# to check\n",
    "for video, emb_vec in embeddings.items():\n",
    "    print(video, emb_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2822fa",
   "metadata": {},
   "source": [
    ">**NOTE: Input the query video here :))**\n",
    "\n",
    "If testing different queries with the same set of videos, just <u>run the notebook starting at this cell</u> to skip the preprocessing and loading of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80952079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please type the EXACT filename of the query video\n",
    "QUERY = \"trend5vid2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed98ad",
   "metadata": {},
   "source": [
    "**Cosine similarity computation step:** computes modality-specific cosine similarity scores for each video and a query video, resulting in each video being represented as a vector of 3 similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return None\n",
    "    \n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2)) # maybe cast to float?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modality_similarities(query_video_name: str, embeddings_dir: Path):\n",
    "    \n",
    "    if query_video_name not in embeddings:\n",
    "        raise ValueError(f\"Query video '{query_video_name}' not found in embeddings.\")\n",
    "    \n",
    "    query_emb = embeddings[query_video_name]\n",
    "    \n",
    "    similarity_dict = {}\n",
    "    \n",
    "    for video_name, video_emb in embeddings.items(): \n",
    "        if video_name == query_video_name: # skips self\n",
    "            continue \n",
    "        \n",
    "        sims = []\n",
    "        missing_modalities = []\n",
    "        \n",
    "        for modality in [\"audio\", \"visual\", \"text\"]:\n",
    "            if modality not in video_emb or modality not in query_emb:\n",
    "                missing_modalities.append(modality)\n",
    "                sims.append(np.nan)  # for missing embeddings\n",
    "            else:\n",
    "                sims.append(cosine_similarity(video_emb[modality], query_emb[modality]))\n",
    "        \n",
    "        if missing_modalities:\n",
    "            print(f\"[WARNING] {video_name} missing embeddings for: {', '.join(missing_modalities)}\")\n",
    "        \n",
    "        similarity_dict[video_name] = np.array(sims)\n",
    "    \n",
    "    return similarity_dict\n",
    "\n",
    "similarities = compute_modality_similarities(QUERY, \"embeddings_out\") # get similarity vector with similarities[\"video_name\"]\n",
    "\n",
    "# to check\n",
    "for video, sim_vec in similarities.items():\n",
    "    print(video, sim_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb7155",
   "metadata": {},
   "source": [
    ">**NOTE: Input the weights here :))**\n",
    "\n",
    "If testing different weights with the same query video and set of videos, just <u>run the notebook starting at this cell</u> to skip computing the modality-specific cosine similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_AUDIO = 1/3\n",
    "WEIGHT_VIDEO = 1/3\n",
    "WEIGHT_TEXT = 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff9c80",
   "metadata": {},
   "source": [
    "**Weighted-sum fusion step:** uses weighted linear combination to form a final similarity score for each video and a query video, where the weights can be modified according to the different test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum_fusion(similarity_dict, weight_audio, weight_video, weight_text):\n",
    "\n",
    "    weights = np.array([weight_audio, weight_video, weight_text])\n",
    "    weights = weights / weights.sum() # apparently we need to normalize this cuz it might not equal 1\n",
    "    final_weighted_dict = {}\n",
    "    \n",
    "    for video, sim_vec in similarity_dict.items():\n",
    "        if len(sim_vec) != 3:\n",
    "            raise ValueError(f\"Expected 3 modalities in similarity vector for {video}, got {len(sim_vec)}\")\n",
    "        \n",
    "        sim_audio, sim_video, sim_text = sim_vec\n",
    "        weighted_score = (sim_audio*weights[0] + sim_video*weights[1] + sim_text*weights[2])\n",
    "        final_weighted_dict[video] = float(weighted_score)\n",
    "\n",
    "    return final_weighted_dict\n",
    "\n",
    "final_scores = weighted_sum_fusion(similarities, WEIGHT_AUDIO, WEIGHT_VIDEO, WEIGHT_TEXT)\n",
    "\n",
    "# to check\n",
    "for video, score in final_scores.items():\n",
    "    print(video, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a9235",
   "metadata": {},
   "source": [
    "**Ranking step:** uses the final scores from weighted sum fusion to rank all videos by their similarity score with the query video, printed in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd1a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_by_score(final_weighted_dict, top_k=None):\n",
    "    ranked_videos = sorted(final_weighted_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if top_k is not None:\n",
    "        ranked_videos = ranked_videos[:top_k]\n",
    "    \n",
    "    return ranked_videos\n",
    "\n",
    "k = 5 # no of similar videos to retrieve\n",
    "most_similar_videos = rank_by_score(final_scores, top_k = k)\n",
    "\n",
    "print(f\"Top {k} most similar videos to {QUERY}\")\n",
    "for video, score in most_similar_videos:\n",
    "    print(f\"{video}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
