{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99223acb",
   "metadata": {},
   "source": [
    "<img src=\"../DLSU-ALTDSI-logo.png\" width=\"100%\" style=\"margin-bottom:10px; margin-top:0px;\"/>\n",
    "\n",
    "**This notebook contains the context-aware video retrieval pipeline used in the study:**\n",
    "\n",
    "## *Comparing Modality Representation Schemes in Video Retrieval for More Context-Aware Auto-Annotation of Trending Short-Form Videos*\n",
    "\n",
    "**By the following researchers from the Andrew L. Tan Data Science Institute:**\n",
    "1. Ong, Matthew Kristoffer Y. (matthew_kristoffer_ong@dlsu.edu.ph)\n",
    "2. Presas, Shanette Giane G. (shanette_giane_presas@dlsu.edu.ph)\n",
    "3. Sarreal, Sophia Althea R. (sophia_sarreal@dlsu.edu.ph)\n",
    "4. To, Jersey Jaclyn K. (jers_to@dlsu.edu.ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a61d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6f596",
   "metadata": {},
   "source": [
    "Note to thesismates:\n",
    "1. Run this to activate venv for the terminal instance: .venv\\Scripts\\activate\n",
    "2. NOTE: you will also need the ff files:\n",
    "    1. 'class_labels_indices.csv'\n",
    "    2. 'Cnn14_mAP=0.431.pth' (these are the model weights to be used) from https://zenodo.org/records/3987831"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa930e",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# audio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ffmpeg\n",
    "import torch\n",
    "import librosa\n",
    "from panns_inference import AudioTagging\n",
    "\n",
    "# visuals\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Make sure cuda (gpu) is active!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38964d22",
   "metadata": {},
   "source": [
    "---\n",
    "## **AUDIO MODALITY**\n",
    "**Goal**: Produce embeddings representing the audio modality of a given set of videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25bf68",
   "metadata": {},
   "source": [
    "**Preprocessing step:** extracts 32kHz waveform files from the input videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd08338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_to_wavs(video_path: str, out32: str, overwrite: bool=True):\n",
    "    extract_32k=(\n",
    "        ffmpeg.input(video_path).output(out32, format='wav', acodec='pcm_s16le', ac=1, ar=32000)\n",
    "    )\n",
    "    if overwrite:\n",
    "        extract_32k = extract_32k.overwrite_output()\n",
    "    \n",
    "    extract_32k.run(quiet=True)\n",
    "    print(\"Wrote 32kHz\", out32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6aefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path: str, out_dir: str =\"proc_out\"):\n",
    "    out_dir = Path(out_dir)\n",
    "    audio_dir = out_dir.parent / (out_dir.name + \"_32kHz\")\n",
    "    audio_dir.mkdir(parents=True, exist_ok=True) # 32kHz goes to audio_dir\n",
    "\n",
    "    video = Path(video_path)\n",
    "    out32 = audio_dir / (video.stem + \"_32k.wav\") # 32kHz output\n",
    "\n",
    "    # Extract audio\n",
    "    extract_audio_to_wavs(str(video), str(out32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_dir = Path(\"media\")\n",
    "videos = list(media_dir.glob(\"*.mp4\"))\n",
    "print(f\"{len(videos)} videos found!\")\n",
    "\n",
    "for video in videos:\n",
    "    print(f\"\\nProcessing: {video.name}\")\n",
    "    process_video(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f21d0d",
   "metadata": {},
   "source": [
    "**Feature extraction step:** produces embeddings in the form of a 2048-dimensional feature vector representing the audio of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ca0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_out_32kHz_dir = Path(\"proc_out_32kHz\")\n",
    "emb_out_dir = Path(\"embeddings_out/audio2048\") # 2048-d vectors go here\n",
    "emb_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "at_model = AudioTagging(checkpoint_path=None, device=device) #this is the pretrained CNN14\n",
    "\n",
    "wav_files = sorted(proc_out_32kHz_dir.glob(\"*_32k.wav\"))\n",
    "print(f\"{len(wav_files)} WAV files found!\")\n",
    "\n",
    "for wav_path in wav_files:\n",
    "    print(f\"\\nProcessing: {wav_path.name}\")\n",
    "    wav, sr = librosa.load(str(wav_path), sr=32000, mono=True) # just to make sure wav is 32kHz\n",
    "    audio_batch = np.expand_dims(wav, axis=0) # matches the expected shape of PANN\n",
    "\n",
    "    _, embedding = at_model.inference(audio_batch) # gets the embedding as numpy array\n",
    "\n",
    "    embedding_vec = embedding[0] # first element of embedding array\n",
    "\n",
    "    # just removing the \"_32k\" for filename consistency\n",
    "    stem = wav_path.stem\n",
    "    if stem.endswith(\"_32k\"):\n",
    "        stem = stem[:-4]\n",
    "\n",
    "    out_path = emb_out_dir / f\"{stem}_emb-audio2048.npy\"\n",
    "    np.save(str(out_path), embedding_vec)\n",
    "    print(\"Embedding saved: \", out_path)\n",
    "\n",
    "    print(embedding_vec) # if you want to see the vector\n",
    "    print(embedding_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213682a5",
   "metadata": {},
   "source": [
    "---\n",
    "## **VISUAL MODALITY**\n",
    "**Goal**: Produce embeddings representing the visual modality of a given set of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf09c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. SET YOUR LOCAL DIRECTORIES ---\n",
    "INPUT_DIR = Path(\"media\")\n",
    "OUTPUT_DIR = Path(\"embeddings_out/video2048\")\n",
    "\n",
    "# --- 2. SET YOUR MODEL PARAMETERS ---\n",
    "FRAME_SAMPLE_RATE = 30\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- 3. DEFINE VIDEO EXTENSIONS TO FIND ---\n",
    "VIDEO_EXTENSIONS = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a263dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_model(device: str):\n",
    "    \"\"\"Loads the pre-trained ResNet-50 model and its associated transforms.\"\"\"\n",
    "    weights = models.ResNet50_Weights.DEFAULT\n",
    "    model = models.resnet50(weights=weights)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preprocess = weights.transforms()\n",
    "    return model, preprocess\n",
    "\n",
    "model, preprocess = get_resnet_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_resnet_embeddings(\n",
    "    video_path: Path, \n",
    "    model, \n",
    "    preprocess, \n",
    "    device: str, \n",
    "    frame_sample_rate: int = 30, \n",
    "    batch_size: int = 32\n",
    ") -> np.ndarray:\n",
    "    if not video_path.exists():\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video file: {video_path}\")\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    all_features = []\n",
    "    frame_batch = []\n",
    "    frame_idx = 0\n",
    "    \n",
    "    pbar = tqdm(total=frame_count, desc=f\"Frames for {video_path.name}\", leave=True, disable=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if frame_idx % frame_sample_rate == 0:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(frame_rgb)\n",
    "                frame_batch.append(pil_img)\n",
    "\n",
    "                if len(frame_batch) == batch_size:\n",
    "                    image_inputs = torch.stack(\n",
    "                        [preprocess(img) for img in frame_batch]\n",
    "                    ).to(device)\n",
    "                    image_features = model(image_inputs)\n",
    "                    all_features.append(image_features.squeeze().cpu().numpy())\n",
    "                    frame_batch = []\n",
    "            frame_idx += 1\n",
    "        \n",
    "        if frame_batch:\n",
    "            image_inputs = torch.stack(\n",
    "                [preprocess(img) for img in frame_batch]\n",
    "            ).to(device)\n",
    "            image_features = model(image_inputs)\n",
    "            all_features.append(image_features.squeeze().cpu().numpy())\n",
    "\n",
    "    cap.release()\n",
    "    pbar.close()\n",
    "    if not all_features:\n",
    "        raise ValueError(f\"No frames sampled for {video_path.name}\")\n",
    "\n",
    "    embeddings = np.vstack(all_features)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Reading videos from: {INPUT_DIR.resolve()}\")\n",
    "print(f\"Saving embeddings to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# Find all video files\n",
    "video_files = []\n",
    "for ext in VIDEO_EXTENSIONS:\n",
    "    video_files.extend(INPUT_DIR.glob(f\"*{ext}\"))\n",
    "print(f\"Found {len(video_files)} videos.\")\n",
    "\n",
    "# Get list of files ALREADY in the output folder to skip them\n",
    "existing_embeddings = {f.name for f in OUTPUT_DIR.glob('*_resnet.npy')}\n",
    "print(f\"Found {len(existing_embeddings)} existing ResNet embeddings.\")\n",
    "\n",
    "for video_path in tqdm(video_files, desc=\"Processing Videos (ResNet)\"):\n",
    "    output_filename = f\"{video_path.stem}_emb-visual2048.npy\"\n",
    "\n",
    "    # Skip if already processed\n",
    "    if output_filename in existing_embeddings:\n",
    "        continue\n",
    "    \n",
    "    output_path = OUTPUT_DIR / output_filename\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing {video_path.name}...\")\n",
    "        mean_embedding = extract_resnet_embeddings(\n",
    "            video_path=video_path,\n",
    "            model=model,\n",
    "            preprocess=preprocess,\n",
    "            device=device,\n",
    "            frame_sample_rate=FRAME_SAMPLE_RATE,\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "        np.save(output_path, mean_embedding)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Failed to process {video_path.name}: {e}\")\n",
    "\n",
    "print(\"\\n--- Batch processing complete. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7365085",
   "metadata": {},
   "source": [
    "---\n",
    "## **TEXT MODALITY**\n",
    "**Goal**: Produce embeddings representing the text modality of a given set of videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3c38b",
   "metadata": {},
   "source": [
    "---\n",
    "## **RETRIEVING SIMILAR VIDEOS**\n",
    "**Goal**: Produce a list of most similar videos based on a weighted combination of modality-specific cosine similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f8cbe8",
   "metadata": {},
   "source": [
    "notes:\n",
    "1. Get query video\n",
    "2. Compute modality-specific cosine similarity scores for each video and the query, resulting in each video being represented as a vector of 3 similarity scores\n",
    "3. Weighted linear combination code with the 3 similarity scores forming one final similarity score for each video and the query, where I can change the weights to easily test out different test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed98ad",
   "metadata": {},
   "source": [
    "**Cosine similarity computation step:** computes modality-specific cosine similarity scores for each video and a query video, resulting in each video being represented as a vector of 3 similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff9c80",
   "metadata": {},
   "source": [
    "**Weighted-sum fusion step:** uses weighted linear combination to form a final similarity score for each video and a query video, where the weights can be modified according to the different test cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
