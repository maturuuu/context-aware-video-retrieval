{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99223acb",
   "metadata": {},
   "source": [
    "<img src=\"../DLSU-ALTDSI-logo.png\" width=\"100%\" style=\"margin-bottom:10px; margin-top:0px;\"/>\n",
    "\n",
    "**This notebook contains the context-aware video retrieval pipeline used in the study:**\n",
    "\n",
    "## *Comparing Modality Representation Schemes in Video Retrieval for More Context-Aware Auto-Annotation of Trending Short-Form Videos*\n",
    "\n",
    "**By the following researchers from the Andrew L. Tan Data Science Institute:**\n",
    "1. Ong, Matthew Kristoffer Y. (matthew_kristoffer_ong@dlsu.edu.ph)\n",
    "2. Presas, Shanette Giane G. (shanette_giane_presas@dlsu.edu.ph)\n",
    "3. Sarreal, Sophia Althea R. (sophia_sarreal@dlsu.edu.ph)\n",
    "4. To, Jersey Jaclyn K. (jers_to@dlsu.edu.ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a61d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6f596",
   "metadata": {},
   "source": [
    "Note to thesismates:\n",
    "1. Navigate first to the similarity pipeline folder\n",
    "2. Run this to activate venv for the terminal instance: .venv\\Scripts\\activate\n",
    "3. NOTE: you will also need the ff files:\n",
    "    1. 'class_labels_indices.csv'\n",
    "    2. 'Cnn14_mAP=0.431.pth' (these are the model weights to be used) from https://zenodo.org/records/3987831\n",
    "    3. This specific torchaudio/torchvision model: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa930e",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# audio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ffmpeg\n",
    "import torch\n",
    "import librosa\n",
    "from panns_inference import AudioTagging\n",
    "\n",
    "# visuals\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#text\n",
    "import easyocr\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "import string\n",
    "from ftfy import fix_text\n",
    "from wordsegment import load as ws_load, segment as ws_segment\n",
    "from spellchecker import SpellChecker\n",
    "from transformers import pipeline\n",
    "import wordninja\n",
    "from wordfreq import zipf_frequency\n",
    "import glob\n",
    "import pathlib\n",
    "import subprocess\n",
    "import sys\n",
    "from faster_whisper import WhisperModel\n",
    "from openai import OpenAI\n",
    "from difflib import SequenceMatcher\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#similarity\n",
    "from numpy.linalg import norm\n",
    "\n",
    "#gemini\n",
    "import getpass\n",
    "from typing import List, Dict, Any\n",
    "import google.generativeai as genai\n",
    "\n",
    "import faulthandler\n",
    "faulthandler.enable()\n",
    "\n",
    "# Make sure cuda (gpu) is active!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38964d22",
   "metadata": {},
   "source": [
    "---\n",
    "## **AUDIO MODALITY**\n",
    "**Goal**: Produce embeddings representing the audio modality of a given set of videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25bf68",
   "metadata": {},
   "source": [
    "**Preprocessing step:** extracts 32kHz waveform files from the input videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd08338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_to_wavs(video_path: str, out32: str, overwrite: bool=True):\n",
    "    extract_32k=(\n",
    "        ffmpeg.input(video_path).output(out32, format='wav', acodec='pcm_s16le', ac=1, ar=32000)\n",
    "    )\n",
    "    if overwrite:\n",
    "        extract_32k = extract_32k.overwrite_output()\n",
    "    \n",
    "    extract_32k.run(quiet=True)\n",
    "    print(\"Wrote 32kHz\", out32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6aefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path: str, out_dir: str =\"proc_out\"):\n",
    "    out_dir = Path(out_dir)\n",
    "    audio_dir = out_dir.parent / (out_dir.name + \"_32kHz\")\n",
    "    audio_dir.mkdir(parents=True, exist_ok=True) # 32kHz goes to audio_dir\n",
    "\n",
    "    video = Path(video_path)\n",
    "    out32 = audio_dir / (video.stem + \"_32k.wav\") # 32kHz output\n",
    "\n",
    "    # Extract audio\n",
    "    extract_audio_to_wavs(str(video), str(out32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_dir = Path(\"media\")\n",
    "videos = list(media_dir.glob(\"*.mp4\"))\n",
    "print(f\"{len(videos)} videos found!\")\n",
    "\n",
    "for video in videos:\n",
    "    print(f\"\\nProcessing: {video.name}\")\n",
    "    process_video(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f21d0d",
   "metadata": {},
   "source": [
    "**Feature extraction step:** produces embeddings in the form of a 2048-dimensional feature vector representing the audio of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ca0fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proc_out_32kHz_dir = Path(\"proc_out_32kHz\")\n",
    "emb_out_dir = Path(\"embeddings_out/audio2048\") # 2048-d vectors go here\n",
    "emb_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "at_model = AudioTagging(checkpoint_path=None, device=device) #this is the pretrained CNN14\n",
    "\n",
    "wav_files = sorted(proc_out_32kHz_dir.glob(\"*_32k.wav\"))\n",
    "print(f\"{len(wav_files)} WAV files found!\")\n",
    "\n",
    "for wav_path in wav_files:\n",
    "    print(f\"\\nProcessing: {wav_path.name}\")\n",
    "    wav, sr = librosa.load(str(wav_path), sr=32000, mono=True) # just to make sure wav is 32kHz\n",
    "    audio_batch = np.expand_dims(wav, axis=0) # matches the expected shape of PANN\n",
    "\n",
    "    _, embedding = at_model.inference(audio_batch) # gets the embedding as numpy array\n",
    "\n",
    "    embedding_vec = embedding[0] # first element of embedding array\n",
    "\n",
    "    # just removing the \"_32k\" for filename consistency\n",
    "    stem = wav_path.stem\n",
    "    if stem.endswith(\"_32k\"):\n",
    "        stem = stem[:-4]\n",
    "\n",
    "    out_path = emb_out_dir / f\"{stem}_emb-audio2048.npy\"\n",
    "    np.save(str(out_path), embedding_vec)\n",
    "    print(\"Embedding saved: \", out_path)\n",
    "\n",
    "    print(embedding_vec) # if you want to see the vector\n",
    "    print(embedding_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213682a5",
   "metadata": {},
   "source": [
    "---\n",
    "## **VISUAL MODALITY**\n",
    "**Goal**: Produce embeddings representing the visual modality of a given set of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf09c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. SET YOUR LOCAL DIRECTORIES ---\n",
    "INPUT_DIR = Path(\"media\")\n",
    "OUTPUT_DIR = Path(\"embeddings_out/video2048\")\n",
    "\n",
    "# --- 2. SET YOUR MODEL PARAMETERS ---\n",
    "FRAME_SAMPLE_RATE = 30\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- 3. DEFINE VIDEO EXTENSIONS TO FIND ---\n",
    "VIDEO_EXTENSIONS = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a263dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_model(device: str):\n",
    "    \"\"\"Loads the pre-trained ResNet-50 model and its associated transforms.\"\"\"\n",
    "    weights = models.ResNet50_Weights.DEFAULT\n",
    "    model = models.resnet50(weights=weights)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preprocess = weights.transforms()\n",
    "    return model, preprocess\n",
    "\n",
    "model, preprocess = get_resnet_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_resnet_embeddings(\n",
    "    video_path: Path, \n",
    "    model, \n",
    "    preprocess, \n",
    "    device: str, \n",
    "    frame_sample_rate: int = 30, \n",
    "    batch_size: int = 32\n",
    ") -> np.ndarray:\n",
    "    if not video_path.exists():\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video file: {video_path}\")\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    all_features = []\n",
    "    frame_batch = []\n",
    "    frame_idx = 0\n",
    "    \n",
    "    pbar = tqdm(total=frame_count, desc=f\"Frames for {video_path.name}\", leave=True, disable=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if frame_idx % frame_sample_rate == 0:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(frame_rgb)\n",
    "                frame_batch.append(pil_img)\n",
    "\n",
    "                if len(frame_batch) == batch_size:\n",
    "                    image_inputs = torch.stack(\n",
    "                        [preprocess(img) for img in frame_batch]\n",
    "                    ).to(device)\n",
    "                    image_features = model(image_inputs)\n",
    "                    all_features.append(image_features.squeeze().cpu().numpy())\n",
    "                    frame_batch = []\n",
    "            frame_idx += 1\n",
    "        \n",
    "        if frame_batch:\n",
    "            image_inputs = torch.stack(\n",
    "                [preprocess(img) for img in frame_batch]\n",
    "            ).to(device)\n",
    "            image_features = model(image_inputs)\n",
    "            all_features.append(image_features.squeeze().cpu().numpy())\n",
    "\n",
    "    cap.release()\n",
    "    pbar.close()\n",
    "    if not all_features:\n",
    "        raise ValueError(f\"No frames sampled for {video_path.name}\")\n",
    "\n",
    "    embeddings = np.vstack(all_features)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Reading videos from: {INPUT_DIR.resolve()}\")\n",
    "print(f\"Saving embeddings to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# Find all video files\n",
    "video_files = []\n",
    "for ext in VIDEO_EXTENSIONS:\n",
    "    video_files.extend(INPUT_DIR.glob(f\"*{ext}\"))\n",
    "print(f\"Found {len(video_files)} videos.\")\n",
    "\n",
    "# Get list of files ALREADY in the output folder to skip them\n",
    "existing_embeddings = {f.name for f in OUTPUT_DIR.glob('*_resnet.npy')}\n",
    "print(f\"Found {len(existing_embeddings)} existing ResNet embeddings.\")\n",
    "\n",
    "for video_path in tqdm(video_files, desc=\"Processing Videos (ResNet)\"):\n",
    "    output_filename = f\"{video_path.stem}_emb-visual2048.npy\"\n",
    "\n",
    "    # Skip if already processed\n",
    "    if output_filename in existing_embeddings:\n",
    "        continue\n",
    "    \n",
    "    output_path = OUTPUT_DIR / output_filename\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing {video_path.name}...\")\n",
    "        mean_embedding = extract_resnet_embeddings(\n",
    "            video_path=video_path,\n",
    "            model=model,\n",
    "            preprocess=preprocess,\n",
    "            device=device,\n",
    "            frame_sample_rate=FRAME_SAMPLE_RATE,\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "        np.save(output_path, mean_embedding)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Failed to process {video_path.name}: {e}\")\n",
    "\n",
    "print(\"\\n--- Batch processing complete. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7365085",
   "metadata": {},
   "source": [
    "---\n",
    "## **TEXT MODALITY**\n",
    "**Goal**: Produce embeddings representing the text modality of a given set of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce55d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CSV = \"video_text_outputs.csv\"\n",
    "WHISPER_MODEL = WhisperModel(\"base\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                    compute_type=\"int8_float16\" if torch.cuda.is_available() else \"int8\")\n",
    "OPENAI_API_KEY = \"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\"\n",
    "# OPENAI_API_KEY = \"\" # test w/o consuming tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _list_local_videos(root_dir):\n",
    "    exts = ('.mp4', '.mov', '.m4v', '.mkv', '.avi', '.webm')\n",
    "    paths = []\n",
    "    for ext in exts:\n",
    "        paths.extend(glob.glob(os.path.join(root_dir, f\"**/*{ext}\"), recursive=True))\n",
    "    return sorted(paths)\n",
    "\n",
    "def _fetch_videos_from_folder(folder_path):\n",
    "    if not folder_path or folder_path.strip() == \"\":\n",
    "        raise ValueError(\"Folder path is empty.\")\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        return _list_local_videos(folder_path)\n",
    "\n",
    "    raise ValueError(f\"Not a valid local directory: {folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee87f5",
   "metadata": {},
   "source": [
    "**Automatic speech recognition (ASR) step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2790103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_with_whisper(video_path):\n",
    "    try:\n",
    "        segments, _ = WHISPER_MODEL.transcribe(video_path, beam_size=1)\n",
    "        return \" \".join(s.text for s in segments).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"ASR Error: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f07a7",
   "metadata": {},
   "source": [
    "**Optical character recognition (OCR) step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ccb50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_text(text):\n",
    "    if not text or len(text.strip()) < 2:\n",
    "        return False\n",
    "    clean = re.sub(r'[^\\w#@]', '', text)\n",
    "    return len(clean) > 0\n",
    "\n",
    "def preprocess_frame_for_ocr(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "    denoised = cv2.GaussianBlur(enhanced, (3, 3), 0)\n",
    "    kernel = np.array([[-1,-1,-1], [-1, 9,-1], [-1,-1,-1]])\n",
    "    sharpened = cv2.filter2D(denoised, -1, kernel)\n",
    "    return sharpened\n",
    "\n",
    "def extract_ocr_from_video(video_path, sample_rate_fps=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Cannot open video {video_path}\")\n",
    "        return {}\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "    frame_interval = max(1, int(round(fps / max(0.1, sample_rate_fps))))\n",
    "\n",
    "    reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "    text_detections = defaultdict(lambda: {'count': 0, 'timestamps': [], 'positions': []})\n",
    "\n",
    "    print(\"Processing video frames for OCR...\")\n",
    "    processed = 0\n",
    "\n",
    "    for frame_idx in range(0, total_frames, frame_interval):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame is None:\n",
    "            continue\n",
    "\n",
    "        h, w = frame.shape[:2]\n",
    "        max_w = 960\n",
    "        if w > max_w:\n",
    "            scale = max_w / float(w)\n",
    "            frame = cv2.resize(frame, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        timestamp = frame_idx / fps\n",
    "\n",
    "        try:\n",
    "            frame_pp = preprocess_frame_for_ocr(frame)\n",
    "            results = reader.readtext(frame_pp, detail=1, paragraph=False)\n",
    "            for (bbox, text, confidence) in results:\n",
    "                if confidence > 0.5 and is_valid_text(text):\n",
    "                    xs = [p[0] for p in bbox]\n",
    "                    ys = [p[1] for p in bbox]\n",
    "                    x_left = float(min(xs))\n",
    "                    y_top = float(min(ys))\n",
    "\n",
    "                    text_detections[text]['count'] += 1\n",
    "                    if len(text_detections[text]['timestamps']) < 5:\n",
    "                        text_detections[text]['timestamps'].append(round(timestamp, 2))\n",
    "                    if len(text_detections[text]['positions']) < 5:\n",
    "                        text_detections[text]['positions'].append((round(y_top, 2), round(x_left, 2)))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"OCR error at frame {frame_idx}: {e}\")\n",
    "\n",
    "        processed += 1\n",
    "        if processed % 10 == 0:\n",
    "            print(f\"Processed {processed} sampled frames...\")\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"OCR processing complete. Found {len(text_detections)} unique text phrases.\")\n",
    "    return dict(text_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ff2d9",
   "metadata": {},
   "source": [
    "**OCR cleaning step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8304a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ocr_with_openai(ocr_phrases, api_key, model=\"gpt-4o-mini\"):\n",
    "    phrases = list(ocr_phrases.keys())\n",
    "    if not phrases:\n",
    "        return []\n",
    "\n",
    "    print(f\"Cleaning {len(phrases)} OCR phrases with {model}...\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    system_prompt = \"\"\"You are an OCR error correction assistant. Fix only obvious OCR mistakes.\n",
    "\n",
    "Common OCR errors:\n",
    "- Character confusion: 'v' → 'y', 'rn' → 'm', '0' → 'O', 'i' → 'l', 'vv' → 'w', '@' → 'a', '@' → 'o'\n",
    "- Missing spaces: 'helloworld' → 'hello world'\n",
    "- Extra spaces: 'hel lo' → 'hello'\n",
    "\n",
    "Rules:\n",
    "1. ONLY fix clear OCR errors - do not rephrase or change meaning\n",
    "2. Preserve hashtags (#) exactly\n",
    "3. Keep original capitalization only for proper nouns, otherwise make everything lowercase.\n",
    "4. Output ONLY the corrected text (no quotes, explanations, or extra words)\n",
    "5. If a word has the letter 'v' in it and it looks misspelled, try swapping the 'v' with a 'y' to see if it makes more sense, and vice versa.\n",
    "6. If a word other than \"I\" has the letter 'i' in it and it looks misspelled, try swapping the 'i' with a 'l' to see if it makes more sense, and vice versa.\n",
    "7. Unless an acronym makes sense in the context, make it lowercase.\"\"\"\n",
    "\n",
    "    cleaned = []\n",
    "    total_cost = 0.0\n",
    "\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            print(f\"  Cleaned {i}/{len(phrases)} (Cost: ${total_cost:.4f})\")\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"Fix OCR errors: {phrase}\"}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=100\n",
    "            )\n",
    "\n",
    "            result = response.choices[0].message.content.strip()\n",
    "\n",
    "            # Track cost\n",
    "            if hasattr(response, 'usage') and response.usage:\n",
    "                input_tok = response.usage.prompt_tokens or 0\n",
    "                output_tok = response.usage.completion_tokens or 0\n",
    "                total_cost += (input_tok * 0.15 + output_tok * 0.60) / 1_000_000\n",
    "\n",
    "            # Remove quotes if added\n",
    "            if (result.startswith('\"') and result.endswith('\"')) or \\\n",
    "               (result.startswith(\"'\") and result.endswith(\"'\")):\n",
    "                result = result[1:-1]\n",
    "\n",
    "            # Fallback if result is empty or way too different\n",
    "            if not result or len(result) > len(phrase) * 3:\n",
    "                result = phrase\n",
    "\n",
    "            cleaned.append(result.strip())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error cleaning '{phrase}': {e}\")\n",
    "            cleaned.append(phrase)\n",
    "\n",
    "    print(f\"Cleaning complete! Total cost: ${total_cost:.4f}\")\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828ed7f",
   "metadata": {},
   "source": [
    "**Deduplication and merge step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b795f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text)  # Keep hashtags and mentions\n",
    "    return re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "def text_similarity(s1, s2):\n",
    "    return SequenceMatcher(None, normalize_text(s1), normalize_text(s2)).ratio()\n",
    "\n",
    "def smart_deduplicate_and_merge(ocr_data, cleaned_phrases):\n",
    "    # Create phrase objects with metadata\n",
    "    phrases = []\n",
    "    for orig, clean in zip(ocr_data.keys(), cleaned_phrases):\n",
    "        data = ocr_data[orig]\n",
    "        phrases.append({\n",
    "            'original': orig,\n",
    "            'clean': clean,\n",
    "            'normalized': normalize_text(clean),\n",
    "            'count': data['count'],\n",
    "            'timestamps': data.get('timestamps', []),\n",
    "            'positions': data.get('positions', [])\n",
    "        })\n",
    "\n",
    "    # Sort by frequency (most common first) and timestamp (earliest first)\n",
    "    phrases.sort(key=lambda x: (-x['count'], min(x['timestamps']) if x['timestamps'] else float('inf')))\n",
    "\n",
    "    merged = []\n",
    "    skip_indices = set()\n",
    "\n",
    "    for i, phrase1 in enumerate(phrases):\n",
    "        if i in skip_indices:\n",
    "            continue\n",
    "\n",
    "        # Start with this phrase as the canonical version\n",
    "        canonical = phrase1.copy()\n",
    "\n",
    "        # Check against remaining phrases\n",
    "        for j in range(i + 1, len(phrases)):\n",
    "            if j in skip_indices:\n",
    "                continue\n",
    "\n",
    "            phrase2 = phrases[j]\n",
    "\n",
    "            # Calculate similarity\n",
    "            similarity = text_similarity(phrase1['clean'], phrase2['clean'])\n",
    "\n",
    "            # Merge if very similar (likely same text with OCR errors)\n",
    "            if similarity > 0.85:\n",
    "                # Choose the better version (longer, more common, or earlier)\n",
    "                if len(phrase2['clean']) > len(canonical['clean']):\n",
    "                    canonical['clean'] = phrase2['clean']\n",
    "                    canonical['normalized'] = phrase2['normalized']\n",
    "\n",
    "                # Merge metadata\n",
    "                canonical['count'] += phrase2['count']\n",
    "                canonical['timestamps'].extend(phrase2['timestamps'])\n",
    "                canonical['positions'].extend(phrase2['positions'])\n",
    "\n",
    "                skip_indices.add(j)\n",
    "\n",
    "            # Check if one is substring of another\n",
    "            elif canonical['normalized'] in phrase2['normalized']:\n",
    "                # phrase1 is substring of phrase2, keep phrase2's text\n",
    "                canonical['clean'] = phrase2['clean']\n",
    "                canonical['normalized'] = phrase2['normalized']\n",
    "                canonical['count'] += phrase2['count']\n",
    "                canonical['timestamps'].extend(phrase2['timestamps'])\n",
    "                canonical['positions'].extend(phrase2['positions'])\n",
    "                skip_indices.add(j)\n",
    "\n",
    "            elif phrase2['normalized'] in canonical['normalized']:\n",
    "                # phrase2 is substring of phrase1, keep canonical and merge counts\n",
    "                canonical['count'] += phrase2['count']\n",
    "                canonical['timestamps'].extend(phrase2['timestamps'])\n",
    "                canonical['positions'].extend(phrase2['positions'])\n",
    "                skip_indices.add(j)\n",
    "\n",
    "        # Clean up merged data\n",
    "        canonical['timestamps'] = sorted(set(canonical['timestamps']))[:10]\n",
    "        canonical['positions'] = list(set(map(tuple, canonical['positions'])))[:10]\n",
    "\n",
    "        merged.append(canonical)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8fce8",
   "metadata": {},
   "source": [
    "**Final ASR + OCR string assembly and export to CSV step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_final_text(merged_phrases, api_key, model=\"gpt-4o-mini\"):\n",
    "    if not merged_phrases:\n",
    "        return \"\"\n",
    "\n",
    "    # Sort by timestamp (chronological order)\n",
    "    sorted_phrases = sorted(merged_phrases,\n",
    "                           key=lambda x: min(x['timestamps']) if x['timestamps'] else float('inf'))\n",
    "\n",
    "    # Simple fallback assembly\n",
    "    simple_assembly = \" \".join(p['clean'] for p in sorted_phrases)\n",
    "\n",
    "    # Try LLM assembly for better coherence\n",
    "    try:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "\n",
    "        phrases_list = [p['clean'] for p in sorted_phrases]\n",
    "\n",
    "        system_prompt = \"\"\"You are a Gen-Z person familiar with Tiktok trends assembling OCR text fragments into one coherent sentence or phrase.\n",
    "\n",
    "Rules:\n",
    "1. Arrange the fragments by timestamp; if it doesn't make sense, then you can rearrange it minimally.\n",
    "2. Remove duplicate or very similar fragments\n",
    "3. Add minimal punctuation ONLY where clearly needed\n",
    "4. Do NOT add new words or rephrase or change existing words\n",
    "5. Preserve all hashtags\n",
    "6. Output ONE clean line of text\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Assemble these OCR fragments in order into one coherent line:\n",
    "\n",
    "{chr(10).join(f'{i+1}. {p}' for i, p in enumerate(phrases_list))}\n",
    "\n",
    "Assembled text:\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=200\n",
    "        )\n",
    "\n",
    "        result = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Remove quotes if present\n",
    "        if (result.startswith('\"') and result.endswith('\"')) or \\\n",
    "           (result.startswith(\"'\") and result.endswith(\"'\")):\n",
    "            result = result[1:-1]\n",
    "\n",
    "        # Validate result isn't too different from source material\n",
    "        if result and len(result) > 10 and len(result) < len(simple_assembly) * 2:\n",
    "            print(\"Using LLM-assembled text\")\n",
    "            return result\n",
    "        else:\n",
    "            print(\"LLM assembly invalid, using simple assembly\")\n",
    "            return simple_assembly\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LLM assembly failed ({e}), using simple assembly\")\n",
    "        return simple_assembly\n",
    "    \n",
    "def create_output_csv(asr_text, merged_phrases, final_text, output_csv):\n",
    "    rows = []\n",
    "\n",
    "    # Add ASR\n",
    "    if asr_text:\n",
    "        rows.append({\n",
    "            \"source\": \"ASR\",\n",
    "            \"text\": asr_text,\n",
    "            \"count\": 1,\n",
    "            \"timestamps\": \"[]\",\n",
    "            \"original_text\": \"\"\n",
    "        })\n",
    "\n",
    "    # Add individual OCR phrases\n",
    "    for phrase in merged_phrases:\n",
    "        rows.append({\n",
    "            \"source\": \"OCR_PHRASE\",\n",
    "            \"text\": phrase['clean'],\n",
    "            \"count\": phrase['count'],\n",
    "            \"timestamps\": json.dumps(phrase['timestamps'][:5]),\n",
    "            \"original_text\": phrase['original']\n",
    "        })\n",
    "\n",
    "    # Add final assembled text\n",
    "    if final_text:\n",
    "        rows.append({\n",
    "            \"source\": \"OCR_FINAL\",\n",
    "            \"text\": final_text,\n",
    "            \"count\": sum(p['count'] for p in merged_phrases),\n",
    "            \"timestamps\": \"[]\",\n",
    "            \"original_text\": \"\"\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ab036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _final_cleaned_phrase_list(merged_phrases):\n",
    "    # Order by earliest timestamp first\n",
    "    ordered = sorted(\n",
    "        merged_phrases,\n",
    "        key=lambda x: min(x['timestamps']) if x.get('timestamps') else float('inf')\n",
    "    )\n",
    "\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in ordered:\n",
    "        key = normalize_text(p.get('clean', ''))\n",
    "        if key and key not in seen:\n",
    "            out.append(p['clean'])\n",
    "            seen.add(key)\n",
    "    return out\n",
    "\n",
    "def _process_one_video(video_path):\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"==============================\\n\")\n",
    "\n",
    "    # === STEP 1: ASR (unchanged) ===\n",
    "    print(\"=== STEP 1: Audio Transcription ===\")\n",
    "    asr_text = extract_audio_with_whisper(video_path)\n",
    "    print(f\"ASR Result: {asr_text[:200]}{'...' if len(asr_text) > 200 else ''}\\n\")\n",
    "\n",
    "    # === STEP 2: OCR (unchanged) ===\n",
    "    print(\"=== STEP 2: OCR Extraction ===\")\n",
    "    ocr_data = extract_ocr_from_video(video_path, sample_rate_fps=1)\n",
    "    print(f\"Extracted {len(ocr_data)} unique text phrases\\n\")\n",
    "\n",
    "    # === STEP 3: Clean OCR (unchanged) ===\n",
    "    print(\"=== STEP 3: OCR Cleaning ===\")\n",
    "    cleaned_phrases = clean_ocr_with_openai(ocr_data, OPENAI_API_KEY)\n",
    "\n",
    "    print(\"\\nOCR Corrections (sample):\")\n",
    "    for orig, clean in list(zip(ocr_data.keys(), cleaned_phrases))[:10]:\n",
    "        if orig != clean:\n",
    "            print(f\"  ✓ '{orig}' → '{clean}'\")\n",
    "    print()\n",
    "\n",
    "    # === STEP 4: Dedup & Merge (unchanged) ===\n",
    "    print(\"=== STEP 4: Deduplication & Merging ===\")\n",
    "    merged_phrases = smart_deduplicate_and_merge(ocr_data, cleaned_phrases)\n",
    "    print(f\"Consolidated to {len(merged_phrases)} unique phrases\\n\")\n",
    "\n",
    "    # === STEP 5: Final Assembly (unchanged) ===\n",
    "    print(\"=== STEP 5: Final Text Assembly ===\")\n",
    "    final_text = assemble_final_text(merged_phrases, OPENAI_API_KEY)\n",
    "    print(f\"Final Text: {final_text}\\n\")\n",
    "\n",
    "    return asr_text, merged_phrases, final_text\n",
    "\n",
    "def main():\n",
    "    print(\"Discovering videos...\\n\")\n",
    "\n",
    "    MEDIA_FOLDER = r\"./media\"\n",
    "\n",
    "    video_paths = _fetch_videos_from_folder(MEDIA_FOLDER)\n",
    "    if not video_paths:\n",
    "        print(\"No videos found. Please check the media folder\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(video_paths)} video(s).\")\n",
    "    for v in video_paths:\n",
    "        print(\" -\", v)\n",
    "    print()\n",
    "\n",
    "    rows = []\n",
    "    for vp in video_paths:\n",
    "        asr_text, merged_phrases, final_text = _process_one_video(vp)\n",
    "        phrases_list = _final_cleaned_phrase_list(merged_phrases)  # simple list of final cleaned phrases\n",
    "\n",
    "        rows.append({\n",
    "            \"video\": os.path.basename(vp),\n",
    "            \"asr\": asr_text,\n",
    "            \"ocr_final\": final_text,\n",
    "            \"cleaned_phrases\": json.dumps(phrases_list, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"video\", \"asr\", \"ocr_final\", \"cleaned_phrases\"])\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n✓ Batch results saved to: {OUTPUT_CSV}\")\n",
    "    print(f\"Total videos processed: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bcfc7",
   "metadata": {},
   "source": [
    "**Loading csv and metadata JSON step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb045193",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"video_text_outputs.csv\"\n",
    "JSON_DIR = \"meta\"\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "SAVE_DIR = \"embeddings_out/text768\"\n",
    "DEVICE = device\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the csv\n",
    "df = pd.read_csv(CSV_PATH).fillna(\"\")\n",
    "df[\"video_base\"] = df[\"video\"].apply(lambda x: os.path.splitext(os.path.basename(str(x)))[0])\n",
    "print(f\"{len(df)} video entries found in CSV!\")\n",
    "\n",
    "# loads the json\n",
    "json_map = {}\n",
    "for fname in os.listdir(JSON_DIR):\n",
    "    if fname.lower().endswith(\".json\"):\n",
    "        json_map[os.path.splitext(fname)[0]] = os.path.join(JSON_DIR, fname)\n",
    "\n",
    "descs, hashtags_texts = [], []\n",
    "for base in df[\"video_base\"]:\n",
    "    path = json_map.get(base)\n",
    "    if not path:\n",
    "        descs.append(\"\")\n",
    "        hashtags_texts.append(\"\")\n",
    "        continue\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        vm = data.get(\"video_metadata\", data)\n",
    "        descs.append(vm.get(\"description\", \"\") or \"\")\n",
    "        hashtags = vm.get(\"hashtags\", [])\n",
    "        if isinstance(hashtags, list):\n",
    "            hashtags_texts.append(\" \".join(f\"#{h}\" for h in hashtags))\n",
    "        else:\n",
    "            hashtags_texts.append(str(hashtags))\n",
    "    except Exception:\n",
    "        descs.append(\"\")\n",
    "        hashtags_texts.append(\"\")\n",
    "\n",
    "df[\"description\"] = descs\n",
    "df[\"hashtags_text\"] = hashtags_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a7da09",
   "metadata": {},
   "source": [
    "**Model loading and text field encoding step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "weights = np.array([0.4, 0.1, 0.3, 0.2])\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "print(f\"Loaded model: {MODEL_NAME}\")\n",
    "\n",
    "modalities = [\"ocr_final\", \"hashtags_text\", \"asr\", \"description\"]\n",
    "embs = {}\n",
    "\n",
    "for m in modalities:\n",
    "    print(f\"Encoding {m}...\")\n",
    "    texts = df[m].fillna(\"\").astype(str).tolist()\n",
    "    embs[m] = model.encode(texts, batch_size=8, convert_to_numpy=True,\n",
    "                           show_progress_bar=True, normalize_embeddings=True)\n",
    "    \n",
    "# concatenate all text\n",
    "df[\"concatenated_text\"] = df.apply(\n",
    "    lambda row: \" \".join([\n",
    "        str(row.get(\"ocr_final\", \"\")),\n",
    "        str(row.get(\"hashtags_text\", \"\")),\n",
    "        str(row.get(\"asr\", \"\")),\n",
    "        str(row.get(\"description\", \"\"))\n",
    "    ]).strip(),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"\\n===== CONCATENATED TEXTS =====\")\n",
    "for i, text in enumerate(df[\"concatenated_text\"].tolist(), start=1):\n",
    "    print(f\"[{i}] {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703c27b",
   "metadata": {},
   "source": [
    "**Fuse modalities and save:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embs = (\n",
    "    weights[0] * embs[\"ocr_final\"] +\n",
    "    weights[1] * embs[\"hashtags_text\"] +\n",
    "    weights[2] * embs[\"asr\"] +\n",
    "    weights[3] * embs[\"description\"]\n",
    ")\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Saving embeddings\"):\n",
    "    video_name = row[\"video_base\"]\n",
    "    out_path = os.path.join(SAVE_DIR, f\"{video_name}_emb-text768.npy\")\n",
    "    np.save(out_path, combined_embs[i])\n",
    "    print(f\"✓ Saved: {out_path}\")\n",
    "\n",
    "print(f\"\\nAll text embeddings saved in: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3c38b",
   "metadata": {},
   "source": [
    "---\n",
    "## **RETRIEVING SIMILAR VIDEOS**\n",
    "**Goal**: Produce a list of most similar videos based on a weighted combination of modality-specific cosine similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a2252",
   "metadata": {},
   "source": [
    "**Embedding loading step:** creates a dict of embedding vectors following the below format to keep everything organized and so embedding retrieval for each video is trivial.\n",
    "$$\n",
    "video\\_name \\;\\rightarrow\\; \\{ audio,\\; video,\\; text \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_embeddings(base_dir=\"embeddings_out\"):\n",
    "    base_dir = Path(base_dir)\n",
    "\n",
    "    folders = {\n",
    "        \"audio\":  base_dir / \"audio2048\",\n",
    "        \"visual\": base_dir / \"video2048\",\n",
    "        \"text\":   base_dir / \"text768\",\n",
    "    }\n",
    "\n",
    "    suffix_map = {\n",
    "        \"audio\":  \"audio2048\",\n",
    "        \"visual\": \"visual2048\",\n",
    "        \"text\":   \"text768\",\n",
    "    }\n",
    "\n",
    "    modality_files = {} # collect keys per modality\n",
    "    for modality, folder in folders.items():\n",
    "        files = list(folder.glob(f\"*emb-{suffix_map[modality]}.npy\"))\n",
    "        modality_files[modality] = {f.stem.split(\"_emb-\")[0]: f for f in files}\n",
    "\n",
    "    all_video_ids = set()\n",
    "    for d in modality_files.values():\n",
    "        all_video_ids.update(d.keys())\n",
    "\n",
    "    embeddings = {}\n",
    "    missing = []\n",
    "\n",
    "    for vid in all_video_ids:\n",
    "        embeddings[vid] = {}\n",
    "        for modality in [\"audio\", \"visual\", \"text\"]:\n",
    "            file = modality_files[modality].get(vid, None)\n",
    "            if file is None:\n",
    "                missing.append((vid, modality))\n",
    "                embeddings[vid][modality] = None\n",
    "            else:\n",
    "                embeddings[vid][modality] = np.load(str(file))\n",
    "\n",
    "    if missing:\n",
    "        print(\"WARNING: Missing modality embeddings detected:\") # just to be safe\n",
    "        for vid, modality in missing:\n",
    "            print(f\"  - {vid} missing {modality}\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings = load_all_embeddings() # get embeddings with embeddings[\"video_name\"]\n",
    "\n",
    "# to check\n",
    "for video, emb_vec in embeddings.items():\n",
    "    print(video, emb_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2822fa",
   "metadata": {},
   "source": [
    ">**NOTE: Input the query video here :))**\n",
    "\n",
    "If testing different queries with the same set of videos, just <u>run the notebook starting at this cell</u> to skip the preprocessing and loading of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80952079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please type the EXACT filename of the query video\n",
    "QUERY = \"trend1vid1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed98ad",
   "metadata": {},
   "source": [
    "**Cosine similarity computation step:** computes modality-specific cosine similarity scores for each video and a query video, resulting in each video being represented as a vector of 3 similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return None\n",
    "    \n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2)) # maybe cast to float?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modality_similarities(query_video_name: str, embeddings_dir: Path):\n",
    "    \n",
    "    if query_video_name not in embeddings:\n",
    "        raise ValueError(f\"Query video '{query_video_name}' not found in embeddings.\")\n",
    "    \n",
    "    query_emb = embeddings[query_video_name]\n",
    "    \n",
    "    similarity_dict = {}\n",
    "    \n",
    "    for video_name, video_emb in embeddings.items(): \n",
    "        if video_name == query_video_name: # skips self\n",
    "            continue \n",
    "        \n",
    "        sims = []\n",
    "        missing_modalities = []\n",
    "        \n",
    "        for modality in [\"audio\", \"visual\", \"text\"]:\n",
    "            if modality not in video_emb or modality not in query_emb:\n",
    "                missing_modalities.append(modality)\n",
    "                sims.append(np.nan)  # for missing embeddings\n",
    "            else:\n",
    "                sims.append(cosine_similarity(video_emb[modality], query_emb[modality]))\n",
    "        \n",
    "        if missing_modalities:\n",
    "            print(f\"[WARNING] {video_name} missing embeddings for: {', '.join(missing_modalities)}\")\n",
    "        \n",
    "        similarity_dict[video_name] = np.array(sims)\n",
    "    \n",
    "    return similarity_dict\n",
    "\n",
    "similarities = compute_modality_similarities(QUERY, \"embeddings_out\") # get similarity vector with similarities[\"video_name\"]\n",
    "\n",
    "# to check\n",
    "for video, sim_vec in similarities.items():\n",
    "    print(video, sim_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb7155",
   "metadata": {},
   "source": [
    ">**NOTE: Input the weights here :))**\n",
    "\n",
    "If testing different weights with the same query video and set of videos, just <u>run the notebook starting at this cell</u> to skip computing the modality-specific cosine similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7208c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_AUDIO = 1/3\n",
    "WEIGHT_VIDEO = 1/3\n",
    "WEIGHT_TEXT = 1/3\n",
    "\n",
    "CONDITION_NAME = \"baseline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff9c80",
   "metadata": {},
   "source": [
    "**Weighted-sum fusion step:** uses weighted linear combination to form a final similarity score for each video and a query video, where the weights can be modified according to the different test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum_fusion(similarity_dict, weight_audio, weight_video, weight_text):\n",
    "\n",
    "    weights = np.array([weight_audio, weight_video, weight_text])\n",
    "    weights = weights / weights.sum() # apparently we need to normalize this cuz it might not equal 1\n",
    "    final_weighted_dict = {}\n",
    "    \n",
    "    for video, sim_vec in similarity_dict.items():\n",
    "        if len(sim_vec) != 3:\n",
    "            raise ValueError(f\"Expected 3 modalities in similarity vector for {video}, got {len(sim_vec)}\")\n",
    "        \n",
    "        sim_audio, sim_video, sim_text = sim_vec\n",
    "        weighted_score = (sim_audio*weights[0] + sim_video*weights[1] + sim_text*weights[2])\n",
    "        final_weighted_dict[video] = float(weighted_score)\n",
    "\n",
    "    return final_weighted_dict\n",
    "\n",
    "final_scores = weighted_sum_fusion(similarities, WEIGHT_AUDIO, WEIGHT_VIDEO, WEIGHT_TEXT)\n",
    "\n",
    "# to check\n",
    "for video, score in final_scores.items():\n",
    "    print(video, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a9235",
   "metadata": {},
   "source": [
    "**Ranking step:** uses the final scores from weighted sum fusion to rank all videos by their similarity score with the query video, printed in descending order.\n",
    "\n",
    "***most_similar_videos*** is the final output which will be fed into the annotation generation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd1a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar videos to trend1vid1\n",
      "trend1vid3: 0.6684\n",
      "trend1vid10: 0.6595\n",
      "trend1vid4: 0.5551\n",
      "trend3vid1: 0.5460\n",
      "trend5vid8: 0.5451\n",
      "\n",
      "Gemini output array format:\n",
      "['trend1vid1', 'trend1vid3', 'trend1vid10', 'trend1vid4', 'trend3vid1', 'trend5vid8']\n"
     ]
    }
   ],
   "source": [
    "k = 5 # no of similar videos to retrieve (5 seems best)\n",
    "\n",
    "def rank_by_score(final_weighted_dict, top_k=None):\n",
    "    ranked_videos = sorted(final_weighted_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if top_k is not None:\n",
    "        ranked_videos = ranked_videos[:top_k]\n",
    "    \n",
    "    return ranked_videos\n",
    "\n",
    "most_similar_videos = rank_by_score(final_scores, top_k = k)\n",
    "\n",
    "print(f\"Top {k} most similar videos to {QUERY}\")\n",
    "for video, score in most_similar_videos:\n",
    "    print(f\"{video}: {score:.4f}\")\n",
    "\n",
    "similar_videos_output = [QUERY] + [video for video, _ in most_similar_videos]\n",
    "\n",
    "print(\"\\nGemini output array format:\")\n",
    "print(similar_videos_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dd5b69",
   "metadata": {},
   "source": [
    "---\n",
    "## **GENERATING ANNOTATIONS**\n",
    "**Goal**: Produce annotations based on the query video and list of similar videos produced using the earlier weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2094d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_VIDEO_PATH = rf\"media\\{QUERY}.mp4\"\n",
    "BASE_MEDIA_PATH = r\"media\" \n",
    "\n",
    "CONTEXT_VIDEO_TUPLES = most_similar_videos\n",
    "\n",
    "TOP_K = k\n",
    "\n",
    "OUTPUT_DIR = \"annotations\\{QUERY}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "JSON_OUTPUT_PATH = os.path.join(OUTPUT_DIR, f\"{QUERY}_{CONDITION_NAME}.json\")\n",
    "CSV_OUTPUT_PATH = os.path.join(OUTPUT_DIR, f\"{QUERY}_{CONDITION_NAME}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91812779",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    GEMINI_API_KEY = getpass.getpass(\"Enter your Gemini API Key: \")\n",
    "except ImportError:\n",
    "    GEMINI_API_KEY = \"PASTE_YOUR_API_KEY_HERE\"\n",
    "    \n",
    "GENAI_MODEL_NAME = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "697b05a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing Annotation for: trend1vid1 ---\n",
      "Condition: baseline\n",
      "Context Videos: []\n"
     ]
    }
   ],
   "source": [
    "def build_paths(similar_list: List, base_path: str, k: int):\n",
    "    video_names = [video_name for video_name, _ in similar_list]\n",
    "    top_k_names = video_names[:k]\n",
    "    return [os.path.join(base_path, f\"{name}.mp4\") for name in top_k_names]\n",
    "\n",
    "if CONDITION_NAME == \"baseline\":\n",
    "    CONTEXT_VIDEO_PATHS = []\n",
    "else:\n",
    "    CONTEXT_VIDEO_PATHS = build_paths(CONTEXT_VIDEO_TUPLES, BASE_MEDIA_PATH, TOP_K)\n",
    "\n",
    "print(f\"--- Preparing Annotation for: {QUERY} ---\")\n",
    "print(f\"Condition: {CONDITION_NAME}\")\n",
    "print(f\"Context Videos: {CONTEXT_VIDEO_PATHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4619302",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_SYSTEM = \"You are an assistant tasked with generating a brief summary of a short video. Use only the information available in the video. Do not rely on any external knowledge or assumptions. Focus on describing what is happening in the video concisely.\"\n",
    "CONTEXT_AWARE_SYSTEM = \"You are an assistant tasked with generating a summary of a short video. You are provided with the main video and a few additional videos that are semantically related. Use all available information to generate a summary that best describes what is happening in the main video. Focus on enhancing your understanding using the related videos, but ensure the summary reflects the main video.\"\n",
    "BASELINE_USER = \"Please generate a 2–3 sentence summary of the following video based solely on its content.\"\n",
    "CONTEXT_AWARE_USER = \"Please summarize the main video using all the information provided. The first video is the main one, and the others are related videos that may provide helpful context. Your summary should describe what is happening in the main video in 2–3 sentences.\"\n",
    "\n",
    "_uploaded_cache: Dict[str, Any] = {}\n",
    "def _upload_video(path: str):\n",
    "    global _uploaded_cache\n",
    "    full = str(Path(path).resolve())\n",
    "    if full not in _uploaded_cache:\n",
    "        print(f\"Uploading: {full}\")\n",
    "        try:\n",
    "            file_obj = genai.upload_file(path=full)\n",
    "            print(\"Uploaded, waiting for processing...\")\n",
    "            while True:\n",
    "                file_obj = genai.get_file(file_obj.name)\n",
    "                if file_obj.state.name == \"ACTIVE\":\n",
    "                    print(f\"File is ACTIVE: {file_obj.name}\"); break\n",
    "                elif file_obj.state.name == \"FAILED\":\n",
    "                    raise RuntimeError(f\"File {file_obj.name} failed to process.\")\n",
    "                time.sleep(2)\n",
    "            _uploaded_cache[full] = file_obj\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {path}: {e}\"); return None\n",
    "    return _uploaded_cache.get(full)\n",
    "\n",
    "def _make_model(system_instruction: str):\n",
    "    return genai.GenerativeModel(\n",
    "        model_name=GENAI_MODEL_NAME,\n",
    "        system_instruction=system_instruction\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5d5227d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Annotation ---\n",
      "Uploading: C:\\Users\\mkyod\\OneDrive\\Documents\\GitHub\\context-aware-video-retrieval\\similarity pipeline\\media\\trend1vid1.mp4\n",
      "Uploaded, waiting for processing...\n",
      "File is ACTIVE: files/5a43qrwj5hsz\n",
      "Running BASELINE annotation...\n",
      "\n",
      "--- Annotation Complete ---\n",
      "Result: A man opens a refrigerator, takes out a blue container, and inspects it. He then drops the container onto the kitchen floor, spilling white liquid.\n",
      "Saved results to:\n",
      "  annotations\\trend1vid1_baseline.csv\n",
      "  annotations\\trend1vid1_baseline.json\n"
     ]
    }
   ],
   "source": [
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "print(\"--- Starting Annotation ---\")\n",
    "\n",
    "_uploaded_cache = {} \n",
    "annotation_text = \"\"\n",
    "final_record = {}\n",
    "\n",
    "query_file = _upload_video(QUERY_VIDEO_PATH)\n",
    "\n",
    "if query_file is None:\n",
    "    print(f\"Aborting: Failed to upload main query video {QUERY_VIDEO_PATH}\")\n",
    "else:\n",
    "    if CONDITION_NAME == \"baseline\":\n",
    "        print(\"Running BASELINE annotation...\")\n",
    "        model = _make_model(BASELINE_SYSTEM)\n",
    "        try:\n",
    "            response = model.generate_content([query_file, BASELINE_USER])\n",
    "            annotation_text = response.text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in baseline generation: {e}\"); annotation_text = f\"ERROR: {e}\"\n",
    "        \n",
    "        final_record = {\n",
    "            \"query_id\": QUERY, \"condition_name\": CONDITION_NAME,\n",
    "            \"context_video_paths\": [], \"annotation_text\": annotation_text,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        print(f\"Running CONTEXT-AWARE annotation for: {CONDITION_NAME}...\")\n",
    "        model = _make_model(CONTEXT_AWARE_SYSTEM)\n",
    "        \n",
    "        ctx_files = []\n",
    "        for p in CONTEXT_VIDEO_PATHS:\n",
    "            f = _upload_video(p)\n",
    "            if f: ctx_files.append(f)\n",
    "        \n",
    "        contents = [query_file] + ctx_files + [CONTEXT_AWARE_USER]\n",
    "        \n",
    "        try:\n",
    "            response = model.generate_content(contents)\n",
    "            annotation_text = response.text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in context generation: {e}\"); annotation_text = f\"ERROR: {e}\"\n",
    "        \n",
    "        final_record = {\n",
    "            \"query_id\": QUERY, \"condition_name\": CONDITION_NAME,\n",
    "            \"context_video_paths\": CONTEXT_VIDEO_PATHS, \"annotation_text\": annotation_text,\n",
    "        }\n",
    "\n",
    "    print(\"\\n--- Annotation Complete ---\")\n",
    "    print(f\"Result: {annotation_text}\")\n",
    "\n",
    "    if final_record:\n",
    "        pd.DataFrame([final_record]).to_csv(CSV_OUTPUT_PATH, index=False)\n",
    "        with open(JSON_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final_record, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved results to:\\n  {CSV_OUTPUT_PATH}\\n  {JSON_OUTPUT_PATH}\")\n",
    "    else:\n",
    "        print(\"No result to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
