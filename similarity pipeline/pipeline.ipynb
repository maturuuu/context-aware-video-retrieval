{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99223acb",
   "metadata": {},
   "source": [
    "<img src=\"../DLSU-ALTDSI-logo.png\" width=\"100%\" style=\"margin-bottom:10px; margin-top:0px;\"/>\n",
    "\n",
    "**This notebook contains the context-aware video retrieval pipeline used in the study:**\n",
    "\n",
    "## *Comparing Modality Representation Schemes in Video Retrieval for More Context-Aware Auto-Annotation of Trending Short-Form Videos*\n",
    "\n",
    "**By the following researchers from the Andrew L. Tan Data Science Institute:**\n",
    "1. Ong, Matthew Kristoffer Y. (matthew_kristoffer_ong@dlsu.edu.ph)\n",
    "2. Presas, Shanette Giane G. (shanette_giane_presas@dlsu.edu.ph)\n",
    "3. Sarreal, Sophia Althea R. (sophia_sarreal@dlsu.edu.ph)\n",
    "4. To, Jersey Jaclyn K. (jers_to@dlsu.edu.ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a61d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6f596",
   "metadata": {},
   "source": [
    "Note to thesismates:\n",
    "1. Navigate first to the similarity pipeline folder\n",
    "2. Run this to activate venv for the terminal instance: .venv\\Scripts\\activate\n",
    "3. NOTE: you will also need the ff files:\n",
    "    1. 'class_labels_indices.csv'\n",
    "    2. 'Cnn14_mAP=0.431.pth' (these are the model weights to be used) from https://zenodo.org/records/3987831\n",
    "    3. This specific torchaudio/torchvision model: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa930e",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# audio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ffmpeg\n",
    "import torch\n",
    "import librosa\n",
    "from panns_inference import AudioTagging\n",
    "\n",
    "# visuals\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#text\n",
    "import easyocr\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "import string\n",
    "from ftfy import fix_text\n",
    "from wordsegment import load as ws_load, segment as ws_segment\n",
    "from spellchecker import SpellChecker\n",
    "from transformers import pipeline\n",
    "import wordninja\n",
    "from wordfreq import zipf_frequency\n",
    "import glob\n",
    "import pathlib\n",
    "import subprocess\n",
    "import sys\n",
    "from faster_whisper import WhisperModel\n",
    "from openai import OpenAI\n",
    "from difflib import SequenceMatcher\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#similarity\n",
    "from numpy.linalg import norm\n",
    "\n",
    "#gemini\n",
    "import getpass\n",
    "from typing import List, Dict, Any\n",
    "import google.generativeai as genai\n",
    "\n",
    "import faulthandler\n",
    "faulthandler.enable()\n",
    "\n",
    "# Make sure cuda (gpu) is active!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38964d22",
   "metadata": {},
   "source": [
    "---\n",
    "## **AUDIO MODALITY**\n",
    "**Goal**: Produce embeddings representing the audio modality of a given set of videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25bf68",
   "metadata": {},
   "source": [
    "**Preprocessing step:** extracts 32kHz waveform files from the input videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd08338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_to_wavs(video_path: str, out32: str, overwrite: bool=True):\n",
    "    extract_32k=(\n",
    "        ffmpeg.input(video_path).output(out32, format='wav', acodec='pcm_s16le', ac=1, ar=32000)\n",
    "    )\n",
    "    if overwrite:\n",
    "        extract_32k = extract_32k.overwrite_output()\n",
    "    \n",
    "    extract_32k.run(quiet=True)\n",
    "    print(\"Wrote 32kHz\", out32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6aefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path: str, out_dir: str =\"proc_out\"):\n",
    "    out_dir = Path(out_dir)\n",
    "    audio_dir = out_dir.parent / (out_dir.name + \"_32kHz\")\n",
    "    audio_dir.mkdir(parents=True, exist_ok=True) # 32kHz goes to audio_dir\n",
    "\n",
    "    video = Path(video_path)\n",
    "    out32 = audio_dir / (video.stem + \"_32k.wav\") # 32kHz output\n",
    "\n",
    "    # Extract audio\n",
    "    extract_audio_to_wavs(str(video), str(out32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_dir = Path(\"media\")\n",
    "videos = list(media_dir.glob(\"*.mp4\"))\n",
    "print(f\"{len(videos)} videos found!\")\n",
    "\n",
    "for video in videos:\n",
    "    print(f\"\\nProcessing: {video.name}\")\n",
    "    process_video(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f21d0d",
   "metadata": {},
   "source": [
    "**Feature extraction step:** produces embeddings in the form of a 2048-dimensional feature vector representing the audio of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ca0fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proc_out_32kHz_dir = Path(\"proc_out_32kHz\")\n",
    "emb_out_dir = Path(\"embeddings_out/audio2048\") # 2048-d vectors go here\n",
    "emb_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "at_model = AudioTagging(checkpoint_path=None, device=device) #this is the pretrained CNN14\n",
    "\n",
    "wav_files = sorted(proc_out_32kHz_dir.glob(\"*_32k.wav\"))\n",
    "print(f\"{len(wav_files)} WAV files found!\")\n",
    "\n",
    "for wav_path in wav_files:\n",
    "    print(f\"\\nProcessing: {wav_path.name}\")\n",
    "    wav, sr = librosa.load(str(wav_path), sr=32000, mono=True) # just to make sure wav is 32kHz\n",
    "    audio_batch = np.expand_dims(wav, axis=0) # matches the expected shape of PANN\n",
    "\n",
    "    _, embedding = at_model.inference(audio_batch) # gets the embedding as numpy array\n",
    "\n",
    "    embedding_vec = embedding[0] # first element of embedding array\n",
    "\n",
    "    # just removing the \"_32k\" for filename consistency\n",
    "    stem = wav_path.stem\n",
    "    if stem.endswith(\"_32k\"):\n",
    "        stem = stem[:-4]\n",
    "\n",
    "    out_path = emb_out_dir / f\"{stem}_emb-audio2048.npy\"\n",
    "    np.save(str(out_path), embedding_vec)\n",
    "    print(\"Embedding saved: \", out_path)\n",
    "\n",
    "    print(embedding_vec) # if you want to see the vector\n",
    "    print(embedding_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213682a5",
   "metadata": {},
   "source": [
    "---\n",
    "## **VISUAL MODALITY**\n",
    "**Goal**: Produce embeddings representing the visual modality of a given set of videos, a 2048-dimension vector that numerically represents its visual content (objects, scenes, textures).\n",
    "**Model:** a pre-trained **ResNet-50** model.\n",
    "**Output:** A `.npy` file for each video (e.g., `trend1vid1_emb-visual2048.npy`) saved to the `embeddings_out/video2048` folder.\n",
    "\n",
    "This vector will serve as the **\"visual\"** component for our similarity retrieval pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a171934",
   "metadata": {},
   "source": [
    "**1. Configuration**\n",
    "\n",
    "This cell sets the key parameters for the visual pipeline.\n",
    "\n",
    "* `INPUT_DIR`: The local folder where the videos are stored.\n",
    "* `OUTPUT_DIR`: The local folder where the finished embeddings will be saved.\n",
    "* `FRAME_SAMPLE_RATE = 30`: Every nth frame processed (e.g., at 30fps, this is one frame per second). This is a trade-off between speed and accuracy.\n",
    "* `BATCH_SIZE = 32`: For efficiency, we will feed frames to the GPU in batches of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf09c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = Path(\"media\")\n",
    "OUTPUT_DIR = Path(\"embeddings_out/video2048\")\n",
    "\n",
    "FRAME_SAMPLE_RATE = 30\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "VIDEO_EXTENSIONS = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf39896",
   "metadata": {},
   "source": [
    "**2. Model Definition: ResNet-50 as a Feature Extractor**\n",
    "This function loads the ResNet-50 model, which is pre-trained on the ImageNet dataset.\n",
    "\n",
    "The most important step is in this line:\n",
    "`model = torch.nn.Sequential(*list(model.children())[:-1])`\n",
    "\n",
    "This \"decapitates\" the model by **removing its final classification layer**. Instead of outputting a single word like \"dog\" or \"cat\", the model now outputs the **2048-dimension feature vector** from the second-to-last layer. This vector is the rich \"visual fingerprint\" we use for similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a263dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_model(device: str):\n",
    "    \"\"\"Loads the pre-trained ResNet-50 model and its associated transforms.\"\"\"\n",
    "    weights = models.ResNet50_Weights.DEFAULT\n",
    "    model = models.resnet50(weights=weights)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preprocess = weights.transforms()\n",
    "    return model, preprocess\n",
    "\n",
    "model, preprocess = get_resnet_model(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed1f7c",
   "metadata": {},
   "source": [
    "**3. Core Logic: The Video-to-Vector Function**\n",
    "\n",
    "This function, `extract_resnet_embeddings`, contains the core logic for processing a single video file. It does three things:\n",
    "\n",
    "1.  **Sampling:** It opens the video with `cv2.VideoCapture` and loops through it, grabbing one frame every `FRAME_SAMPLE_RATE`.\n",
    "2.  **Batching:** It collects these frames into a `frame_batch`. When the batch is full (`len(frame_batch) == batch_size`), it stacks them into a single tensor and sends them to the GPU. This is much faster than processing frames one by one.\n",
    "3.  **Aggregation (Mean Pooling):** After all frames are processed, the function has many 2048-dim vectors. It calculates the **average** (`np.mean`) of all these vectors to create *one single vector* that represents the entire video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_resnet_embeddings(\n",
    "    video_path: Path, \n",
    "    model, \n",
    "    preprocess, \n",
    "    device: str, \n",
    "    frame_sample_rate: int = 30, \n",
    "    batch_size: int = 32\n",
    ") -> np.ndarray:\n",
    "    if not video_path.exists():\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video file: {video_path}\")\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    all_features = []\n",
    "    frame_batch = []\n",
    "    frame_idx = 0\n",
    "    \n",
    "    pbar = tqdm(total=frame_count, desc=f\"Frames for {video_path.name}\", leave=True, disable=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if frame_idx % frame_sample_rate == 0:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(frame_rgb)\n",
    "                frame_batch.append(pil_img)\n",
    "\n",
    "                if len(frame_batch) == batch_size:\n",
    "                    image_inputs = torch.stack(\n",
    "                        [preprocess(img) for img in frame_batch]\n",
    "                    ).to(device)\n",
    "                    image_features = model(image_inputs)\n",
    "                    all_features.append(image_features.squeeze().cpu().numpy())\n",
    "                    frame_batch = []\n",
    "            frame_idx += 1\n",
    "        \n",
    "        if frame_batch:\n",
    "            image_inputs = torch.stack(\n",
    "                [preprocess(img) for img in frame_batch]\n",
    "            ).to(device)\n",
    "            image_features = model(image_inputs)\n",
    "            all_features.append(image_features.squeeze().cpu().numpy())\n",
    "\n",
    "    cap.release()\n",
    "    pbar.close()\n",
    "    if not all_features:\n",
    "        raise ValueError(f\"No frames sampled for {video_path.name}\")\n",
    "\n",
    "    embeddings = np.vstack(all_features)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Reading videos from: {INPUT_DIR.resolve()}\")\n",
    "print(f\"Saving embeddings to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "video_files = []\n",
    "for ext in VIDEO_EXTENSIONS:\n",
    "    video_files.extend(INPUT_DIR.glob(f\"*{ext}\"))\n",
    "print(f\"Found {len(video_files)} videos.\")\n",
    "\n",
    "existing_embeddings = {f.name for f in OUTPUT_DIR.glob('*_resnet.npy')}\n",
    "print(f\"Found {len(existing_embeddings)} existing ResNet embeddings.\")\n",
    "\n",
    "for video_path in tqdm(video_files, desc=\"Processing Videos (ResNet)\"):\n",
    "    output_filename = f\"{video_path.stem}_emb-visual2048.npy\"\n",
    "\n",
    "    if output_filename in existing_embeddings:\n",
    "        continue\n",
    "    \n",
    "    output_path = OUTPUT_DIR / output_filename\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing {video_path.name}...\")\n",
    "        mean_embedding = extract_resnet_embeddings(\n",
    "            video_path=video_path,\n",
    "            model=model,\n",
    "            preprocess=preprocess,\n",
    "            device=device,\n",
    "            frame_sample_rate=FRAME_SAMPLE_RATE,\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "        np.save(output_path, mean_embedding)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Failed to process {video_path.name}: {e}\")\n",
    "\n",
    "print(\"\\n--- Batch processing complete. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7365085",
   "metadata": {},
   "source": [
    "---\n",
    "## **TEXT MODALITY**\n",
    "**Goal**: Produce embeddings representing the text modality of a given set of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce55d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CSV = \"video_text_outputs.csv\"\n",
    "WHISPER_MODEL = WhisperModel(\"base\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                    compute_type=\"int8_float16\" if torch.cuda.is_available() else \"int8\")\n",
    "\n",
    "try:\n",
    "    OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "except ImportError:\n",
    "    OPENAI_API_KEY = \"PASTE_YOUR_OPENAI_KEY_HERE\"\n",
    "except Exception as e:\n",
    "    print(f\"Error getting API key: {e}\")\n",
    "    OPENAI_API_KEY = \"\"\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY == \"\":\n",
    "    print(\"Warning: OpenAI API Key is not set. OCR cleaning (Step 3) will fail.\")\n",
    "else:\n",
    "    print(\"OpenAI API Key received.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _list_local_videos(root_dir):\n",
    "    exts = ('.mp4', '.mov', '.m4v', '.mkv', '.avi', '.webm')\n",
    "    paths = []\n",
    "    for ext in exts:\n",
    "        paths.extend(glob.glob(os.path.join(root_dir, f\"**/*{ext}\"), recursive=True))\n",
    "    return sorted(paths)\n",
    "\n",
    "def _fetch_videos_from_folder(folder_path):\n",
    "    if not folder_path or folder_path.strip() == \"\":\n",
    "        raise ValueError(\"Folder path is empty.\")\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        return _list_local_videos(folder_path)\n",
    "\n",
    "    raise ValueError(f\"Not a valid local directory: {folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f84b3",
   "metadata": {},
   "source": [
    "### Automatic Speech Recognition (ASR) Extraction  \n",
    "This cell defines the **audio transcription stage** using the `faster_whisper` model.  \n",
    "It loads a lightweight Whisper model (`base`) with GPU acceleration if available, and transcribes speech segments into text.  \n",
    "  \n",
    "All transcribed text segments are concatenated into one clean string, which becomes the `ASR` output for that video.  \n",
    "This step ensures spoken content is captured in parallel with on-screen text for multimodal fusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2790103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_with_whisper(video_path):\n",
    "    try:\n",
    "        segments, _ = WHISPER_MODEL.transcribe(video_path, beam_size=1)\n",
    "        return \" \".join(s.text for s in segments).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"ASR Error: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a581df4",
   "metadata": {},
   "source": [
    "### Text Validation and Frame Preprocessing for OCR  \n",
    "This cell prepares each video frame for optimal text detection.  \n",
    "It defines a validation function that filters out short or meaningless OCR detections, ensuring only text-like content is retained.  \n",
    "`preprocess_frame_for_ocr()` enhances frames using:\n",
    "1. **Grayscale conversion** for consistency  \n",
    "2. **CLAHE (Contrast Limited Adaptive Histogram Equalization)** to amplify local text contrast  \n",
    "3. **Gaussian blur** to reduce noise  \n",
    "4. **Sharpening filter** to reinforce text edges  \n",
    "This preprocessing pipeline improves EasyOCR accuracy for low-contrast overlays typical in short-form videos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cbbca3",
   "metadata": {},
   "source": [
    "### Optical Character Recognition (OCR) from Video Frames  \n",
    "This cell also handles the frame-wise extraction of visible text from video overlays.  \n",
    "It uses **OpenCV** to read frames and **EasyOCR** for text detection and recognition.  \n",
    "  \n",
    "For each detected text region, the function records the recognized text and timestamps and positions (up to five samples each per unique text).\n",
    " \n",
    "All detections are aggregated into a structured dictionary keyed by unique text phrases.  \n",
    "This creates a high-resolution temporal map of textual elements appearing throughout the video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ccb50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_text(text):\n",
    "    if not text or len(text.strip()) < 2:\n",
    "        return False\n",
    "    clean = re.sub(r'[^\\w#@]', '', text)\n",
    "    return len(clean) > 0\n",
    "\n",
    "def preprocess_frame_for_ocr(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "    denoised = cv2.GaussianBlur(enhanced, (3, 3), 0)\n",
    "    kernel = np.array([[-1,-1,-1], [-1, 9,-1], [-1,-1,-1]])\n",
    "    sharpened = cv2.filter2D(denoised, -1, kernel)\n",
    "    return sharpened\n",
    "\n",
    "def extract_ocr_from_video(video_path, sample_rate_fps=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Cannot open video {video_path}\")\n",
    "        return {}\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "    frame_interval = max(1, int(round(fps / max(0.1, sample_rate_fps))))\n",
    "\n",
    "    reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "    text_detections = defaultdict(lambda: {'count': 0, 'timestamps': [], 'positions': []})\n",
    "\n",
    "    print(\"Processing video frames for OCR...\")\n",
    "    processed = 0\n",
    "\n",
    "    for frame_idx in range(0, total_frames, frame_interval):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame is None:\n",
    "            continue\n",
    "\n",
    "        h, w = frame.shape[:2]\n",
    "        max_w = 960\n",
    "        if w > max_w:\n",
    "            scale = max_w / float(w)\n",
    "            frame = cv2.resize(frame, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        timestamp = frame_idx / fps\n",
    "\n",
    "        try:\n",
    "            frame_pp = preprocess_frame_for_ocr(frame)\n",
    "            results = reader.readtext(frame_pp, detail=1, paragraph=False)\n",
    "            for (bbox, text, confidence) in results:\n",
    "                if confidence > 0.5 and is_valid_text(text):\n",
    "                    xs = [p[0] for p in bbox]\n",
    "                    ys = [p[1] for p in bbox]\n",
    "                    x_left = float(min(xs))\n",
    "                    y_top = float(min(ys))\n",
    "\n",
    "                    text_detections[text]['count'] += 1\n",
    "                    if len(text_detections[text]['timestamps']) < 5:\n",
    "                        text_detections[text]['timestamps'].append(round(timestamp, 2))\n",
    "                    if len(text_detections[text]['positions']) < 5:\n",
    "                        text_detections[text]['positions'].append((round(y_top, 2), round(x_left, 2)))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"OCR error at frame {frame_idx}: {e}\")\n",
    "\n",
    "        processed += 1\n",
    "        if processed % 10 == 0:\n",
    "            print(f\"Processed {processed} sampled frames...\")\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"OCR processing complete. Found {len(text_detections)} unique text phrases.\")\n",
    "    return dict(text_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeec71a",
   "metadata": {},
   "source": [
    "### Text Cleaning and OCR Correction  \n",
    "This cell defines the **OCR post-processing function** used to clean noisy text extracted from video frames.  \n",
    "It uses OpenAI’s GPT-4o-mini model to fix common OCR errors such as spacing, casing, and misread characters (`v`→`y`, `rn`→`m`, etc.).  \n",
    "Corrections are strictly constrained — the function does **not** rephrase or alter meaning, ensuring all text remains faithful to the original overlay.  \n",
    "Progress is logged, cost is tracked, and fallback behavior ensures the pipeline never breaks if API calls fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8304a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ocr_with_openai(ocr_phrases, api_key, model=\"gpt-4o-mini\"):\n",
    "    phrases = list(ocr_phrases.keys())\n",
    "    if not phrases:\n",
    "        return []\n",
    "\n",
    "    print(f\"Cleaning {len(phrases)} OCR phrases with {model}...\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    system_prompt = \"\"\"You are an OCR error correction assistant. Fix only obvious OCR mistakes.\n",
    "\n",
    "Common OCR errors:\n",
    "- Character confusion: 'v' → 'y', 'rn' → 'm', '0' → 'O', 'i' → 'l', 'vv' → 'w', '@' → 'a', '@' → 'o'\n",
    "- Missing spaces: 'helloworld' → 'hello world'\n",
    "- Extra spaces: 'hel lo' → 'hello'\n",
    "\n",
    "Rules:\n",
    "1. ONLY fix clear OCR errors - do not rephrase or change meaning\n",
    "2. Preserve hashtags (#) exactly\n",
    "3. Keep original capitalization only for proper nouns, otherwise make everything lowercase.\n",
    "4. Output ONLY the corrected text (no quotes, explanations, or extra words)\n",
    "5. If a word has the letter 'v' in it and it looks misspelled, try swapping the 'v' with a 'y' to see if it makes more sense, and vice versa.\n",
    "6. If a word other than \"I\" has the letter 'i' in it and it looks misspelled, try swapping the 'i' with a 'l' to see if it makes more sense, and vice versa.\n",
    "7. Unless an acronym makes sense in the context, make it lowercase.\"\"\"\n",
    "\n",
    "    cleaned = []\n",
    "    total_cost = 0.0\n",
    "\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            print(f\"  Cleaned {i}/{len(phrases)} (Cost: ${total_cost:.4f})\")\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"Fix OCR errors: {phrase}\"}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=100\n",
    "            )\n",
    "\n",
    "            result = response.choices[0].message.content.strip()\n",
    "\n",
    "            # Track cost\n",
    "            if hasattr(response, 'usage') and response.usage:\n",
    "                input_tok = response.usage.prompt_tokens or 0\n",
    "                output_tok = response.usage.completion_tokens or 0\n",
    "                total_cost += (input_tok * 0.15 + output_tok * 0.60) / 1_000_000\n",
    "\n",
    "            # Remove quotes if added\n",
    "            if (result.startswith('\"') and result.endswith('\"')) or \\\n",
    "               (result.startswith(\"'\") and result.endswith(\"'\")):\n",
    "                result = result[1:-1]\n",
    "\n",
    "            # Fallback if result is empty or way too different\n",
    "            if not result or len(result) > len(phrase) * 3:\n",
    "                result = phrase\n",
    "\n",
    "            cleaned.append(result.strip())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error cleaning '{phrase}': {e}\")\n",
    "            cleaned.append(phrase)\n",
    "\n",
    "    print(f\"Cleaning complete! Total cost: ${total_cost:.4f}\")\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cee4d1",
   "metadata": {},
   "source": [
    "### Deduplication and Phrase Merging  \n",
    "This cell merges redundant or near-duplicate OCR fragments produced across video frames.  \n",
    "It compares text using normalized string similarity (SequenceMatcher) and merges phrases that are visually or semantically similar.  \n",
    "Metadata such as occurrence counts, timestamps, and bounding box positions are aggregated.  \n",
    "This step ensures each distinct caption fragment appears once, forming a clean set of unique textual units per video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b795f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text)  # Keep hashtags and mentions\n",
    "    return re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "def text_similarity(s1, s2):\n",
    "    return SequenceMatcher(None, normalize_text(s1), normalize_text(s2)).ratio()\n",
    "\n",
    "def smart_deduplicate_and_merge(ocr_data, cleaned_phrases):\n",
    "    # Create phrase objects with metadata\n",
    "    phrases = []\n",
    "    for orig, clean in zip(ocr_data.keys(), cleaned_phrases):\n",
    "        data = ocr_data[orig]\n",
    "        phrases.append({\n",
    "            'original': orig,\n",
    "            'clean': clean,\n",
    "            'normalized': normalize_text(clean),\n",
    "            'count': data['count'],\n",
    "            'timestamps': data.get('timestamps', []),\n",
    "            'positions': data.get('positions', [])\n",
    "        })\n",
    "\n",
    "    # Sort by frequency (most common first) and timestamp (earliest first)\n",
    "    phrases.sort(key=lambda x: (-x['count'], min(x['timestamps']) if x['timestamps'] else float('inf')))\n",
    "\n",
    "    merged = []\n",
    "    skip_indices = set()\n",
    "\n",
    "    for i, phrase1 in enumerate(phrases):\n",
    "        if i in skip_indices:\n",
    "            continue\n",
    "\n",
    "        # Start with this phrase as the canonical version\n",
    "        canonical = phrase1.copy()\n",
    "\n",
    "        # Check against remaining phrases\n",
    "        for j in range(i + 1, len(phrases)):\n",
    "            if j in skip_indices:\n",
    "                continue\n",
    "\n",
    "            phrase2 = phrases[j]\n",
    "\n",
    "            # Calculate similarity\n",
    "            similarity = text_similarity(phrase1['clean'], phrase2['clean'])\n",
    "\n",
    "            # Merge if very similar (likely same text with OCR errors)\n",
    "            if similarity > 0.85:\n",
    "                # Choose the better version (longer, more common, or earlier)\n",
    "                if len(phrase2['clean']) > len(canonical['clean']):\n",
    "                    canonical['clean'] = phrase2['clean']\n",
    "                    canonical['normalized'] = phrase2['normalized']\n",
    "\n",
    "                # Merge metadata\n",
    "                canonical['count'] += phrase2['count']\n",
    "                canonical['timestamps'].extend(phrase2['timestamps'])\n",
    "                canonical['positions'].extend(phrase2['positions'])\n",
    "\n",
    "                skip_indices.add(j)\n",
    "\n",
    "            # Check if one is substring of another\n",
    "            elif canonical['normalized'] in phrase2['normalized']:\n",
    "                # phrase1 is substring of phrase2, keep phrase2's text\n",
    "                canonical['clean'] = phrase2['clean']\n",
    "                canonical['normalized'] = phrase2['normalized']\n",
    "                canonical['count'] += phrase2['count']\n",
    "                canonical['timestamps'].extend(phrase2['timestamps'])\n",
    "                canonical['positions'].extend(phrase2['positions'])\n",
    "                skip_indices.add(j)\n",
    "\n",
    "            elif phrase2['normalized'] in canonical['normalized']:\n",
    "                # phrase2 is substring of phrase1, keep canonical and merge counts\n",
    "                canonical['count'] += phrase2['count']\n",
    "                canonical['timestamps'].extend(phrase2['timestamps'])\n",
    "                canonical['positions'].extend(phrase2['positions'])\n",
    "                skip_indices.add(j)\n",
    "\n",
    "        # Clean up merged data\n",
    "        canonical['timestamps'] = sorted(set(canonical['timestamps']))[:10]\n",
    "        canonical['positions'] = list(set(map(tuple, canonical['positions'])))[:10]\n",
    "\n",
    "        merged.append(canonical)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b371df",
   "metadata": {},
   "source": [
    "### Final Assembly of OCR Fragments  \n",
    "This cell converts all cleaned and merged OCR phrases into a single **chronologically ordered text line** per video.  \n",
    "The fragments are first arranged by their earliest timestamps and concatenated.  \n",
    "Then, an LLM-based refinement (GPT-4o-mini) attempts to minimally join the pieces into a coherent but faithful sentence — no paraphrasing or addition of new words.  \n",
    "If the LLM output fails validation (too long or dissimilar), the code automatically falls back to the raw chronological assembly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_final_text(merged_phrases, api_key, model=\"gpt-4o-mini\"):\n",
    "    if not merged_phrases:\n",
    "        return \"\"\n",
    "\n",
    "    # Sort by timestamp (chronological order)\n",
    "    sorted_phrases = sorted(merged_phrases,\n",
    "                           key=lambda x: min(x['timestamps']) if x['timestamps'] else float('inf'))\n",
    "\n",
    "    # Simple fallback assembly\n",
    "    simple_assembly = \" \".join(p['clean'] for p in sorted_phrases)\n",
    "\n",
    "    # Try LLM assembly for better coherence\n",
    "    try:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "\n",
    "        phrases_list = [p['clean'] for p in sorted_phrases]\n",
    "\n",
    "        system_prompt = \"\"\"You are a Gen-Z person familiar with Tiktok culture assembling OCR text fragments into one coherent sentence or phrase.\n",
    "\n",
    "Rules:\n",
    "1. Arrange the fragments by timestamp; if it doesn't make sense, then you can rearrange it minimally.\n",
    "2. Remove duplicate or very similar fragments\n",
    "3. Add minimal punctuation ONLY where clearly needed\n",
    "4. Do NOT add new words or rephrase or change existing words\n",
    "5. Preserve all hashtags\n",
    "6. Output ONE clean line of text\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Assemble these OCR fragments in order into one coherent line:\n",
    "\n",
    "{chr(10).join(f'{i+1}. {p}' for i, p in enumerate(phrases_list))}\n",
    "\n",
    "Assembled text:\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=200\n",
    "        )\n",
    "\n",
    "        result = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Remove quotes if present\n",
    "        if (result.startswith('\"') and result.endswith('\"')) or \\\n",
    "           (result.startswith(\"'\") and result.endswith(\"'\")):\n",
    "            result = result[1:-1]\n",
    "\n",
    "        # Validate result isn't too different from source material\n",
    "        if result and len(result) > 10 and len(result) < len(simple_assembly) * 2:\n",
    "            print(\"Using LLM-assembled text\")\n",
    "            return result\n",
    "        else:\n",
    "            print(\"LLM assembly invalid, using simple assembly\")\n",
    "            return simple_assembly\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LLM assembly failed ({e}), using simple assembly\")\n",
    "        return simple_assembly\n",
    "    \n",
    "def create_output_csv(asr_text, merged_phrases, final_text, output_csv):\n",
    "    rows = []\n",
    "\n",
    "    # Add ASR\n",
    "    if asr_text:\n",
    "        rows.append({\n",
    "            \"source\": \"ASR\",\n",
    "            \"text\": asr_text,\n",
    "            \"count\": 1,\n",
    "            \"timestamps\": \"[]\",\n",
    "            \"original_text\": \"\"\n",
    "        })\n",
    "\n",
    "    # Add individual OCR phrases\n",
    "    for phrase in merged_phrases:\n",
    "        rows.append({\n",
    "            \"source\": \"OCR_PHRASE\",\n",
    "            \"text\": phrase['clean'],\n",
    "            \"count\": phrase['count'],\n",
    "            \"timestamps\": json.dumps(phrase['timestamps'][:5]),\n",
    "            \"original_text\": phrase['original']\n",
    "        })\n",
    "\n",
    "    # Add final assembled text\n",
    "    if final_text:\n",
    "        rows.append({\n",
    "            \"source\": \"OCR_FINAL\",\n",
    "            \"text\": final_text,\n",
    "            \"count\": sum(p['count'] for p in merged_phrases),\n",
    "            \"timestamps\": \"[]\",\n",
    "            \"original_text\": \"\"\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ab036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _final_cleaned_phrase_list(merged_phrases):\n",
    "    # Order by earliest timestamp first\n",
    "    ordered = sorted(\n",
    "        merged_phrases,\n",
    "        key=lambda x: min(x['timestamps']) if x.get('timestamps') else float('inf')\n",
    "    )\n",
    "\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in ordered:\n",
    "        key = normalize_text(p.get('clean', ''))\n",
    "        if key and key not in seen:\n",
    "            out.append(p['clean'])\n",
    "            seen.add(key)\n",
    "    return out\n",
    "\n",
    "def _process_one_video(video_path):\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"==============================\\n\")\n",
    "\n",
    "    # === STEP 1: ASR ===\n",
    "    print(\"=== STEP 1: Audio Transcription ===\")\n",
    "    asr_text = extract_audio_with_whisper(video_path)\n",
    "    print(f\"ASR Result: {asr_text[:200]}{'...' if len(asr_text) > 200 else ''}\\n\")\n",
    "\n",
    "    # === STEP 2: OCR ===\n",
    "    print(\"=== STEP 2: OCR Extraction ===\")\n",
    "    ocr_data = extract_ocr_from_video(video_path, sample_rate_fps=1)\n",
    "    print(f\"Extracted {len(ocr_data)} unique text phrases\\n\")\n",
    "\n",
    "    # === STEP 3: Clean OCR ===\n",
    "    print(\"=== STEP 3: OCR Cleaning ===\")\n",
    "    cleaned_phrases = clean_ocr_with_openai(ocr_data, OPENAI_API_KEY)\n",
    "\n",
    "    print(\"\\nOCR Corrections (sample):\")\n",
    "    for orig, clean in list(zip(ocr_data.keys(), cleaned_phrases))[:10]:\n",
    "        if orig != clean:\n",
    "            print(f\"  ✓ '{orig}' → '{clean}'\")\n",
    "    print()\n",
    "\n",
    "    # === STEP 4: Dedup & Merge ===\n",
    "    print(\"=== STEP 4: Deduplication & Merging ===\")\n",
    "    merged_phrases = smart_deduplicate_and_merge(ocr_data, cleaned_phrases)\n",
    "    print(f\"Consolidated to {len(merged_phrases)} unique phrases\\n\")\n",
    "\n",
    "    # === STEP 5: Final Assembly ===\n",
    "    print(\"=== STEP 5: Final Text Assembly ===\")\n",
    "    final_text = assemble_final_text(merged_phrases, OPENAI_API_KEY)\n",
    "    print(f\"Final Text: {final_text}\\n\")\n",
    "\n",
    "    return asr_text, merged_phrases, final_text\n",
    "\n",
    "def main():\n",
    "    print(\"Discovering videos...\\n\")\n",
    "\n",
    "    MEDIA_FOLDER = r\"./media\"\n",
    "\n",
    "    video_paths = _fetch_videos_from_folder(MEDIA_FOLDER)\n",
    "    if not video_paths:\n",
    "        print(\"No videos found. Please check the media folder\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(video_paths)} video(s).\")\n",
    "    for v in video_paths:\n",
    "        print(\" -\", v)\n",
    "    print()\n",
    "\n",
    "    rows = []\n",
    "    for vp in video_paths:\n",
    "        asr_text, merged_phrases, final_text = _process_one_video(vp)\n",
    "        phrases_list = _final_cleaned_phrase_list(merged_phrases)  # simple list of final cleaned phrases\n",
    "\n",
    "        rows.append({\n",
    "            \"video\": os.path.basename(vp),\n",
    "            \"asr\": asr_text,\n",
    "            \"ocr_final\": final_text,\n",
    "            \"cleaned_phrases\": json.dumps(phrases_list, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"video\", \"asr\", \"ocr_final\", \"cleaned_phrases\"])\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n✓ Batch results saved to: {OUTPUT_CSV}\")\n",
    "    print(f\"Total videos processed: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     result_df = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af9773",
   "metadata": {},
   "source": [
    "### Objective Quality Scoring of Assembled Text  \n",
    "This cell introduces a **quantitative metric** to evaluate how meaningful each video’s `ocr_final` text is.  \n",
    "The score integrates three sub-factors:\n",
    "- **Completeness** – Does the text have enough tokens (up to 25)?  \n",
    "- **Language-likeness** – Are the tokens real, frequent English words (Zipf ≥ 2.5)?  \n",
    "- **Diversity** – How varied are the tokens (unique / total)?  \n",
    "A weighted sum (0.2 · C + 0.6 · L + 0.2 · D) × 100 yields a normalized 0–100 score, enabling automated quality control over OCR outputs.\n",
    "\n",
    "The quality score, along with the final ASR, final OCR string, cleaned phrases, are recorded into a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4037b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_quality_score(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    tokens = re.findall(r\"[A-Za-z#@']+\", text.lower())\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "\n",
    "    num_tokens = len(tokens)\n",
    "    unique_tokens = len(set(tokens))\n",
    "\n",
    "    completeness = min(num_tokens, 20) / 20.0\n",
    "\n",
    "   \n",
    "    valid_count = 0\n",
    "    for tok in tokens:\n",
    "        clean_tok = tok.lstrip(\"#@\")\n",
    "        if not clean_tok:\n",
    "            continue\n",
    "        z = zipf_frequency(clean_tok, \"en\")\n",
    "        if z >= 2.5: \n",
    "            valid_count += 1\n",
    "    lang_like = valid_count / num_tokens if num_tokens else 0.0\n",
    "\n",
    "    diversity = unique_tokens / num_tokens if num_tokens else 0.0\n",
    "\n",
    "    score = 100.0 * (0.2 * completeness + 0.6 * lang_like + 0.2 * diversity)\n",
    "    score = max(0.0, min(score, 100.0))\n",
    "    return round(score, 1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # result_df = main()\n",
    "    result_df = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "    if isinstance(result_df, pd.DataFrame) and not result_df.empty and \"ocr_final\" in result_df.columns:\n",
    "        result_df[\"quality_score\"] = [\n",
    "            _compute_quality_score(text) for text in result_df[\"ocr_final\"]\n",
    "        ]\n",
    "        result_df.to_csv(OUTPUT_CSV, index=False)\n",
    "        print(\"✓ Added 'quality_score' column to CSV based on ocr_final text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bcfc7",
   "metadata": {},
   "source": [
    "**Loading csv and metadata JSON step:**\n",
    "\n",
    "This step loads the CSV containing video information and extracts base names for each video. It then reads matching JSON files to get descriptions and hashtags, joining hashtags into a single string. Missing files or fields are replaced with empty strings, and the results are added to the DataFrame to be used in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb045193",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"video_text_outputs.csv\"\n",
    "JSON_DIR = \"meta\"\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "SAVE_DIR = \"embeddings_out/text768\"\n",
    "DEVICE = device\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the csv\n",
    "df = pd.read_csv(CSV_PATH).fillna(\"\")\n",
    "df[\"video_base\"] = df[\"video\"].apply(lambda x: os.path.splitext(os.path.basename(str(x)))[0])\n",
    "print(f\"{len(df)} video entries found in CSV!\")\n",
    "\n",
    "# loads the json\n",
    "json_map = {}\n",
    "for fname in os.listdir(JSON_DIR):\n",
    "    if fname.lower().endswith(\".json\"):\n",
    "        json_map[os.path.splitext(fname)[0]] = os.path.join(JSON_DIR, fname)\n",
    "\n",
    "descs, hashtags_texts = [], []\n",
    "for base in df[\"video_base\"]:\n",
    "    path = json_map.get(base)\n",
    "    if not path:\n",
    "        descs.append(\"\")\n",
    "        hashtags_texts.append(\"\")\n",
    "        continue\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        vm = data.get(\"video_metadata\", data)\n",
    "        descs.append(vm.get(\"description\", \"\") or \"\")\n",
    "        hashtags = vm.get(\"hashtags\", [])\n",
    "        if isinstance(hashtags, list):\n",
    "            hashtags_texts.append(\" \".join(f\"#{h}\" for h in hashtags))\n",
    "        else:\n",
    "            hashtags_texts.append(str(hashtags))\n",
    "    except Exception:\n",
    "        descs.append(\"\")\n",
    "        hashtags_texts.append(\"\")\n",
    "\n",
    "df[\"description\"] = descs\n",
    "df[\"hashtags_text\"] = hashtags_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a7da09",
   "metadata": {},
   "source": [
    "**Model loading and text field encoding step:**\n",
    "\n",
    "- Load Sentence-BERT model (all-mpnet-base-v2) for text encoding.\n",
    "\n",
    "- Encode four text types: OCR results, hashtags, ASR transcripts, descriptions → 768-dimensional embeddings.\n",
    "\n",
    "- Combine embeddings using a weighted sum to form a single representation per video.\n",
    "\n",
    "- The best weights [0.4, 0.1, 0.3, 0.2] were found via grid search, selecting the combination that maximizes mean average precision (mAP) using inferred video labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "weights = np.array([0.4, 0.1, 0.3, 0.2])\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "print(f\"Loaded model: {MODEL_NAME}\")\n",
    "\n",
    "modalities = [\"ocr_final\", \"hashtags_text\", \"asr\", \"description\"]\n",
    "embs = {}\n",
    "\n",
    "for m in modalities:\n",
    "    print(f\"Encoding {m}...\")\n",
    "    texts = df[m].fillna(\"\").astype(str).tolist()\n",
    "    embs[m] = model.encode(texts, batch_size=8, convert_to_numpy=True,\n",
    "                           show_progress_bar=True, normalize_embeddings=True)\n",
    "    \n",
    "# concatenate all text\n",
    "df[\"concatenated_text\"] = df.apply(\n",
    "    lambda row: \" \".join([\n",
    "        str(row.get(\"ocr_final\", \"\")),\n",
    "        str(row.get(\"hashtags_text\", \"\")),\n",
    "        str(row.get(\"asr\", \"\")),\n",
    "        str(row.get(\"description\", \"\"))\n",
    "    ]).strip(),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"\\n===== CONCATENATED TEXTS =====\")\n",
    "for i, text in enumerate(df[\"concatenated_text\"].tolist(), start=1):\n",
    "    print(f\"[{i}] {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703c27b",
   "metadata": {},
   "source": [
    "**Fuse modalities and save:**\n",
    "\n",
    "In this step, the embeddings from all four text modalities are fused into a single vector using the previously determined weights. Each combined embedding is then saved as a .npy file for each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embs = (\n",
    "    weights[0] * embs[\"ocr_final\"] +\n",
    "    weights[1] * embs[\"hashtags_text\"] +\n",
    "    weights[2] * embs[\"asr\"] +\n",
    "    weights[3] * embs[\"description\"]\n",
    ")\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Saving embeddings\"):\n",
    "    video_name = row[\"video_base\"]\n",
    "    out_path = os.path.join(SAVE_DIR, f\"{video_name}_emb-text768.npy\")\n",
    "    np.save(out_path, combined_embs[i])\n",
    "    print(f\"✓ Saved: {out_path}\")\n",
    "\n",
    "print(f\"\\nAll text embeddings saved in: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3c38b",
   "metadata": {},
   "source": [
    "---\n",
    "## **RETRIEVING SIMILAR VIDEOS**\n",
    "**Goal**: Produce a list of most similar videos based on a weighted combination of modality-specific cosine similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a2252",
   "metadata": {},
   "source": [
    "**Embedding loading step:** creates a dict of embedding vectors following the below format to keep everything organized and so embedding retrieval for each video is trivial.\n",
    "$$\n",
    "video\\_name \\;\\rightarrow\\; \\{ audio,\\; video,\\; text \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_embeddings(base_dir=\"embeddings_out\"):\n",
    "    base_dir = Path(base_dir)\n",
    "\n",
    "    folders = {\n",
    "        \"audio\":  base_dir / \"audio2048\",\n",
    "        \"visual\": base_dir / \"video2048\",\n",
    "        \"text\":   base_dir / \"text768\",\n",
    "    }\n",
    "\n",
    "    suffix_map = {\n",
    "        \"audio\":  \"audio2048\",\n",
    "        \"visual\": \"visual2048\",\n",
    "        \"text\":   \"text768\",\n",
    "    }\n",
    "\n",
    "    modality_files = {} # collect keys per modality\n",
    "    for modality, folder in folders.items():\n",
    "        files = list(folder.glob(f\"*emb-{suffix_map[modality]}.npy\"))\n",
    "        modality_files[modality] = {f.stem.split(\"_emb-\")[0]: f for f in files}\n",
    "\n",
    "    all_video_ids = set()\n",
    "    for d in modality_files.values():\n",
    "        all_video_ids.update(d.keys())\n",
    "\n",
    "    embeddings = {}\n",
    "    missing = []\n",
    "\n",
    "    for vid in all_video_ids:\n",
    "        embeddings[vid] = {}\n",
    "        for modality in [\"audio\", \"visual\", \"text\"]:\n",
    "            file = modality_files[modality].get(vid, None)\n",
    "            if file is None:\n",
    "                missing.append((vid, modality))\n",
    "                embeddings[vid][modality] = None\n",
    "            else:\n",
    "                embeddings[vid][modality] = np.load(str(file))\n",
    "\n",
    "    if missing:\n",
    "        print(\"WARNING: Missing modality embeddings detected:\") # just to be safe\n",
    "        for vid, modality in missing:\n",
    "            print(f\"  - {vid} missing {modality}\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings = load_all_embeddings() # get embeddings with embeddings[\"video_name\"]\n",
    "\n",
    "# to check\n",
    "for video, emb_vec in embeddings.items():\n",
    "    print(video, emb_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2822fa",
   "metadata": {},
   "source": [
    ">**NOTE: Input the query video here :))**\n",
    "\n",
    "If testing different queries with the same set of videos, just <u>run the notebook starting at this cell</u> to skip the preprocessing and loading of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80952079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please type the EXACT filename of the query video\n",
    "QUERY = \"trend1vid1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed98ad",
   "metadata": {},
   "source": [
    "**Cosine similarity computation step:** computes modality-specific cosine similarity scores for each video and a query video, resulting in each video being represented as a vector of 3 similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return None\n",
    "    \n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2)) # maybe cast to float?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modality_similarities(query_video_name: str, embeddings_dir: Path):\n",
    "    \n",
    "    if query_video_name not in embeddings:\n",
    "        raise ValueError(f\"Query video '{query_video_name}' not found in embeddings.\")\n",
    "    \n",
    "    query_emb = embeddings[query_video_name]\n",
    "    \n",
    "    similarity_dict = {}\n",
    "    \n",
    "    for video_name, video_emb in embeddings.items(): \n",
    "        if video_name == query_video_name: # skips self\n",
    "            continue \n",
    "        \n",
    "        sims = []\n",
    "        missing_modalities = []\n",
    "        \n",
    "        for modality in [\"audio\", \"visual\", \"text\"]:\n",
    "            if modality not in video_emb or modality not in query_emb:\n",
    "                missing_modalities.append(modality)\n",
    "                sims.append(np.nan)  # for missing embeddings\n",
    "            else:\n",
    "                sims.append(cosine_similarity(video_emb[modality], query_emb[modality]))\n",
    "        \n",
    "        if missing_modalities:\n",
    "            print(f\"[WARNING] {video_name} missing embeddings for: {', '.join(missing_modalities)}\")\n",
    "        \n",
    "        similarity_dict[video_name] = np.array(sims)\n",
    "    \n",
    "    return similarity_dict\n",
    "\n",
    "similarities = compute_modality_similarities(QUERY, \"embeddings_out\") # get similarity vector with similarities[\"video_name\"]\n",
    "\n",
    "# to check\n",
    "for video, sim_vec in similarities.items():\n",
    "    print(video, sim_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb7155",
   "metadata": {},
   "source": [
    ">**NOTE: Input the weights here :))**\n",
    "\n",
    "If testing different weights with the same query video and set of videos, just <u>run the notebook starting at this cell</u> to skip computing the modality-specific cosine similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDITION_NAME = \"audio-video-text\"\n",
    "# baseline, audio-only, video-only, text-only, audio-video, audio-text, video-text, audio-video-text\n",
    "# can also do \"custom\"\n",
    "\n",
    "WEIGHT_AUDIO = WEIGHT_VIDEO = WEIGHT_TEXT = 0.0\n",
    "\n",
    "if CONDITION_NAME == \"baseline\":\n",
    "    WEIGHT_AUDIO = WEIGHT_VIDEO = WEIGHT_TEXT = 0.0\n",
    "elif CONDITION_NAME == \"audio-only\":\n",
    "    WEIGHT_AUDIO = 1.0\n",
    "elif CONDITION_NAME == \"video-only\":\n",
    "    WEIGHT_VIDEO = 1.0\n",
    "elif CONDITION_NAME == \"text-only\":\n",
    "    WEIGHT_TEXT = 1.0\n",
    "elif CONDITION_NAME == \"audio-video\":\n",
    "    WEIGHT_AUDIO = 0.5\n",
    "    WEIGHT_VIDEO = 0.5\n",
    "elif CONDITION_NAME == \"audio-text\":\n",
    "    WEIGHT_AUDIO = 0.5\n",
    "    WEIGHT_TEXT = 0.5\n",
    "elif CONDITION_NAME == \"video-text\":\n",
    "    WEIGHT_VIDEO = 0.5\n",
    "    WEIGHT_TEXT = 0.5\n",
    "elif CONDITION_NAME == \"audio-video-text\":\n",
    "    WEIGHT_AUDIO = WEIGHT_VIDEO = WEIGHT_TEXT = 1/3\n",
    "elif CONDITION_NAME == \"custom\":\n",
    "    WEIGHT_AUDIO = 0\n",
    "    WEIGHT_VIDEO = 0\n",
    "    WEIGHT_TEXT = 0\n",
    "else:\n",
    "    raise ValueError(f\"Unknown CONDITION_NAME: {CONDITION_NAME}\")\n",
    "\n",
    "print(f\"Weights -> Audio: {WEIGHT_AUDIO}, Video: {WEIGHT_VIDEO}, Text: {WEIGHT_TEXT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff9c80",
   "metadata": {},
   "source": [
    "**Weighted-sum fusion step:** uses weighted linear combination to form a final similarity score for each video and a query video, where the weights can be modified according to the different test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum_fusion(similarity_dict, weight_audio, weight_video, weight_text):\n",
    "\n",
    "    weights = np.array([weight_audio, weight_video, weight_text])\n",
    "    weights = weights / weights.sum() # apparently we need to normalize this cuz it might not equal 1\n",
    "    final_weighted_dict = {}\n",
    "    \n",
    "    for video, sim_vec in similarity_dict.items():\n",
    "        if len(sim_vec) != 3:\n",
    "            raise ValueError(f\"Expected 3 modalities in similarity vector for {video}, got {len(sim_vec)}\")\n",
    "        \n",
    "        sim_audio, sim_video, sim_text = sim_vec\n",
    "        weighted_score = (sim_audio*weights[0] + sim_video*weights[1] + sim_text*weights[2])\n",
    "        final_weighted_dict[video] = float(weighted_score)\n",
    "\n",
    "    return final_weighted_dict\n",
    "\n",
    "final_scores = weighted_sum_fusion(similarities, WEIGHT_AUDIO, WEIGHT_VIDEO, WEIGHT_TEXT)\n",
    "\n",
    "# to check\n",
    "for video, score in final_scores.items():\n",
    "    print(video, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a9235",
   "metadata": {},
   "source": [
    "**Ranking step:** uses the final scores from weighted sum fusion to rank all videos by their similarity score with the query video, printed in descending order.\n",
    "\n",
    "***most_similar_videos*** is the final output which will be fed into the annotation generation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd1a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3 # no of similar videos to retrieve (3-5 seems best)\n",
    "\n",
    "def rank_by_score(final_weighted_dict, top_k=None):\n",
    "    ranked_videos = sorted(final_weighted_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if top_k is not None:\n",
    "        ranked_videos = ranked_videos[:top_k]\n",
    "    \n",
    "    return ranked_videos\n",
    "\n",
    "most_similar_videos = rank_by_score(final_scores, top_k = k)\n",
    "\n",
    "print(f\"Top {k} most similar videos to {QUERY}\")\n",
    "for video, score in most_similar_videos:\n",
    "    print(f\"{video}: {score:.4f}\")\n",
    "\n",
    "similar_videos_output = [QUERY] + [video for video, _ in most_similar_videos]\n",
    "\n",
    "print(\"\\nGemini output array format:\")\n",
    "print(similar_videos_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dd5b69",
   "metadata": {},
   "source": [
    "---\n",
    "## **GENERATING ANNOTATIONS**\n",
    "**Goal**: Produce annotations based on the query video and list of similar videos produced using the earlier weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2094d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_VIDEO_PATH = rf\"media\\{QUERY}.mp4\"\n",
    "BASE_MEDIA_PATH = r\"media\" \n",
    "\n",
    "CONTEXT_VIDEO_TUPLES = most_similar_videos\n",
    "\n",
    "TOP_K = k\n",
    "\n",
    "OUTPUT_DIR = f\"annotations\\{QUERY}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "JSON_OUTPUT_PATH = os.path.join(OUTPUT_DIR, f\"{QUERY}_{CONDITION_NAME}.json\")\n",
    "CSV_OUTPUT_PATH = os.path.join(OUTPUT_DIR, f\"{QUERY}_{CONDITION_NAME}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91812779",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    GEMINI_API_KEY = getpass.getpass(\"Enter your Gemini API Key: \")\n",
    "except ImportError:\n",
    "    GEMINI_API_KEY = \"PASTE_YOUR_API_KEY_HERE\"\n",
    "    \n",
    "GENAI_MODEL_NAME = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_paths(similar_list: List, base_path: str, k: int):\n",
    "    video_names = [video_name for video_name, _ in similar_list]\n",
    "    top_k_names = video_names[:k]\n",
    "    return [os.path.join(base_path, f\"{name}.mp4\") for name in top_k_names]\n",
    "\n",
    "if CONDITION_NAME == \"baseline\":\n",
    "    CONTEXT_VIDEO_PATHS = []\n",
    "else:\n",
    "    CONTEXT_VIDEO_PATHS = build_paths(CONTEXT_VIDEO_TUPLES, BASE_MEDIA_PATH, TOP_K)\n",
    "\n",
    "print(f\"--- Preparing Annotation for: {QUERY} ---\")\n",
    "print(f\"Condition: {CONDITION_NAME}\")\n",
    "print(f\"Context Videos: {CONTEXT_VIDEO_PATHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dbc2cc",
   "metadata": {},
   "source": [
    "**Core Logic: Prompts & Helper Functions**\n",
    "\n",
    "This cell defines the \"brains\" of the annotation experiment.\n",
    "\n",
    "* **System Prompts:** `BASELINE_SYSTEM` and `CONTEXT_AWARE_SYSTEM` are the two \"personalities\" we give the AI. This is the core of our A/B test.\n",
    "* **`_upload_video`:** This is a crucial optimization. It uploads a video file to the Gemini API and then **caches** the result. If we try to upload the same video (e.g., the query video) 8 times, this cache ensures it only *actually* uploads it the first time, making our runs much faster.\n",
    "* **`_make_model`:** This helper creates the Gemini model and correctly passes our \"system prompt\" to it using a **keyword argument** (`system_instruction=...`) to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4619302",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_SYSTEM = \"You are an assistant tasked with generating a brief summary of a short video. Use only the information available in the video. Do not rely on any external knowledge or assumptions. Focus on describing what is happening in the video concisely.\"\n",
    "CONTEXT_AWARE_SYSTEM = \"You are an assistant tasked with generating a summary of a short video. You are provided with the main video and a few additional videos that are semantically related. Use all available information to generate a summary that best describes what is happening in the main video. Focus on enhancing your understanding using the related videos, but ensure the summary reflects the main video.\"\n",
    "BASELINE_USER = \"Please generate a 2–3 sentence summary of the following video based solely on its content.\"\n",
    "CONTEXT_AWARE_USER = \"Please summarize the main video using all the information provided. The first video is the main one, and the others are related videos that may provide helpful context. Your summary should describe what is happening in the main video in 2–3 sentences.\"\n",
    "\n",
    "_uploaded_cache: Dict[str, Any] = {}\n",
    "def _upload_video(path: str):\n",
    "    global _uploaded_cache\n",
    "    full = str(Path(path).resolve())\n",
    "    if full not in _uploaded_cache:\n",
    "        print(f\"Uploading: {full}\")\n",
    "        try:\n",
    "            file_obj = genai.upload_file(path=full)\n",
    "            print(\"Uploaded, waiting for processing...\")\n",
    "            while True:\n",
    "                file_obj = genai.get_file(file_obj.name)\n",
    "                if file_obj.state.name == \"ACTIVE\":\n",
    "                    print(f\"File is ACTIVE: {file_obj.name}\"); break\n",
    "                elif file_obj.state.name == \"FAILED\":\n",
    "                    raise RuntimeError(f\"File {file_obj.name} failed to process.\")\n",
    "                time.sleep(2)\n",
    "            _uploaded_cache[full] = file_obj\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {path}: {e}\"); return None\n",
    "    return _uploaded_cache.get(full)\n",
    "\n",
    "def _make_model(system_instruction: str):\n",
    "    return genai.GenerativeModel(\n",
    "        model_name=GENAI_MODEL_NAME,\n",
    "        system_instruction=system_instruction\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7037587c",
   "metadata": {},
   "source": [
    "**Run the Annotation**\n",
    "\n",
    "This is the final \"Run\" button for this experiment. This cell takes all the variables you just set up in the configuration cell and:\n",
    "\n",
    "1.  Configures the Gemini API with a secure `GEMINI_API_KEY`.\n",
    "2.  Uploads the main `QUERY_VIDEO_PATH` using the cache.\n",
    "3.  Checks the `CONDITION_NAME`:\n",
    "    * If it's `\"baseline\"`, it runs the \"control\" experiment (video only).\n",
    "    * If it's anything else, it runs the \"context-aware\" experiment, uploading and attaching the `CONTEXT_VIDEO_PATHS` to the prompt.\n",
    "4.  Saves the final annotation text from Gemini into the `.json` and `.csv` files defined (e.g., `trend1vid1_visual_only.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "print(\"--- Starting Annotation ---\")\n",
    "\n",
    "_uploaded_cache = {} \n",
    "annotation_text = \"\"\n",
    "final_record = {}\n",
    "\n",
    "query_file = _upload_video(QUERY_VIDEO_PATH)\n",
    "\n",
    "if query_file is None:\n",
    "    print(f\"Aborting: Failed to upload main query video {QUERY_VIDEO_PATH}\")\n",
    "else:\n",
    "    if CONDITION_NAME == \"baseline\":\n",
    "        print(\"Running BASELINE annotation...\")\n",
    "        model = _make_model(BASELINE_SYSTEM)\n",
    "        try:\n",
    "            response = model.generate_content([query_file, BASELINE_USER])\n",
    "            annotation_text = response.text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in baseline generation: {e}\"); annotation_text = f\"ERROR: {e}\"\n",
    "        \n",
    "        final_record = {\n",
    "            \"query_id\": QUERY, \"condition_name\": CONDITION_NAME,\n",
    "            \"context_video_paths\": [], \"annotation_text\": annotation_text,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        print(f\"Running CONTEXT-AWARE annotation for: {CONDITION_NAME}...\")\n",
    "        model = _make_model(CONTEXT_AWARE_SYSTEM)\n",
    "        \n",
    "        ctx_files = []\n",
    "        for p in CONTEXT_VIDEO_PATHS:\n",
    "            f = _upload_video(p)\n",
    "            if f: ctx_files.append(f)\n",
    "        \n",
    "        contents = [query_file] + ctx_files + [CONTEXT_AWARE_USER]\n",
    "        \n",
    "        try:\n",
    "            response = model.generate_content(contents)\n",
    "            annotation_text = response.text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in context generation: {e}\"); annotation_text = f\"ERROR: {e}\"\n",
    "        \n",
    "        final_record = {\n",
    "            \"query_id\": QUERY, \"condition_name\": CONDITION_NAME,\n",
    "            \"context_video_paths\": CONTEXT_VIDEO_PATHS, \"annotation_text\": annotation_text,\n",
    "        }\n",
    "\n",
    "    print(\"\\n--- Annotation Complete ---\")\n",
    "    print(f\"Result: {annotation_text}\")\n",
    "\n",
    "    if final_record:\n",
    "        pd.DataFrame([final_record]).to_csv(CSV_OUTPUT_PATH, index=False)\n",
    "        with open(JSON_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final_record, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved results to:\\n  {CSV_OUTPUT_PATH}\\n  {JSON_OUTPUT_PATH}\")\n",
    "    else:\n",
    "        print(\"No result to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
