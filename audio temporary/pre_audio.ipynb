{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d9b781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.8.0+cu129\n",
      "torchaudio version: 2.8.0+cu129\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Run this to activate venv for the terminal instance: .venv\\Scripts\\activate\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ffmpeg\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "# Should be 2.8.0 (torch & torchaudio) and 20250625 (whisper)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torchaudio version:\", torchaudio.__version__)\n",
    "\n",
    "# Make sure cuda (gpu) is active!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e4fae",
   "metadata": {},
   "source": [
    "**STEP 1**\n",
    "\n",
    "*\"For each video, ffmpeg, a command-line multimedia framework, is used to isolate the audio stream from the video file and set it in two formats: the first as a 16kHz mono .wav file, and the second as a 32kHz mono .wav file.\"*\n",
    "\n",
    "ffmpeg parameters reference:\n",
    "1. out16 = output file path\n",
    "2. format/-f = formats the container, we want wav\n",
    "3. acodec = audio codec, this is usually pcm_s16le -> signed 16-bit little-endian\n",
    "4. ac = number of audio channels, we want mono aka 1\n",
    "5. ar = sample rate, we want 16kHz/32kHz aka 16000/32000\n",
    "\n",
    "NOTE: overwrite will be set to true for now while testing for single video.\n",
    "\n",
    "NOTE: I referred to the ffmpeg documentation [here](https://ffmpeg.org/documentation.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003957cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_to_wavs(video_path: str, out16: str, out32: str, overwrite: bool=True):\n",
    "    # 16kHz mono\n",
    "    extract_16k=(\n",
    "        ffmpeg.input(video_path).output(out16, format='wav', acodec='pcm_s16le', ac=1, ar=16000)\n",
    "    )\n",
    "    if overwrite:\n",
    "        extract_16k = extract_16k.overwrite_output()\n",
    "    \n",
    "    extract_16k.run(quiet=True)\n",
    "    print(\"Wrote 16kHz\", out16)\n",
    "\n",
    "    # 32kHz mono\n",
    "    extract_32k=(\n",
    "        ffmpeg.input(video_path).output(out32, format='wav', acodec='pcm_s16le', ac=1, ar=32000)\n",
    "    )\n",
    "    if overwrite:\n",
    "        extract_32k = extract_32k.overwrite_output()\n",
    "    \n",
    "    extract_32k.run(quiet=True)\n",
    "    print(\"Wrote 32kHz\", out32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9031bfb",
   "metadata": {},
   "source": [
    "**STEP 2**\n",
    "\n",
    "*\"Then, the Python library torchaudio loads the files, and both waveforms are then coverted into log-Mel spectogram form.\"*\n",
    "\n",
    "Fourier Transform reference:\n",
    "1. win_length = number of samples per frame\n",
    "2. hop_length = how far the window moves each step (in samples)\n",
    "3. n_fft = number of frequency bins, usually chosen as a power of 2\n",
    "\n",
    "NOTE: we will likely need to modify parameters so that the log-Mel will be plug-and-play when feeding it to CNN14 later on.\n",
    "\n",
    "NOTE: small inaccuracy pala sa paper, we only need to convert the 32kHz wav into log-Mel since Whisper will convert the 16kHz one automatically.\n",
    "\n",
    "NOTE: I referred to the torchaudio documentation [here](https://docs.pytorch.org/audio/main/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7cfe230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_log_mel(wav_path: str, sample_rate: int=32000, n_mels: int=80, win_ms: float=25.0, \n",
    "                       hop_ms: float=10.0, n_fft: int=None, device: str=\"cpu\"):\n",
    "    waveform, sr = torchaudio.load(wav_path)\n",
    "\n",
    "    # Make sure we're passing out32 and not out16\n",
    "    if sr != sample_rate: \n",
    "        raise ValueError(f\"Expected {sample_rate} Hz .wav but got {sr} Hz\")\n",
    "    \n",
    "    # Make sure we're passing mono, downmixes them in case (can remove later)\n",
    "    if waveform.shape[0]>1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # STFT parameters\n",
    "    win_length = int(round((win_ms/1000.0)*sample_rate))\n",
    "    hop_length = int(round((hop_ms/1000.0)*sample_rate))\n",
    "\n",
    "    # Number of frequency bins per FFT\n",
    "    if n_fft is None:\n",
    "        n_fft=1 << (win_length-1).bit_length()\n",
    "\n",
    "    # Heaviest parts, we assign to GPU (device)\n",
    "    # Outputs the spectogram\n",
    "    mel_spec_transform = MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, win_length=win_length, \n",
    "                                        hop_length=hop_length,n_mels=n_mels,power=2.0).to(device)\n",
    "    # Converts spectogram power values into decibels\n",
    "    to_decibel = AmplitudeToDB(stype=\"power\").to(device)\n",
    "\n",
    "    waveform = waveform.to(device)\n",
    "    mel_spec = mel_spec_transform(waveform)\n",
    "\n",
    "    # converts to log scale aka decibels\n",
    "    log_mel = to_decibel(mel_spec)\n",
    "\n",
    "    return log_mel.squeeze(0).cpu()\n",
    "\n",
    "# For visualizing the spectogram using pyplot\n",
    "def plot_log_mel(log_mel, sample_rate, title=\"Log-Mel Spectogram\"):\n",
    "    log_mel_np = log_mel.detach().cpu().numpy()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(log_mel_np, \n",
    "               origin=\"lower\", \n",
    "               aspect=\"auto\", \n",
    "               interpolation=\"nearest\")\n",
    "    plt.colorbar(format=\"%+2.f dB\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Frames\")\n",
    "    plt.ylabel(\"Mel bins\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124765e",
   "metadata": {},
   "source": [
    "**STEP 3**\n",
    "\n",
    "*\"The spectogram belonging to the 16kHz file is fed into the Whisper model for transcription.\"*\n",
    "\n",
    "NOTE: I referred to the Whisper documentation [here](https://pypi.org/project/openai-whisper/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "890162b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transcribe_with_whisper(wav_16k_path: str, model_size: str=\"small\", task: str=\"transcribe\"):\n",
    "#     print(\"Loading the Whisper model: \", model_size)\n",
    "#     model = whisper.load_model(model_size)\n",
    "#     result = model.transcribe(wav_16k_path, task=task)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed0fd3",
   "metadata": {},
   "source": [
    "**SAMPLE RUN ON A TIKTOK VIDEO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6726ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path: str, model_size: str=\"small\", out_dir: str =\"proc_out\"):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True) #16kHz, logMel, and transcript go to out_dir\n",
    "    audio_dir = out_dir.parent / (out_dir.name + \"_32kHz\")\n",
    "    audio_dir.mkdir(parents=True, exist_ok=True) # 32kHz goes to audio_dir\n",
    "\n",
    "    video = Path(video_path)\n",
    "    out16 = out_dir / (video.stem + \"_16k.wav\") # 16kHz wav output\n",
    "    out32 = audio_dir / (video.stem + \"_32k.wav\") # 32kHz output\n",
    "    npy = out_dir / (video.stem + \"_32k_logmel.npy\") # log-Mel output\n",
    "    txt = out_dir / (video.stem + \"_transcript.txt\") # whisper transcript\n",
    "\n",
    "    # Extract audio\n",
    "    extract_audio_to_wavs(str(video), str(out16), str(out32))\n",
    "\n",
    "    # Convert to log-Mel spectogram\n",
    "    log_mel = convert_to_log_mel(str(out32), sample_rate=32000, n_mels=80)\n",
    "    np.save(str(npy), log_mel.numpy())\n",
    "    print(\"Saved log-Mel:\", npy, log_mel.shape)\n",
    "    # plot_log_mel(log_mel, 32000, \"32kHz Log-Mel Spectogram\") # if you wanna see the spectogram\n",
    "\n",
    "    # # Whisper transcription, can disable since jersey is doing Whisper as well\n",
    "    # res = transcribe_with_whisper(str(out16), model_size=model_size)\n",
    "    # with open(txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    #     f.write(res[\"text\"])\n",
    "    # print(\"Transcript saved:\", txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1b2c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 videos found!\n",
      "\n",
      "Processing: trend3vid6.mp4\n",
      "Wrote 16kHz proc_out\\trend3vid6_16k.wav\n",
      "Wrote 32kHz proc_out_32kHz\\trend3vid6_32k.wav\n",
      "Saved log-Mel: proc_out\\trend3vid6_32k_logmel.npy torch.Size([80, 704])\n",
      "\n",
      "Processing: trend3vid7.mp4\n",
      "Wrote 16kHz"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mkyod\\OneDrive\\Documents\\GitHub\\context-aware-video-retrieval\\audio temporary\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " proc_out\\trend3vid7_16k.wav\n",
      "Wrote 32kHz proc_out_32kHz\\trend3vid7_32k.wav\n",
      "Saved log-Mel: proc_out\\trend3vid7_32k_logmel.npy torch.Size([80, 1039])\n",
      "\n",
      "Processing: trend3vid8.mp4\n",
      "Wrote 16kHz proc_out\\trend3vid8_16k.wav\n",
      "Wrote 32kHz proc_out_32kHz\\trend3vid8_32k.wav\n",
      "Saved log-Mel: proc_out\\trend3vid8_32k_logmel.npy torch.Size([80, 681])\n",
      "\n",
      "Processing: trend5vid2.mp4\n",
      "Wrote 16kHz proc_out\\trend5vid2_16k.wav\n",
      "Wrote 32kHz proc_out_32kHz\\trend5vid2_32k.wav\n",
      "Saved log-Mel: proc_out\\trend5vid2_32k_logmel.npy torch.Size([80, 546])\n",
      "\n",
      "Processing: trend5vid3.mp4\n",
      "Wrote 16kHz proc_out\\trend5vid3_16k.wav\n",
      "Wrote 32kHz proc_out_32kHz\\trend5vid3_32k.wav\n",
      "Saved log-Mel: proc_out\\trend5vid3_32k_logmel.npy torch.Size([80, 560])\n",
      "\n",
      "Processing: trend5vid4.mp4\n",
      "Wrote 16kHz proc_out\\trend5vid4_16k.wav\n",
      "Wrote 32kHz proc_out_32kHz\\trend5vid4_32k.wav\n",
      "Saved log-Mel: proc_out\\trend5vid4_32k_logmel.npy torch.Size([80, 551])\n"
     ]
    }
   ],
   "source": [
    "# video_path = \"media/testvid2.mp4\"\n",
    "# process_video(video_path)\n",
    "\n",
    "media_dir = Path(\"media\")\n",
    "videos = list(media_dir.glob(\"*.mp4\"))\n",
    "print(f\"{len(videos)} videos found!\")\n",
    "\n",
    "for video in videos:\n",
    "    print(f\"\\nProcessing: {video.name}\")\n",
    "    process_video(video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
