{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be90fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip uninstall pandas numpy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5e09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60781fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b402c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shanette\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.6) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71cc13d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 51) (2318235494.py, line 51)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 51\u001b[1;36m\u001b[0m\n\u001b[1;33m    PATH_VA = r\"media\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 51)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Gemini API Configuration ---\n",
    "GEMINI_API_KEY = \"AIzaSyBO1deeym5AKDTGLOUGMiKDzAyRsKK75r8\"\n",
    "\n",
    "GENAI_MODEL_NAME = \"gemini-1.5-pro-latest\"\n",
    "\n",
    "# --- 2. Input Query & Output Files ---\n",
    "QUERY_ID = \"airball_1\"\n",
    "QUERY_VIDEO_PATH = rf\"media\\{QUERY_ID}.mp4\" # Path to the main video to annotate\n",
    "\n",
    "JSON_OUTPUT_PATH = f\"{QUERY_ID}_annotations.json\"\n",
    "CSV_OUTPUT_PATH = f\"{QUERY_ID}_annotations.csv\"\n",
    "\n",
    "# --- 3. (NEW) Similarity Results & \"Top-K\" ---\n",
    "# -----------------------------------------------------------------\n",
    "# This is where you paste the output from your other script.\n",
    "# (I'm just using dummy data here as an example)\n",
    "# -----------------------------------------------------------------\n",
    "TOP_K = 3 # How many context videos to use? (e.g., Top 3)\n",
    "\n",
    "# Paste your list of (video_name, score) tuples for each condition\n",
    "most_similar_visual = [('airball_8.mp4', 0.85), ('airball_3.mp4', 0.79), ('where_1.mp4', 0.75), ('airball_11.mp4', 0.72)]\n",
    "most_similar_audio = [('airball_8.mp4', 0.92), ('airball_9.mp4', 0.88), ('airball_10.mp4', 0.81)]\n",
    "most_similar_text = [('airball_5.mp4', 0.78), ('airball_9.mp4', 0.77), ('airball_10.mp4', 0.71)]\n",
    "most_similar_vis_aud = [('airball_3.mp4', 0.91), ('airball_8.mp4', 0.89), ('airball_11.mp4', 0.85)]\n",
    "most_similar_aud_txt = [('airball_9.mp4', 0.95), ('airball_5.mp4', 0.92), ('airball_10.mp4', 0.90)]\n",
    "most_similar_vis_txt = [('airball_3.mp4', 0.82), ('airball_11.mp4', 0.80), ('airball_5.mp4', 0.79)]\n",
    "most_similar_vis_aud_txt = [('airball_8.mp4', 0.98), ('airball_3.mp4', 0.96), ('airball_9.mp4', 0.95)]\n",
    "\n",
    "# --- 4. (NEW) Helper to Build Context Sets ---\n",
    "# This part automatically builds the full file paths\n",
    "# -----------------------------------------------------------------\n",
    "def build_paths(similar_list: List, base_path: str, k: int) -> List[str]:\n",
    "    # 1. Get just the video names (using your logic)\n",
    "    video_names = [video_name for video_name, _ in similar_list]\n",
    "    \n",
    "    # 2. Get just the Top-K\n",
    "    top_k_names = video_names[:k]\n",
    "    \n",
    "    # 3. Build the full file paths\n",
    "    # Using rf\"\" (raw string literal) to handle Windows paths\n",
    "    return [rf\"{base_path}\\{name}\" for name in top_k_names]\n",
    "\n",
    "# Define the base folders for your context videos\n",
    "# (You might need to adjust these paths)\n",
    "PATH_VIS = r\"media\"\n",
    "PATH_AUD = r\"media\"\n",
    "PATH_TXT = r\"media\"\n",
    "PATH_VA = r\"media\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e943088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System-level instructions ---\n",
    "BASELINE_SYSTEM = (\n",
    "    \"You are an assistant tasked with generating a brief summary of a short video. \"\n",
    "    \"Use only the information available in the video. Do not rely on any external \"\n",
    "    \"knowledge or assumptions. Focus on describing what is happening in the video \"\n",
    "    \"concisely.\"\n",
    ")\n",
    "\n",
    "CONTEXT_AWARE_SYSTEM = (\n",
    "    \"You are an assistant tasked with generating a summary of a short video. \"\n",
    "    \"You are provided with the main video and a few additional videos that are \"\n",
    "    \"semantically related. Use all available information to generate a summary \"\n",
    "    \"that best describes what is happening in the main video. Focus on enhancing \"\n",
    "    \"your understanding using the related videos, but ensure the summary reflects \"\n",
    "    \"the main video.\"\n",
    ")\n",
    "\n",
    "# --- User prompts ---\n",
    "BASELINE_USER = (\n",
    "    \"Please generate a 2–3 sentence summary of the following video based solely \"\n",
    "    \"on its content.\"\n",
    ")\n",
    "\n",
    "CONTEXT_AWARE_USER = (\n",
    "    \"Please summarize the main video using all the information provided. The first \"\n",
    "    \"video is the main one, and the others are related videos that may provide \"\n",
    "    \"helpful context. Your summary should describe what is happening in the main \"\n",
    "    \"video in 2–3 sentences.\"\n",
    ")\n",
    "\n",
    "# --- Names for the 7 context sets ---\n",
    "CONDITION_NAMES = [\n",
    "    \"visual_only\",       # set 1\n",
    "    \"audio_only\",        # set 2\n",
    "    \"text_only\",         # set 3\n",
    "    \"visual_plus_audio\", # set 4\n",
    "    \"audio_plus_text\",   # set 5\n",
    "    \"visual_plus_text\",  # set 6\n",
    "    \"visual_audio_text\", # set 7\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a42b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    print(\"Gemini API Key configured successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring Gemini API: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4337c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache to store uploaded file objects\n",
    "_uploaded_cache: Dict[str, Any] = {}\n",
    "\n",
    "def _upload_video(path: str):\n",
    "    \"\"\"\n",
    "    Upload a video to Gemini Files API once and wait until it's ACTIVE.\n",
    "    \"\"\"\n",
    "    global _uploaded_cache\n",
    "    full = str(Path(path).resolve())\n",
    "\n",
    "    if full not in _uploaded_cache:\n",
    "        print(f\"Uploading: {full}\")\n",
    "        try:\n",
    "            file_obj = genai.upload_file(path=full)\n",
    "            print(\"Uploaded, waiting for processing...\")\n",
    "\n",
    "            # Wait until ACTIVE\n",
    "            while True:\n",
    "                file_obj = genai.get_file(file_obj.name)\n",
    "                if file_obj.state.name == \"ACTIVE\":\n",
    "                    print(f\"✅ File is ACTIVE: {file_obj.name}\")\n",
    "                    break\n",
    "                elif file_obj.state.name == \"FAILED\":\n",
    "                    raise RuntimeError(f\"❌ File {file_obj.name} failed to process.\")\n",
    "                time.sleep(2)\n",
    "            \n",
    "            _uploaded_cache[full] = file_obj\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error uploading {path}: {e}\")\n",
    "            return None # Return None on failure\n",
    "\n",
    "    return _uploaded_cache.get(full)\n",
    "\n",
    "\n",
    "def _make_model(system_instruction: str):\n",
    "    return genai.GenerativeModel(\n",
    "        model_name=GENAI_MODEL_NAME,\n",
    "        system_instruction=system_instruction,\n",
    "    )\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_single_query(\n",
    "    query_video_path: str,\n",
    "    context_sets: List[List[str]],\n",
    "    query_id: str = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run 8 Gemini calls for one query video:\n",
    "      - baseline (no extra context)\n",
    "      - 7 context-aware conditions in the order of CONDITION_NAMES\n",
    "    \"\"\"\n",
    "    if len(context_sets) != 7:\n",
    "        raise ValueError(\"context_sets must be a list of 7 context video sets.\")\n",
    "\n",
    "    if query_id is None:\n",
    "        query_id = Path(query_video_path).stem\n",
    "\n",
    "    baseline_model = _make_model(BASELINE_SYSTEM)\n",
    "    context_model = _make_model(CONTEXT_AWARE_SYSTEM)\n",
    "\n",
    "    query_file = _upload_video(query_video_path)\n",
    "    if query_file is None:\n",
    "        print(f\"❌ Aborting: Failed to upload main query video {query_video_path}\")\n",
    "        return []\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    # --------- 1) Baseline (query only) ---------\n",
    "    print(f\"\\n[Query {query_id}] Baseline annotation …\")\n",
    "    try:\n",
    "        baseline_response = baseline_model.generate_content(\n",
    "            [query_file, BASELINE_USER]\n",
    "        )\n",
    "        baseline_text = baseline_response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error in baseline generation: {e}\")\n",
    "        baseline_text = f\"ERROR: {e}\"\n",
    "\n",
    "    results.append({\n",
    "        \"query_id\": query_id,\n",
    "        \"annotation_type\": \"baseline\",\n",
    "        \"condition_index\": 0,\n",
    "        \"condition_name\": \"baseline\",\n",
    "        \"query_video_path\": query_video_path,\n",
    "        \"context_video_paths\": [],\n",
    "        \"annotation_text\": baseline_text,\n",
    "    })\n",
    "\n",
    "    # --------- 2) 7 context-aware annotations ---------\n",
    "    for idx, (cond_name, ctx_paths) in enumerate(zip(CONDITION_NAMES, context_sets), start=1):\n",
    "        print(f\"[Query {query_id}] Context condition {idx}: {cond_name} \"\n",
    "              f\"with {len(ctx_paths)} context videos …\")\n",
    "        \n",
    "        # Upload context videos, skipping any that fail\n",
    "        ctx_files = []\n",
    "        for p in ctx_paths:\n",
    "            f = _upload_video(p)\n",
    "            if f:\n",
    "                ctx_files.append(f)\n",
    "\n",
    "        # Main video first, then context videos, then the user prompt\n",
    "        contents = [query_file] + ctx_files + [CONTEXT_AWARE_USER]\n",
    "\n",
    "        ctx_text = \"\"\n",
    "        try:\n",
    "            ctx_response = context_model.generate_content(contents)\n",
    "            ctx_text = ctx_response.text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error in context generation for {cond_name}: {e}\")\n",
    "            ctx_text = f\"ERROR: {e}\"\n",
    "\n",
    "        results.append({\n",
    "            \"query_id\": query_id,\n",
    "            \"annotation_type\": \"context_aware\",\n",
    "            \"condition_index\": idx,      # 1..7\n",
    "            \"condition_name\": cond_name, # matches CONDITION_NAMES\n",
    "            \"query_video_path\": query_video_path,\n",
    "            \"context_video_paths\": ctx_paths,\n",
    "            \"annotation_text\": ctx_text,\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"Core annotation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_annotations(\n",
    "    annotations: List[Dict[str, Any]],\n",
    "    json_path: str,\n",
    "    csv_path: str,\n",
    "):\n",
    "    \"\"\"Saves the list of annotation results to JSON and CSV.\"\"\"\n",
    "    if not annotations:\n",
    "        print(\"No annotations to save.\")\n",
    "        return\n",
    "        \n",
    "    # JSON (all records in one list)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(annotations, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # CSV\n",
    "    df = pd.DataFrame(annotations)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"\\nSaved annotations to:\\n  JSON: {json_path}\\n  CSV:  {csv_path}\")\n",
    "\n",
    "print(\"Save utility defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting annotation for query: {QUERY_ID}\")\n",
    "print(f\"Query video: {QUERY_VIDEO_PATH}\")\n",
    "print(f\"Using model: {GENAI_MODEL_NAME}\")\n",
    "\n",
    "# Clear the cache for a fresh run (optional, remove if you want to reuse uploads)\n",
    "_uploaded_cache = {} \n",
    "\n",
    "# --- Run the main function ---\n",
    "all_annotations = annotate_single_query(\n",
    "    query_video_path=QUERY_VIDEO_PATH, # <--- Uses variable from Cell 2\n",
    "    context_sets=CONTEXT_SETS,     # <--- Uses variable from Cell 2\n",
    "    query_id=QUERY_ID,             # <--- Uses variable from Cell 2\n",
    ")\n",
    "\n",
    "# --- Save the results ---\n",
    "if all_annotations:\n",
    "    save_annotations(\n",
    "        annotations=all_annotations,\n",
    "        json_path=JSON_OUTPUT_PATH, # <--- Uses variable from Cell 2\n",
    "        csv_path=CSV_OUTPUT_PATH,   # <--- Uses variable from Cell 2\n",
    "    )\n",
    "else:\n",
    "    print(\"Annotation process failed, no results to save.\")\n",
    "\n",
    "print(\"\\n--- Annotation process complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_results = pd.read_csv(CSV_OUTPUT_PATH)\n",
    "    display(df_results)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Could not find CSV file at {CSV_OUTPUT_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the CSV: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
